{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要モジュール\n",
    "import torch\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 学習用のサンプルデータ\n",
    "from sklearn import datasets\n",
    "\n",
    "# データを整理する\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 全結合層と活性化関数\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 損失関数と最適化関数\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run with cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"run with {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../database/normaraze_stock_price/dataset_20231012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>2021-10-12</th>\n",
       "      <th>2021-10-13</th>\n",
       "      <th>2021-10-14</th>\n",
       "      <th>2021-10-15</th>\n",
       "      <th>2021-10-18</th>\n",
       "      <th>2021-10-19</th>\n",
       "      <th>2021-10-20</th>\n",
       "      <th>2021-10-21</th>\n",
       "      <th>2021-10-22</th>\n",
       "      <th>...</th>\n",
       "      <th>2023-09-28</th>\n",
       "      <th>2023-09-29</th>\n",
       "      <th>2023-10-02</th>\n",
       "      <th>2023-10-03</th>\n",
       "      <th>2023-10-04</th>\n",
       "      <th>2023-10-05</th>\n",
       "      <th>2023-10-06</th>\n",
       "      <th>2023-10-09</th>\n",
       "      <th>2023-10-10</th>\n",
       "      <th>2023-10-11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AADI</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.950808</td>\n",
       "      <td>0.940970</td>\n",
       "      <td>0.956079</td>\n",
       "      <td>0.921644</td>\n",
       "      <td>0.936753</td>\n",
       "      <td>0.975755</td>\n",
       "      <td>0.943429</td>\n",
       "      <td>0.951862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174631</td>\n",
       "      <td>0.170063</td>\n",
       "      <td>0.151089</td>\n",
       "      <td>0.141251</td>\n",
       "      <td>0.147576</td>\n",
       "      <td>0.153549</td>\n",
       "      <td>0.157414</td>\n",
       "      <td>0.149332</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>0.897574</td>\n",
       "      <td>0.891285</td>\n",
       "      <td>0.877359</td>\n",
       "      <td>0.876909</td>\n",
       "      <td>0.893531</td>\n",
       "      <td>0.860287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580413</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.572776</td>\n",
       "      <td>0.552111</td>\n",
       "      <td>0.571878</td>\n",
       "      <td>0.577269</td>\n",
       "      <td>0.573226</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>0.550764</td>\n",
       "      <td>0.560647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>0.971175</td>\n",
       "      <td>0.955654</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.960089</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>0.920177</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>0.929047</td>\n",
       "      <td>0.906874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414846</td>\n",
       "      <td>0.435137</td>\n",
       "      <td>0.430628</td>\n",
       "      <td>0.423864</td>\n",
       "      <td>0.412591</td>\n",
       "      <td>0.412591</td>\n",
       "      <td>0.408082</td>\n",
       "      <td>0.394554</td>\n",
       "      <td>0.408082</td>\n",
       "      <td>0.410336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.474160</td>\n",
       "      <td>0.466408</td>\n",
       "      <td>0.462532</td>\n",
       "      <td>0.479328</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.485788</td>\n",
       "      <td>0.484496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711886</td>\n",
       "      <td>0.708656</td>\n",
       "      <td>0.728036</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.586563</td>\n",
       "      <td>0.489987</td>\n",
       "      <td>0.481912</td>\n",
       "      <td>0.503230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>0.619085</td>\n",
       "      <td>0.621760</td>\n",
       "      <td>0.639195</td>\n",
       "      <td>0.645745</td>\n",
       "      <td>0.653586</td>\n",
       "      <td>0.659213</td>\n",
       "      <td>0.650726</td>\n",
       "      <td>0.648051</td>\n",
       "      <td>0.652387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810549</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>0.803923</td>\n",
       "      <td>0.781651</td>\n",
       "      <td>0.792787</td>\n",
       "      <td>0.789545</td>\n",
       "      <td>0.807448</td>\n",
       "      <td>0.813650</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.832539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>ZUMZ</td>\n",
       "      <td>0.740436</td>\n",
       "      <td>0.730551</td>\n",
       "      <td>0.730368</td>\n",
       "      <td>0.723961</td>\n",
       "      <td>0.747392</td>\n",
       "      <td>0.734944</td>\n",
       "      <td>0.738056</td>\n",
       "      <td>0.754164</td>\n",
       "      <td>0.751236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323815</td>\n",
       "      <td>0.325828</td>\n",
       "      <td>0.316859</td>\n",
       "      <td>0.312283</td>\n",
       "      <td>0.318872</td>\n",
       "      <td>0.308805</td>\n",
       "      <td>0.307889</td>\n",
       "      <td>0.307706</td>\n",
       "      <td>0.312649</td>\n",
       "      <td>0.313381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>ZVRA</td>\n",
       "      <td>0.948343</td>\n",
       "      <td>0.951267</td>\n",
       "      <td>0.950292</td>\n",
       "      <td>0.952242</td>\n",
       "      <td>0.916179</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.949318</td>\n",
       "      <td>0.983431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482943</td>\n",
       "      <td>0.469786</td>\n",
       "      <td>0.460039</td>\n",
       "      <td>0.448343</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.458090</td>\n",
       "      <td>0.450292</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.464912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>ZYME</td>\n",
       "      <td>0.984580</td>\n",
       "      <td>0.974942</td>\n",
       "      <td>0.953739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925598</td>\n",
       "      <td>0.878951</td>\n",
       "      <td>0.890517</td>\n",
       "      <td>0.900540</td>\n",
       "      <td>0.886662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240941</td>\n",
       "      <td>0.244410</td>\n",
       "      <td>0.238628</td>\n",
       "      <td>0.238242</td>\n",
       "      <td>0.239784</td>\n",
       "      <td>0.241326</td>\n",
       "      <td>0.246723</td>\n",
       "      <td>0.242097</td>\n",
       "      <td>0.248651</td>\n",
       "      <td>0.250385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>ZYNE</td>\n",
       "      <td>0.955814</td>\n",
       "      <td>0.958139</td>\n",
       "      <td>0.960465</td>\n",
       "      <td>0.944186</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.960465</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.296512</td>\n",
       "      <td>0.295349</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.281395</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.295349</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.302326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>ZYXI</td>\n",
       "      <td>0.603695</td>\n",
       "      <td>0.618146</td>\n",
       "      <td>0.659891</td>\n",
       "      <td>0.645976</td>\n",
       "      <td>0.645440</td>\n",
       "      <td>0.670594</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.659891</td>\n",
       "      <td>0.659355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491071</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.480357</td>\n",
       "      <td>0.477381</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.490476</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2649 rows × 504 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  2021-10-12  2021-10-13  2021-10-14  2021-10-15  2021-10-18  \\\n",
       "0     AADI    0.958538    0.950808    0.940970    0.956079    0.921644   \n",
       "1      AAL    0.911500    0.880952    0.882300    0.897574    0.891285   \n",
       "2     AAME    0.971175    0.955654    0.975610    0.960089    0.942350   \n",
       "3     AAOI    0.436047    0.465116    0.474160    0.466408    0.462532   \n",
       "4     AAON    0.619085    0.621760    0.639195    0.645745    0.653586   \n",
       "...    ...         ...         ...         ...         ...         ...   \n",
       "2644  ZUMZ    0.740436    0.730551    0.730368    0.723961    0.747392   \n",
       "2645  ZVRA    0.948343    0.951267    0.950292    0.952242    0.916179   \n",
       "2646  ZYME    0.984580    0.974942    0.953739    1.000000    0.925598   \n",
       "2647  ZYNE    0.955814    0.958139    0.960465    0.944186    0.906977   \n",
       "2648  ZYXI    0.603695    0.618146    0.659891    0.645976    0.645440   \n",
       "\n",
       "      2021-10-19  2021-10-20  2021-10-21  2021-10-22  ...  2023-09-28  \\\n",
       "0       0.936753    0.975755    0.943429    0.951862  ...    0.174631   \n",
       "1       0.877359    0.876909    0.893531    0.860287  ...    0.580413   \n",
       "2       0.920177    0.942350    0.929047    0.906874  ...    0.414846   \n",
       "3       0.479328    0.478682    0.485788    0.484496  ...    0.711886   \n",
       "4       0.659213    0.650726    0.648051    0.652387  ...    0.810549   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2644    0.734944    0.738056    0.754164    0.751236  ...    0.323815   \n",
       "2645    0.944444    0.976608    0.949318    0.983431  ...    0.482943   \n",
       "2646    0.878951    0.890517    0.900540    0.886662  ...    0.240941   \n",
       "2647    0.934884    0.934884    0.960465    0.900000  ...    0.297674   \n",
       "2648    0.670594    0.672200    0.659891    0.659355  ...    0.491071   \n",
       "\n",
       "      2023-09-29  2023-10-02  2023-10-03  2023-10-04  2023-10-05  2023-10-06  \\\n",
       "0       0.170063    0.151089    0.141251    0.147576    0.153549    0.157414   \n",
       "1       0.575472    0.572776    0.552111    0.571878    0.577269    0.573226   \n",
       "2       0.435137    0.430628    0.423864    0.412591    0.412591    0.408082   \n",
       "3       0.708656    0.728036    0.666667    0.575581    0.589147    0.586563   \n",
       "4       0.801668    0.803923    0.781651    0.792787    0.789545    0.807448   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2644    0.325828    0.316859    0.312283    0.318872    0.308805    0.307889   \n",
       "2645    0.469786    0.460039    0.448343    0.447368    0.447368    0.458090   \n",
       "2646    0.244410    0.238628    0.238242    0.239784    0.241326    0.246723   \n",
       "2647    0.296512    0.295349    0.293023    0.281395    0.290698    0.297674   \n",
       "2648    0.476190    0.480357    0.477381    0.489286    0.489286    0.483333   \n",
       "\n",
       "      2023-10-09  2023-10-10  2023-10-11  \n",
       "0       0.149332    0.161630    0.153900  \n",
       "1       0.549865    0.550764    0.560647  \n",
       "2       0.394554    0.408082    0.410336  \n",
       "3       0.489987    0.481912    0.503230  \n",
       "4       0.813650    0.824786    0.832539  \n",
       "...          ...         ...         ...  \n",
       "2644    0.307706    0.312649    0.313381  \n",
       "2645    0.450292    0.462963    0.464912  \n",
       "2646    0.242097    0.248651    0.250385  \n",
       "2647    0.295349    0.302326    0.302326  \n",
       "2648    0.490476    0.495833    0.479167  \n",
       "\n",
       "[2649 rows x 504 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ラベルを数値に変換する\n",
    "le = LabelEncoder()\n",
    "df['encode_name'] = le.fit_transform(df['name'])\n",
    "df.insert(1, 'encode_name', df.pop('encode_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>encode_name</th>\n",
       "      <th>2021-10-12</th>\n",
       "      <th>2021-10-13</th>\n",
       "      <th>2021-10-14</th>\n",
       "      <th>2021-10-15</th>\n",
       "      <th>2021-10-18</th>\n",
       "      <th>2021-10-19</th>\n",
       "      <th>2021-10-20</th>\n",
       "      <th>2021-10-21</th>\n",
       "      <th>...</th>\n",
       "      <th>2023-09-28</th>\n",
       "      <th>2023-09-29</th>\n",
       "      <th>2023-10-02</th>\n",
       "      <th>2023-10-03</th>\n",
       "      <th>2023-10-04</th>\n",
       "      <th>2023-10-05</th>\n",
       "      <th>2023-10-06</th>\n",
       "      <th>2023-10-09</th>\n",
       "      <th>2023-10-10</th>\n",
       "      <th>2023-10-11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AADI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.950808</td>\n",
       "      <td>0.940970</td>\n",
       "      <td>0.956079</td>\n",
       "      <td>0.921644</td>\n",
       "      <td>0.936753</td>\n",
       "      <td>0.975755</td>\n",
       "      <td>0.943429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174631</td>\n",
       "      <td>0.170063</td>\n",
       "      <td>0.151089</td>\n",
       "      <td>0.141251</td>\n",
       "      <td>0.147576</td>\n",
       "      <td>0.153549</td>\n",
       "      <td>0.157414</td>\n",
       "      <td>0.149332</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>0.897574</td>\n",
       "      <td>0.891285</td>\n",
       "      <td>0.877359</td>\n",
       "      <td>0.876909</td>\n",
       "      <td>0.893531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580413</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.572776</td>\n",
       "      <td>0.552111</td>\n",
       "      <td>0.571878</td>\n",
       "      <td>0.577269</td>\n",
       "      <td>0.573226</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>0.550764</td>\n",
       "      <td>0.560647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>2</td>\n",
       "      <td>0.971175</td>\n",
       "      <td>0.955654</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.960089</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>0.920177</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>0.929047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414846</td>\n",
       "      <td>0.435137</td>\n",
       "      <td>0.430628</td>\n",
       "      <td>0.423864</td>\n",
       "      <td>0.412591</td>\n",
       "      <td>0.412591</td>\n",
       "      <td>0.408082</td>\n",
       "      <td>0.394554</td>\n",
       "      <td>0.408082</td>\n",
       "      <td>0.410336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>3</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.474160</td>\n",
       "      <td>0.466408</td>\n",
       "      <td>0.462532</td>\n",
       "      <td>0.479328</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>0.485788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711886</td>\n",
       "      <td>0.708656</td>\n",
       "      <td>0.728036</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.586563</td>\n",
       "      <td>0.489987</td>\n",
       "      <td>0.481912</td>\n",
       "      <td>0.503230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>4</td>\n",
       "      <td>0.619085</td>\n",
       "      <td>0.621760</td>\n",
       "      <td>0.639195</td>\n",
       "      <td>0.645745</td>\n",
       "      <td>0.653586</td>\n",
       "      <td>0.659213</td>\n",
       "      <td>0.650726</td>\n",
       "      <td>0.648051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810549</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>0.803923</td>\n",
       "      <td>0.781651</td>\n",
       "      <td>0.792787</td>\n",
       "      <td>0.789545</td>\n",
       "      <td>0.807448</td>\n",
       "      <td>0.813650</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.832539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>ZUMZ</td>\n",
       "      <td>2643</td>\n",
       "      <td>0.740436</td>\n",
       "      <td>0.730551</td>\n",
       "      <td>0.730368</td>\n",
       "      <td>0.723961</td>\n",
       "      <td>0.747392</td>\n",
       "      <td>0.734944</td>\n",
       "      <td>0.738056</td>\n",
       "      <td>0.754164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323815</td>\n",
       "      <td>0.325828</td>\n",
       "      <td>0.316859</td>\n",
       "      <td>0.312283</td>\n",
       "      <td>0.318872</td>\n",
       "      <td>0.308805</td>\n",
       "      <td>0.307889</td>\n",
       "      <td>0.307706</td>\n",
       "      <td>0.312649</td>\n",
       "      <td>0.313381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>ZVRA</td>\n",
       "      <td>2644</td>\n",
       "      <td>0.948343</td>\n",
       "      <td>0.951267</td>\n",
       "      <td>0.950292</td>\n",
       "      <td>0.952242</td>\n",
       "      <td>0.916179</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.976608</td>\n",
       "      <td>0.949318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482943</td>\n",
       "      <td>0.469786</td>\n",
       "      <td>0.460039</td>\n",
       "      <td>0.448343</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.458090</td>\n",
       "      <td>0.450292</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.464912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>ZYME</td>\n",
       "      <td>2645</td>\n",
       "      <td>0.984580</td>\n",
       "      <td>0.974942</td>\n",
       "      <td>0.953739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925598</td>\n",
       "      <td>0.878951</td>\n",
       "      <td>0.890517</td>\n",
       "      <td>0.900540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240941</td>\n",
       "      <td>0.244410</td>\n",
       "      <td>0.238628</td>\n",
       "      <td>0.238242</td>\n",
       "      <td>0.239784</td>\n",
       "      <td>0.241326</td>\n",
       "      <td>0.246723</td>\n",
       "      <td>0.242097</td>\n",
       "      <td>0.248651</td>\n",
       "      <td>0.250385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>ZYNE</td>\n",
       "      <td>2646</td>\n",
       "      <td>0.955814</td>\n",
       "      <td>0.958139</td>\n",
       "      <td>0.960465</td>\n",
       "      <td>0.944186</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>0.960465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.296512</td>\n",
       "      <td>0.295349</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.281395</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.295349</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.302326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>ZYXI</td>\n",
       "      <td>2647</td>\n",
       "      <td>0.603695</td>\n",
       "      <td>0.618146</td>\n",
       "      <td>0.659891</td>\n",
       "      <td>0.645976</td>\n",
       "      <td>0.645440</td>\n",
       "      <td>0.670594</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.659891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491071</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.480357</td>\n",
       "      <td>0.477381</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.490476</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2649 rows × 505 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  encode_name  2021-10-12  2021-10-13  2021-10-14  2021-10-15  \\\n",
       "0     AADI            0    0.958538    0.950808    0.940970    0.956079   \n",
       "1      AAL            1    0.911500    0.880952    0.882300    0.897574   \n",
       "2     AAME            2    0.971175    0.955654    0.975610    0.960089   \n",
       "3     AAOI            3    0.436047    0.465116    0.474160    0.466408   \n",
       "4     AAON            4    0.619085    0.621760    0.639195    0.645745   \n",
       "...    ...          ...         ...         ...         ...         ...   \n",
       "2644  ZUMZ         2643    0.740436    0.730551    0.730368    0.723961   \n",
       "2645  ZVRA         2644    0.948343    0.951267    0.950292    0.952242   \n",
       "2646  ZYME         2645    0.984580    0.974942    0.953739    1.000000   \n",
       "2647  ZYNE         2646    0.955814    0.958139    0.960465    0.944186   \n",
       "2648  ZYXI         2647    0.603695    0.618146    0.659891    0.645976   \n",
       "\n",
       "      2021-10-18  2021-10-19  2021-10-20  2021-10-21  ...  2023-09-28  \\\n",
       "0       0.921644    0.936753    0.975755    0.943429  ...    0.174631   \n",
       "1       0.891285    0.877359    0.876909    0.893531  ...    0.580413   \n",
       "2       0.942350    0.920177    0.942350    0.929047  ...    0.414846   \n",
       "3       0.462532    0.479328    0.478682    0.485788  ...    0.711886   \n",
       "4       0.653586    0.659213    0.650726    0.648051  ...    0.810549   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2644    0.747392    0.734944    0.738056    0.754164  ...    0.323815   \n",
       "2645    0.916179    0.944444    0.976608    0.949318  ...    0.482943   \n",
       "2646    0.925598    0.878951    0.890517    0.900540  ...    0.240941   \n",
       "2647    0.906977    0.934884    0.934884    0.960465  ...    0.297674   \n",
       "2648    0.645440    0.670594    0.672200    0.659891  ...    0.491071   \n",
       "\n",
       "      2023-09-29  2023-10-02  2023-10-03  2023-10-04  2023-10-05  2023-10-06  \\\n",
       "0       0.170063    0.151089    0.141251    0.147576    0.153549    0.157414   \n",
       "1       0.575472    0.572776    0.552111    0.571878    0.577269    0.573226   \n",
       "2       0.435137    0.430628    0.423864    0.412591    0.412591    0.408082   \n",
       "3       0.708656    0.728036    0.666667    0.575581    0.589147    0.586563   \n",
       "4       0.801668    0.803923    0.781651    0.792787    0.789545    0.807448   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2644    0.325828    0.316859    0.312283    0.318872    0.308805    0.307889   \n",
       "2645    0.469786    0.460039    0.448343    0.447368    0.447368    0.458090   \n",
       "2646    0.244410    0.238628    0.238242    0.239784    0.241326    0.246723   \n",
       "2647    0.296512    0.295349    0.293023    0.281395    0.290698    0.297674   \n",
       "2648    0.476190    0.480357    0.477381    0.489286    0.489286    0.483333   \n",
       "\n",
       "      2023-10-09  2023-10-10  2023-10-11  \n",
       "0       0.149332    0.161630    0.153900  \n",
       "1       0.549865    0.550764    0.560647  \n",
       "2       0.394554    0.408082    0.410336  \n",
       "3       0.489987    0.481912    0.503230  \n",
       "4       0.813650    0.824786    0.832539  \n",
       "...          ...         ...         ...  \n",
       "2644    0.307706    0.312649    0.313381  \n",
       "2645    0.450292    0.462963    0.464912  \n",
       "2646    0.242097    0.248651    0.250385  \n",
       "2647    0.295349    0.302326    0.302326  \n",
       "2648    0.490476    0.495833    0.479167  \n",
       "\n",
       "[2649 rows x 505 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03\n",
      "308\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[:, 2:]\n",
    "i_split = 0\n",
    "# 日付の特定\n",
    "for i in range(len(x.columns)):\n",
    "    date1 = '2023-01-01'\n",
    "\n",
    "    # 日付をdatetimeオブジェクトに変換する\n",
    "    date1_obj = datetime.strptime(x.columns[i], '%Y-%m-%d')\n",
    "    date2_obj = datetime.strptime(date1, '%Y-%m-%d')\n",
    "\n",
    "    if date1_obj > date2_obj:\n",
    "        print(x.columns[i])\n",
    "        print(i)\n",
    "        i_split = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, i_split-60:]\n",
    "label = df['encode_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2022-10-04</th>\n",
       "      <th>2022-10-05</th>\n",
       "      <th>2022-10-06</th>\n",
       "      <th>2022-10-07</th>\n",
       "      <th>2022-10-10</th>\n",
       "      <th>2022-10-11</th>\n",
       "      <th>2022-10-12</th>\n",
       "      <th>2022-10-13</th>\n",
       "      <th>2022-10-14</th>\n",
       "      <th>2022-10-17</th>\n",
       "      <th>...</th>\n",
       "      <th>2023-09-28</th>\n",
       "      <th>2023-09-29</th>\n",
       "      <th>2023-10-02</th>\n",
       "      <th>2023-10-03</th>\n",
       "      <th>2023-10-04</th>\n",
       "      <th>2023-10-05</th>\n",
       "      <th>2023-10-06</th>\n",
       "      <th>2023-10-09</th>\n",
       "      <th>2023-10-10</th>\n",
       "      <th>2023-10-11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.509487</td>\n",
       "      <td>0.503162</td>\n",
       "      <td>0.502460</td>\n",
       "      <td>0.478215</td>\n",
       "      <td>0.475404</td>\n",
       "      <td>0.474350</td>\n",
       "      <td>0.454322</td>\n",
       "      <td>0.452565</td>\n",
       "      <td>0.443078</td>\n",
       "      <td>0.444835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174631</td>\n",
       "      <td>0.170063</td>\n",
       "      <td>0.151089</td>\n",
       "      <td>0.141251</td>\n",
       "      <td>0.147576</td>\n",
       "      <td>0.153549</td>\n",
       "      <td>0.157414</td>\n",
       "      <td>0.149332</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.581761</td>\n",
       "      <td>0.578167</td>\n",
       "      <td>0.571878</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.541330</td>\n",
       "      <td>0.550764</td>\n",
       "      <td>0.570530</td>\n",
       "      <td>0.586253</td>\n",
       "      <td>0.588949</td>\n",
       "      <td>0.592992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580413</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.572776</td>\n",
       "      <td>0.552111</td>\n",
       "      <td>0.571878</td>\n",
       "      <td>0.577269</td>\n",
       "      <td>0.573226</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>0.550764</td>\n",
       "      <td>0.560647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.624273</td>\n",
       "      <td>0.624273</td>\n",
       "      <td>0.630961</td>\n",
       "      <td>0.613125</td>\n",
       "      <td>0.619813</td>\n",
       "      <td>0.615354</td>\n",
       "      <td>0.613125</td>\n",
       "      <td>0.622043</td>\n",
       "      <td>0.617584</td>\n",
       "      <td>0.601977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414846</td>\n",
       "      <td>0.435137</td>\n",
       "      <td>0.430628</td>\n",
       "      <td>0.423864</td>\n",
       "      <td>0.412591</td>\n",
       "      <td>0.412591</td>\n",
       "      <td>0.408082</td>\n",
       "      <td>0.394554</td>\n",
       "      <td>0.408082</td>\n",
       "      <td>0.410336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.189922</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.189276</td>\n",
       "      <td>0.179587</td>\n",
       "      <td>0.172481</td>\n",
       "      <td>0.169897</td>\n",
       "      <td>0.169897</td>\n",
       "      <td>0.176357</td>\n",
       "      <td>0.177003</td>\n",
       "      <td>0.180879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711886</td>\n",
       "      <td>0.708656</td>\n",
       "      <td>0.728036</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.586563</td>\n",
       "      <td>0.489987</td>\n",
       "      <td>0.481912</td>\n",
       "      <td>0.503230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.524005</td>\n",
       "      <td>0.524471</td>\n",
       "      <td>0.514233</td>\n",
       "      <td>0.494594</td>\n",
       "      <td>0.500737</td>\n",
       "      <td>0.511347</td>\n",
       "      <td>0.500085</td>\n",
       "      <td>0.513581</td>\n",
       "      <td>0.504646</td>\n",
       "      <td>0.517490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810549</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>0.803923</td>\n",
       "      <td>0.781651</td>\n",
       "      <td>0.792787</td>\n",
       "      <td>0.789545</td>\n",
       "      <td>0.807448</td>\n",
       "      <td>0.813650</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.832539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>0.408384</td>\n",
       "      <td>0.403258</td>\n",
       "      <td>0.406187</td>\n",
       "      <td>0.395021</td>\n",
       "      <td>0.389163</td>\n",
       "      <td>0.383855</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.395936</td>\n",
       "      <td>0.387150</td>\n",
       "      <td>0.398499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323815</td>\n",
       "      <td>0.325828</td>\n",
       "      <td>0.316859</td>\n",
       "      <td>0.312283</td>\n",
       "      <td>0.318872</td>\n",
       "      <td>0.308805</td>\n",
       "      <td>0.307889</td>\n",
       "      <td>0.307706</td>\n",
       "      <td>0.312649</td>\n",
       "      <td>0.313381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>0.624756</td>\n",
       "      <td>0.588694</td>\n",
       "      <td>0.594542</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>0.569201</td>\n",
       "      <td>0.571150</td>\n",
       "      <td>0.558480</td>\n",
       "      <td>0.583821</td>\n",
       "      <td>0.544834</td>\n",
       "      <td>0.547758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482943</td>\n",
       "      <td>0.469786</td>\n",
       "      <td>0.460039</td>\n",
       "      <td>0.448343</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.458090</td>\n",
       "      <td>0.450292</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.464912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>0.235544</td>\n",
       "      <td>0.235929</td>\n",
       "      <td>0.238628</td>\n",
       "      <td>0.227448</td>\n",
       "      <td>0.230146</td>\n",
       "      <td>0.206245</td>\n",
       "      <td>0.195066</td>\n",
       "      <td>0.188512</td>\n",
       "      <td>0.212799</td>\n",
       "      <td>0.221665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240941</td>\n",
       "      <td>0.244410</td>\n",
       "      <td>0.238628</td>\n",
       "      <td>0.238242</td>\n",
       "      <td>0.239784</td>\n",
       "      <td>0.241326</td>\n",
       "      <td>0.246723</td>\n",
       "      <td>0.242097</td>\n",
       "      <td>0.248651</td>\n",
       "      <td>0.250385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>0.176744</td>\n",
       "      <td>0.169767</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>0.172326</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.156279</td>\n",
       "      <td>0.158372</td>\n",
       "      <td>0.154186</td>\n",
       "      <td>0.159535</td>\n",
       "      <td>0.165581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.296512</td>\n",
       "      <td>0.295349</td>\n",
       "      <td>0.293023</td>\n",
       "      <td>0.281395</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.297674</td>\n",
       "      <td>0.295349</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.302326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>0.579167</td>\n",
       "      <td>0.566071</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.518452</td>\n",
       "      <td>0.533929</td>\n",
       "      <td>0.520238</td>\n",
       "      <td>0.543452</td>\n",
       "      <td>0.519643</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491071</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.480357</td>\n",
       "      <td>0.477381</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.490476</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.479167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2649 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2022-10-04  2022-10-05  2022-10-06  2022-10-07  2022-10-10  2022-10-11  \\\n",
       "0       0.509487    0.503162    0.502460    0.478215    0.475404    0.474350   \n",
       "1       0.581761    0.578167    0.571878    0.547170    0.541330    0.550764   \n",
       "2       0.624273    0.624273    0.630961    0.613125    0.619813    0.615354   \n",
       "3       0.189922    0.194444    0.189276    0.179587    0.172481    0.169897   \n",
       "4       0.524005    0.524471    0.514233    0.494594    0.500737    0.511347   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2644    0.408384    0.403258    0.406187    0.395021    0.389163    0.383855   \n",
       "2645    0.624756    0.588694    0.594542    0.565302    0.569201    0.571150   \n",
       "2646    0.235544    0.235929    0.238628    0.227448    0.230146    0.206245   \n",
       "2647    0.176744    0.169767    0.191860    0.172326    0.162791    0.156279   \n",
       "2648    0.579167    0.566071    0.553571    0.523810    0.518452    0.533929   \n",
       "\n",
       "      2022-10-12  2022-10-13  2022-10-14  2022-10-17  ...  2023-09-28  \\\n",
       "0       0.454322    0.452565    0.443078    0.444835  ...    0.174631   \n",
       "1       0.570530    0.586253    0.588949    0.592992  ...    0.580413   \n",
       "2       0.613125    0.622043    0.617584    0.601977  ...    0.414846   \n",
       "3       0.169897    0.176357    0.177003    0.180879  ...    0.711886   \n",
       "4       0.500085    0.513581    0.504646    0.517490  ...    0.810549   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2644    0.387516    0.395936    0.387150    0.398499  ...    0.323815   \n",
       "2645    0.558480    0.583821    0.544834    0.547758  ...    0.482943   \n",
       "2646    0.195066    0.188512    0.212799    0.221665  ...    0.240941   \n",
       "2647    0.158372    0.154186    0.159535    0.165581  ...    0.297674   \n",
       "2648    0.520238    0.543452    0.519643    0.542857  ...    0.491071   \n",
       "\n",
       "      2023-09-29  2023-10-02  2023-10-03  2023-10-04  2023-10-05  2023-10-06  \\\n",
       "0       0.170063    0.151089    0.141251    0.147576    0.153549    0.157414   \n",
       "1       0.575472    0.572776    0.552111    0.571878    0.577269    0.573226   \n",
       "2       0.435137    0.430628    0.423864    0.412591    0.412591    0.408082   \n",
       "3       0.708656    0.728036    0.666667    0.575581    0.589147    0.586563   \n",
       "4       0.801668    0.803923    0.781651    0.792787    0.789545    0.807448   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2644    0.325828    0.316859    0.312283    0.318872    0.308805    0.307889   \n",
       "2645    0.469786    0.460039    0.448343    0.447368    0.447368    0.458090   \n",
       "2646    0.244410    0.238628    0.238242    0.239784    0.241326    0.246723   \n",
       "2647    0.296512    0.295349    0.293023    0.281395    0.290698    0.297674   \n",
       "2648    0.476190    0.480357    0.477381    0.489286    0.489286    0.483333   \n",
       "\n",
       "      2023-10-09  2023-10-10  2023-10-11  \n",
       "0       0.149332    0.161630    0.153900  \n",
       "1       0.549865    0.550764    0.560647  \n",
       "2       0.394554    0.408082    0.410336  \n",
       "3       0.489987    0.481912    0.503230  \n",
       "4       0.813650    0.824786    0.832539  \n",
       "...          ...         ...         ...  \n",
       "2644    0.307706    0.312649    0.313381  \n",
       "2645    0.450292    0.462963    0.464912  \n",
       "2646    0.242097    0.248651    0.250385  \n",
       "2647    0.295349    0.302326    0.302326  \n",
       "2648    0.490476    0.495833    0.479167  \n",
       "\n",
       "[2649 rows x 257 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n",
      "(2649, 60)\n"
     ]
    }
   ],
   "source": [
    "num_cols = len(x.columns)\n",
    "grouped_cols = [x.iloc[:, i:i+59] for i in range(0, num_cols) if len(x.iloc[:, i:i+59].columns) == 59]\n",
    "# 各グループを表示\n",
    "grouped_cols2 = []\n",
    "for group in grouped_cols:\n",
    "    df_combined = pd.concat([label, group], axis=1, ignore_index=False)\n",
    "    grouped_cols2.append(df_combined)\n",
    "    print(df_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_cols = grouped_cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encode_name</th>\n",
       "      <th>2022-10-05</th>\n",
       "      <th>2022-10-06</th>\n",
       "      <th>2022-10-07</th>\n",
       "      <th>2022-10-10</th>\n",
       "      <th>2022-10-11</th>\n",
       "      <th>2022-10-12</th>\n",
       "      <th>2022-10-13</th>\n",
       "      <th>2022-10-14</th>\n",
       "      <th>2022-10-17</th>\n",
       "      <th>...</th>\n",
       "      <th>2022-12-14</th>\n",
       "      <th>2022-12-15</th>\n",
       "      <th>2022-12-16</th>\n",
       "      <th>2022-12-19</th>\n",
       "      <th>2022-12-20</th>\n",
       "      <th>2022-12-21</th>\n",
       "      <th>2022-12-22</th>\n",
       "      <th>2022-12-23</th>\n",
       "      <th>2022-12-27</th>\n",
       "      <th>2022-12-28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.503162</td>\n",
       "      <td>0.502460</td>\n",
       "      <td>0.478215</td>\n",
       "      <td>0.475404</td>\n",
       "      <td>0.474350</td>\n",
       "      <td>0.454322</td>\n",
       "      <td>0.452565</td>\n",
       "      <td>0.443078</td>\n",
       "      <td>0.444835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460998</td>\n",
       "      <td>0.446240</td>\n",
       "      <td>0.435699</td>\n",
       "      <td>0.424455</td>\n",
       "      <td>0.432186</td>\n",
       "      <td>0.454322</td>\n",
       "      <td>0.443429</td>\n",
       "      <td>0.439916</td>\n",
       "      <td>0.421293</td>\n",
       "      <td>0.422699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.578167</td>\n",
       "      <td>0.571878</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.541330</td>\n",
       "      <td>0.550764</td>\n",
       "      <td>0.570530</td>\n",
       "      <td>0.586253</td>\n",
       "      <td>0.588949</td>\n",
       "      <td>0.592992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599730</td>\n",
       "      <td>0.580863</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.560647</td>\n",
       "      <td>0.562893</td>\n",
       "      <td>0.585355</td>\n",
       "      <td>0.564241</td>\n",
       "      <td>0.570979</td>\n",
       "      <td>0.562893</td>\n",
       "      <td>0.553459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.624273</td>\n",
       "      <td>0.630961</td>\n",
       "      <td>0.613125</td>\n",
       "      <td>0.619813</td>\n",
       "      <td>0.615354</td>\n",
       "      <td>0.613125</td>\n",
       "      <td>0.622043</td>\n",
       "      <td>0.617584</td>\n",
       "      <td>0.601977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537320</td>\n",
       "      <td>0.537320</td>\n",
       "      <td>0.577452</td>\n",
       "      <td>0.552927</td>\n",
       "      <td>0.546239</td>\n",
       "      <td>0.539550</td>\n",
       "      <td>0.557386</td>\n",
       "      <td>0.539550</td>\n",
       "      <td>0.535091</td>\n",
       "      <td>0.519484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.189276</td>\n",
       "      <td>0.179587</td>\n",
       "      <td>0.172481</td>\n",
       "      <td>0.169897</td>\n",
       "      <td>0.169897</td>\n",
       "      <td>0.176357</td>\n",
       "      <td>0.177003</td>\n",
       "      <td>0.180879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137597</td>\n",
       "      <td>0.136951</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.137597</td>\n",
       "      <td>0.142765</td>\n",
       "      <td>0.137597</td>\n",
       "      <td>0.129199</td>\n",
       "      <td>0.125969</td>\n",
       "      <td>0.118217</td>\n",
       "      <td>0.112403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.524471</td>\n",
       "      <td>0.514233</td>\n",
       "      <td>0.494594</td>\n",
       "      <td>0.500737</td>\n",
       "      <td>0.511347</td>\n",
       "      <td>0.500085</td>\n",
       "      <td>0.513581</td>\n",
       "      <td>0.504646</td>\n",
       "      <td>0.517490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745863</td>\n",
       "      <td>0.731838</td>\n",
       "      <td>0.713792</td>\n",
       "      <td>0.710894</td>\n",
       "      <td>0.705097</td>\n",
       "      <td>0.720898</td>\n",
       "      <td>0.716410</td>\n",
       "      <td>0.709304</td>\n",
       "      <td>0.719028</td>\n",
       "      <td>0.706873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>2643</td>\n",
       "      <td>0.403258</td>\n",
       "      <td>0.406187</td>\n",
       "      <td>0.395021</td>\n",
       "      <td>0.389163</td>\n",
       "      <td>0.383855</td>\n",
       "      <td>0.387516</td>\n",
       "      <td>0.395936</td>\n",
       "      <td>0.387150</td>\n",
       "      <td>0.398499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0.383489</td>\n",
       "      <td>0.371408</td>\n",
       "      <td>0.352004</td>\n",
       "      <td>0.355299</td>\n",
       "      <td>0.369760</td>\n",
       "      <td>0.371591</td>\n",
       "      <td>0.384587</td>\n",
       "      <td>0.387150</td>\n",
       "      <td>0.388248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>2644</td>\n",
       "      <td>0.588694</td>\n",
       "      <td>0.594542</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>0.569201</td>\n",
       "      <td>0.571150</td>\n",
       "      <td>0.558480</td>\n",
       "      <td>0.583821</td>\n",
       "      <td>0.544834</td>\n",
       "      <td>0.547758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448343</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.439571</td>\n",
       "      <td>0.433723</td>\n",
       "      <td>0.435672</td>\n",
       "      <td>0.445419</td>\n",
       "      <td>0.427875</td>\n",
       "      <td>0.418129</td>\n",
       "      <td>0.401559</td>\n",
       "      <td>0.412281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>2645</td>\n",
       "      <td>0.235929</td>\n",
       "      <td>0.238628</td>\n",
       "      <td>0.227448</td>\n",
       "      <td>0.230146</td>\n",
       "      <td>0.206245</td>\n",
       "      <td>0.195066</td>\n",
       "      <td>0.188512</td>\n",
       "      <td>0.212799</td>\n",
       "      <td>0.221665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247494</td>\n",
       "      <td>0.247880</td>\n",
       "      <td>0.249036</td>\n",
       "      <td>0.261758</td>\n",
       "      <td>0.272938</td>\n",
       "      <td>0.337317</td>\n",
       "      <td>0.338859</td>\n",
       "      <td>0.309946</td>\n",
       "      <td>0.288743</td>\n",
       "      <td>0.288743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>2646</td>\n",
       "      <td>0.169767</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>0.172326</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.156279</td>\n",
       "      <td>0.158372</td>\n",
       "      <td>0.154186</td>\n",
       "      <td>0.159535</td>\n",
       "      <td>0.165581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.135116</td>\n",
       "      <td>0.146512</td>\n",
       "      <td>0.146512</td>\n",
       "      <td>0.132558</td>\n",
       "      <td>0.135116</td>\n",
       "      <td>0.125581</td>\n",
       "      <td>0.125581</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>2647</td>\n",
       "      <td>0.566071</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.518452</td>\n",
       "      <td>0.533929</td>\n",
       "      <td>0.520238</td>\n",
       "      <td>0.543452</td>\n",
       "      <td>0.519643</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.825595</td>\n",
       "      <td>0.792857</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.818452</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.822024</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.826191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2649 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      encode_name  2022-10-05  2022-10-06  2022-10-07  2022-10-10  2022-10-11  \\\n",
       "0               0    0.503162    0.502460    0.478215    0.475404    0.474350   \n",
       "1               1    0.578167    0.571878    0.547170    0.541330    0.550764   \n",
       "2               2    0.624273    0.630961    0.613125    0.619813    0.615354   \n",
       "3               3    0.194444    0.189276    0.179587    0.172481    0.169897   \n",
       "4               4    0.524471    0.514233    0.494594    0.500737    0.511347   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "2644         2643    0.403258    0.406187    0.395021    0.389163    0.383855   \n",
       "2645         2644    0.588694    0.594542    0.565302    0.569201    0.571150   \n",
       "2646         2645    0.235929    0.238628    0.227448    0.230146    0.206245   \n",
       "2647         2646    0.169767    0.191860    0.172326    0.162791    0.156279   \n",
       "2648         2647    0.566071    0.553571    0.523810    0.518452    0.533929   \n",
       "\n",
       "      2022-10-12  2022-10-13  2022-10-14  2022-10-17  ...  2022-12-14  \\\n",
       "0       0.454322    0.452565    0.443078    0.444835  ...    0.460998   \n",
       "1       0.570530    0.586253    0.588949    0.592992  ...    0.599730   \n",
       "2       0.613125    0.622043    0.617584    0.601977  ...    0.537320   \n",
       "3       0.169897    0.176357    0.177003    0.180879  ...    0.137597   \n",
       "4       0.500085    0.513581    0.504646    0.517490  ...    0.745863   \n",
       "...          ...         ...         ...         ...  ...         ...   \n",
       "2644    0.387516    0.395936    0.387150    0.398499  ...    0.411312   \n",
       "2645    0.558480    0.583821    0.544834    0.547758  ...    0.448343   \n",
       "2646    0.195066    0.188512    0.212799    0.221665  ...    0.247494   \n",
       "2647    0.158372    0.154186    0.159535    0.165581  ...    0.139535   \n",
       "2648    0.520238    0.543452    0.519643    0.542857  ...    0.850000   \n",
       "\n",
       "      2022-12-15  2022-12-16  2022-12-19  2022-12-20  2022-12-21  2022-12-22  \\\n",
       "0       0.446240    0.435699    0.424455    0.432186    0.454322    0.443429   \n",
       "1       0.580863    0.575472    0.560647    0.562893    0.585355    0.564241   \n",
       "2       0.537320    0.577452    0.552927    0.546239    0.539550    0.557386   \n",
       "3       0.136951    0.138889    0.137597    0.142765    0.137597    0.129199   \n",
       "4       0.731838    0.713792    0.710894    0.705097    0.720898    0.716410   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2644    0.383489    0.371408    0.352004    0.355299    0.369760    0.371591   \n",
       "2645    0.438596    0.439571    0.433723    0.435672    0.445419    0.427875   \n",
       "2646    0.247880    0.249036    0.261758    0.272938    0.337317    0.338859   \n",
       "2647    0.135116    0.146512    0.146512    0.132558    0.135116    0.125581   \n",
       "2648    0.825595    0.792857    0.808333    0.807143    0.818452    0.809524   \n",
       "\n",
       "      2022-12-23  2022-12-27  2022-12-28  \n",
       "0       0.439916    0.421293    0.422699  \n",
       "1       0.570979    0.562893    0.553459  \n",
       "2       0.539550    0.535091    0.519484  \n",
       "3       0.125969    0.118217    0.112403  \n",
       "4       0.709304    0.719028    0.706873  \n",
       "...          ...         ...         ...  \n",
       "2644    0.384587    0.387150    0.388248  \n",
       "2645    0.418129    0.401559    0.412281  \n",
       "2646    0.309946    0.288743    0.288743  \n",
       "2647    0.125581    0.116279    0.120000  \n",
       "2648    0.822024    0.819048    0.826191  \n",
       "\n",
       "[2649 rows x 60 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_cols[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-12-28'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_cols[1].columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # 損失関数と最適化関数\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion_2 = nn.HingeEmbeddingLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x, hidden0=None):        \n",
    "        # Forward pass\n",
    "        out, (hidden, cell) = self.lstm(x, hidden0)\n",
    "        # Index hidden state of last time step\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 60\n",
    "hidden_size = 4\n",
    "num_layers = 4\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルパスを指定\n",
    "file_path = '../predict_model/weights/'\n",
    "\n",
    "# ファイル名を取得\n",
    "file_names = os.listdir(file_path)\n",
    "\n",
    "# CSVファイルのみを取得\n",
    "weights = [f for f in file_names if f.endswith('.pth')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_weight_train_loss_0.15059_test_loss_1.82536_epoch_463.pth',\n",
       " 'model_weight_train_loss_0.15065_test_loss_1.89213_epoch_487.pth',\n",
       " 'model_weight_train_loss_0.15111_test_loss_0.74882_epoch_450.pth',\n",
       " 'model_weight_train_loss_0.15502_test_loss_1.85502_epoch_477.pth',\n",
       " 'model_weight_train_loss_0.15602_test_loss_1.30326_epoch_449.pth',\n",
       " 'model_weight_train_loss_0.15620_test_loss_0.85109_epoch_493.pth',\n",
       " 'model_weight_train_loss_0.15633_test_loss_0.76947_epoch_438.pth',\n",
       " 'model_weight_train_loss_0.15643_test_loss_0.55642_epoch_439.pth',\n",
       " 'model_weight_train_loss_0.15956_test_loss_1.60592_epoch_490.pth',\n",
       " 'model_weight_train_loss_0.16052_test_loss_1.27359_epoch_469.pth',\n",
       " 'model_weight_train_loss_0.16144_test_loss_1.89206_epoch_399.pth',\n",
       " 'model_weight_train_loss_0.16283_test_loss_0.66891_epoch_405.pth',\n",
       " 'model_weight_train_loss_0.16303_test_loss_0.83294_epoch_474.pth',\n",
       " 'model_weight_train_loss_0.16432_test_loss_0.25740_epoch_470.pth',\n",
       " 'model_weight_train_loss_0.16617_test_loss_1.25915_epoch_377.pth',\n",
       " 'model_weight_train_loss_0.16627_test_loss_0.42238_epoch_495.pth',\n",
       " 'model_weight_train_loss_0.16667_test_loss_0.27044_epoch_482.pth',\n",
       " 'model_weight_train_loss_0.16715_test_loss_0.30849_epoch_471.pth',\n",
       " 'model_weight_train_loss_0.16722_test_loss_1.33784_epoch_389.pth',\n",
       " 'model_weight_train_loss_0.16727_test_loss_0.24735_epoch_441.pth',\n",
       " 'model_weight_train_loss_0.16766_test_loss_0.38654_epoch_413.pth',\n",
       " 'model_weight_train_loss_0.16804_test_loss_1.59972_epoch_355.pth',\n",
       " 'model_weight_train_loss_0.16883_test_loss_1.42005_epoch_339.pth',\n",
       " 'model_weight_train_loss_0.16929_test_loss_0.20557_epoch_472.pth',\n",
       " 'model_weight_train_loss_0.17330_test_loss_0.28548_epoch_476.pth',\n",
       " 'model_weight_train_loss_0.17331_test_loss_1.09604_epoch_465.pth',\n",
       " 'model_weight_train_loss_0.17332_test_loss_0.57809_epoch_436.pth',\n",
       " 'model_weight_train_loss_0.17504_test_loss_0.31108_epoch_494.pth',\n",
       " 'model_weight_train_loss_0.17573_test_loss_1.63225_epoch_457.pth',\n",
       " 'model_weight_train_loss_0.17578_test_loss_0.33115_epoch_382.pth',\n",
       " 'model_weight_train_loss_0.17580_test_loss_0.33254_epoch_483.pth',\n",
       " 'model_weight_train_loss_0.17584_test_loss_1.63883_epoch_401.pth',\n",
       " 'model_weight_train_loss_0.17677_test_loss_0.45576_epoch_369.pth',\n",
       " 'model_weight_train_loss_0.17694_test_loss_0.32986_epoch_460.pth',\n",
       " 'model_weight_train_loss_0.17714_test_loss_0.34178_epoch_373.pth',\n",
       " 'model_weight_train_loss_0.17735_test_loss_1.46205_epoch_398.pth',\n",
       " 'model_weight_train_loss_0.17769_test_loss_0.44718_epoch_362.pth',\n",
       " 'model_weight_train_loss_0.17796_test_loss_1.38930_epoch_467.pth',\n",
       " 'model_weight_train_loss_0.17887_test_loss_1.03425_epoch_264.pth',\n",
       " 'model_weight_train_loss_0.17964_test_loss_2.01542_epoch_430.pth',\n",
       " 'model_weight_train_loss_0.18014_test_loss_1.77734_epoch_480.pth',\n",
       " 'model_weight_train_loss_0.18077_test_loss_1.35042_epoch_248.pth',\n",
       " 'model_weight_train_loss_0.18095_test_loss_1.53062_epoch_356.pth',\n",
       " 'model_weight_train_loss_0.18147_test_loss_0.63483_epoch_372.pth',\n",
       " 'model_weight_train_loss_0.18148_test_loss_0.81429_epoch_434.pth',\n",
       " 'model_weight_train_loss_0.18159_test_loss_0.77834_epoch_485.pth',\n",
       " 'model_weight_train_loss_0.18165_test_loss_1.20214_epoch_343.pth',\n",
       " 'model_weight_train_loss_0.18180_test_loss_0.27154_epoch_499.pth',\n",
       " 'model_weight_train_loss_0.18254_test_loss_0.90188_epoch_448.pth',\n",
       " 'model_weight_train_loss_0.18277_test_loss_0.41338_epoch_456.pth',\n",
       " 'model_weight_train_loss_0.18298_test_loss_0.28976_epoch_435.pth',\n",
       " 'model_weight_train_loss_0.18316_test_loss_0.36128_epoch_298.pth',\n",
       " 'model_weight_train_loss_0.18352_test_loss_0.28523_epoch_466.pth',\n",
       " 'model_weight_train_loss_0.18363_test_loss_1.15433_epoch_446.pth',\n",
       " 'model_weight_train_loss_0.18387_test_loss_0.58427_epoch_497.pth',\n",
       " 'model_weight_train_loss_0.18407_test_loss_0.25398_epoch_309.pth',\n",
       " 'model_weight_train_loss_0.18408_test_loss_0.58942_epoch_347.pth',\n",
       " 'model_weight_train_loss_0.18442_test_loss_1.57655_epoch_488.pth',\n",
       " 'model_weight_train_loss_0.18454_test_loss_1.12972_epoch_292.pth',\n",
       " 'model_weight_train_loss_0.18501_test_loss_1.09530_epoch_437.pth',\n",
       " 'model_weight_train_loss_0.18531_test_loss_1.05950_epoch_462.pth',\n",
       " 'model_weight_train_loss_0.18561_test_loss_0.19501_epoch_444.pth',\n",
       " 'model_weight_train_loss_0.18561_test_loss_1.48431_epoch_340.pth',\n",
       " 'model_weight_train_loss_0.18608_test_loss_1.63562_epoch_386.pth',\n",
       " 'model_weight_train_loss_0.18662_test_loss_0.68342_epoch_455.pth',\n",
       " 'model_weight_train_loss_0.18724_test_loss_1.19895_epoch_492.pth',\n",
       " 'model_weight_train_loss_0.18767_test_loss_0.49952_epoch_425.pth',\n",
       " 'model_weight_train_loss_0.18782_test_loss_0.74443_epoch_244.pth',\n",
       " 'model_weight_train_loss_0.18790_test_loss_0.23792_epoch_363.pth',\n",
       " 'model_weight_train_loss_0.18835_test_loss_0.21317_epoch_443.pth',\n",
       " 'model_weight_train_loss_0.18899_test_loss_0.30696_epoch_263.pth',\n",
       " 'model_weight_train_loss_0.18958_test_loss_2.04870_epoch_388.pth',\n",
       " 'model_weight_train_loss_0.18966_test_loss_0.25013_epoch_321.pth',\n",
       " 'model_weight_train_loss_0.19022_test_loss_1.92339_epoch_360.pth',\n",
       " 'model_weight_train_loss_0.19044_test_loss_0.78391_epoch_338.pth',\n",
       " 'model_weight_train_loss_0.19092_test_loss_0.23081_epoch_402.pth',\n",
       " 'model_weight_train_loss_0.19115_test_loss_1.20406_epoch_445.pth',\n",
       " 'model_weight_train_loss_0.19118_test_loss_0.45280_epoch_479.pth',\n",
       " 'model_weight_train_loss_0.19151_test_loss_0.49406_epoch_496.pth',\n",
       " 'model_weight_train_loss_0.19260_test_loss_1.56007_epoch_307.pth',\n",
       " 'model_weight_train_loss_0.19296_test_loss_0.89791_epoch_361.pth',\n",
       " 'model_weight_train_loss_0.19302_test_loss_0.32982_epoch_486.pth',\n",
       " 'model_weight_train_loss_0.19307_test_loss_1.28930_epoch_400.pth',\n",
       " 'model_weight_train_loss_0.19331_test_loss_1.27728_epoch_391.pth',\n",
       " 'model_weight_train_loss_0.19374_test_loss_0.50006_epoch_473.pth',\n",
       " 'model_weight_train_loss_0.19385_test_loss_0.29461_epoch_491.pth',\n",
       " 'model_weight_train_loss_0.19416_test_loss_1.14848_epoch_359.pth',\n",
       " 'model_weight_train_loss_0.19452_test_loss_1.12618_epoch_432.pth',\n",
       " 'model_weight_train_loss_0.19471_test_loss_0.57293_epoch_385.pth',\n",
       " 'model_weight_train_loss_0.19486_test_loss_0.46679_epoch_451.pth',\n",
       " 'model_weight_train_loss_0.19514_test_loss_1.90683_epoch_397.pth',\n",
       " 'model_weight_train_loss_0.19600_test_loss_1.46092_epoch_284.pth',\n",
       " 'model_weight_train_loss_0.19631_test_loss_0.46623_epoch_478.pth',\n",
       " 'model_weight_train_loss_0.19633_test_loss_1.61054_epoch_394.pth',\n",
       " 'model_weight_train_loss_0.19639_test_loss_1.82200_epoch_317.pth',\n",
       " 'model_weight_train_loss_0.19660_test_loss_0.72080_epoch_380.pth',\n",
       " 'model_weight_train_loss_0.19721_test_loss_1.70345_epoch_383.pth',\n",
       " 'model_weight_train_loss_0.19723_test_loss_0.45217_epoch_348.pth',\n",
       " 'model_weight_train_loss_0.19788_test_loss_1.88969_epoch_379.pth',\n",
       " 'model_weight_train_loss_0.19790_test_loss_1.01101_epoch_481.pth',\n",
       " 'model_weight_train_loss_0.19831_test_loss_0.51100_epoch_453.pth',\n",
       " 'model_weight_train_loss_0.19882_test_loss_0.64734_epoch_305.pth',\n",
       " 'model_weight_train_loss_0.19922_test_loss_1.48781_epoch_329.pth',\n",
       " 'model_weight_train_loss_0.19934_test_loss_1.32222_epoch_408.pth',\n",
       " 'model_weight_train_loss_0.19959_test_loss_0.27562_epoch_370.pth',\n",
       " 'model_weight_train_loss_0.19962_test_loss_1.00676_epoch_311.pth',\n",
       " 'model_weight_train_loss_0.19964_test_loss_0.26393_epoch_366.pth',\n",
       " 'model_weight_train_loss_0.19987_test_loss_0.58861_epoch_350.pth',\n",
       " 'model_weight_train_loss_0.20036_test_loss_0.60258_epoch_273.pth',\n",
       " 'model_weight_train_loss_0.20057_test_loss_1.52735_epoch_468.pth',\n",
       " 'model_weight_train_loss_0.20077_test_loss_1.68076_epoch_262.pth',\n",
       " 'model_weight_train_loss_0.20089_test_loss_0.35354_epoch_308.pth',\n",
       " 'model_weight_train_loss_0.20104_test_loss_0.43907_epoch_498.pth',\n",
       " 'model_weight_train_loss_0.20145_test_loss_1.47170_epoch_306.pth',\n",
       " 'model_weight_train_loss_0.20175_test_loss_0.75307_epoch_270.pth',\n",
       " 'model_weight_train_loss_0.20201_test_loss_1.64272_epoch_484.pth',\n",
       " 'model_weight_train_loss_0.20224_test_loss_1.06227_epoch_375.pth',\n",
       " 'model_weight_train_loss_0.20284_test_loss_0.23365_epoch_423.pth',\n",
       " 'model_weight_train_loss_0.20286_test_loss_0.50979_epoch_304.pth',\n",
       " 'model_weight_train_loss_0.20339_test_loss_0.66844_epoch_352.pth',\n",
       " 'model_weight_train_loss_0.20375_test_loss_0.40449_epoch_303.pth',\n",
       " 'model_weight_train_loss_0.20378_test_loss_0.80621_epoch_392.pth',\n",
       " 'model_weight_train_loss_0.20405_test_loss_1.62335_epoch_335.pth',\n",
       " 'model_weight_train_loss_0.20471_test_loss_1.56896_epoch_407.pth',\n",
       " 'model_weight_train_loss_0.20569_test_loss_0.34815_epoch_376.pth',\n",
       " 'model_weight_train_loss_0.20645_test_loss_0.79456_epoch_364.pth',\n",
       " 'model_weight_train_loss_0.20658_test_loss_0.67886_epoch_230.pth',\n",
       " 'model_weight_train_loss_0.20805_test_loss_1.46101_epoch_299.pth',\n",
       " 'model_weight_train_loss_0.20867_test_loss_1.14527_epoch_261.pth',\n",
       " 'model_weight_train_loss_0.20918_test_loss_0.61718_epoch_342.pth',\n",
       " 'model_weight_train_loss_0.20938_test_loss_1.25307_epoch_337.pth',\n",
       " 'model_weight_train_loss_0.20948_test_loss_0.71041_epoch_390.pth',\n",
       " 'model_weight_train_loss_0.20966_test_loss_1.71469_epoch_406.pth',\n",
       " 'model_weight_train_loss_0.21006_test_loss_0.93769_epoch_404.pth',\n",
       " 'model_weight_train_loss_0.21036_test_loss_1.33760_epoch_378.pth',\n",
       " 'model_weight_train_loss_0.21040_test_loss_1.87083_epoch_333.pth',\n",
       " 'model_weight_train_loss_0.21048_test_loss_0.45520_epoch_433.pth',\n",
       " 'model_weight_train_loss_0.21059_test_loss_0.47105_epoch_409.pth',\n",
       " 'model_weight_train_loss_0.21072_test_loss_0.63442_epoch_279.pth',\n",
       " 'model_weight_train_loss_0.21112_test_loss_0.37534_epoch_414.pth',\n",
       " 'model_weight_train_loss_0.21159_test_loss_0.81819_epoch_310.pth',\n",
       " 'model_weight_train_loss_0.21165_test_loss_1.57079_epoch_327.pth',\n",
       " 'model_weight_train_loss_0.21166_test_loss_0.39357_epoch_454.pth',\n",
       " 'model_weight_train_loss_0.21179_test_loss_1.71863_epoch_367.pth',\n",
       " 'model_weight_train_loss_0.21248_test_loss_1.42298_epoch_291.pth',\n",
       " 'model_weight_train_loss_0.21301_test_loss_0.60623_epoch_371.pth',\n",
       " 'model_weight_train_loss_0.21473_test_loss_0.42352_epoch_221.pth',\n",
       " 'model_weight_train_loss_0.21615_test_loss_1.69807_epoch_191.pth',\n",
       " 'model_weight_train_loss_0.21635_test_loss_1.02828_epoch_349.pth',\n",
       " 'model_weight_train_loss_0.21666_test_loss_0.78044_epoch_357.pth',\n",
       " 'model_weight_train_loss_0.21749_test_loss_0.62643_epoch_387.pth',\n",
       " 'model_weight_train_loss_0.21768_test_loss_0.28657_epoch_212.pth',\n",
       " 'model_weight_train_loss_0.21798_test_loss_0.39619_epoch_205.pth',\n",
       " 'model_weight_train_loss_0.21806_test_loss_0.63388_epoch_289.pth',\n",
       " 'model_weight_train_loss_0.21892_test_loss_0.56887_epoch_447.pth',\n",
       " 'model_weight_train_loss_0.21917_test_loss_0.26568_epoch_415.pth',\n",
       " 'model_weight_train_loss_0.21918_test_loss_1.01498_epoch_239.pth',\n",
       " 'model_weight_train_loss_0.21921_test_loss_0.71226_epoch_260.pth',\n",
       " 'model_weight_train_loss_0.21937_test_loss_1.27437_epoch_440.pth',\n",
       " 'model_weight_train_loss_0.21946_test_loss_1.08049_epoch_240.pth',\n",
       " 'model_weight_train_loss_0.22138_test_loss_1.46368_epoch_268.pth',\n",
       " 'model_weight_train_loss_0.22151_test_loss_1.58850_epoch_324.pth',\n",
       " 'model_weight_train_loss_0.22172_test_loss_0.79514_epoch_331.pth',\n",
       " 'model_weight_train_loss_0.22242_test_loss_1.16518_epoch_253.pth',\n",
       " 'model_weight_train_loss_0.22275_test_loss_1.79580_epoch_271.pth',\n",
       " 'model_weight_train_loss_0.22304_test_loss_1.81407_epoch_269.pth',\n",
       " 'model_weight_train_loss_0.22357_test_loss_0.87632_epoch_316.pth',\n",
       " 'model_weight_train_loss_0.22413_test_loss_1.89530_epoch_219.pth',\n",
       " 'model_weight_train_loss_0.22417_test_loss_0.27976_epoch_353.pth',\n",
       " 'model_weight_train_loss_0.22470_test_loss_1.08478_epoch_245.pth',\n",
       " 'model_weight_train_loss_0.22480_test_loss_0.36723_epoch_189.pth',\n",
       " 'model_weight_train_loss_0.22501_test_loss_0.38920_epoch_431.pth',\n",
       " 'model_weight_train_loss_0.22510_test_loss_0.43005_epoch_168.pth',\n",
       " 'model_weight_train_loss_0.22625_test_loss_1.63142_epoch_197.pth',\n",
       " 'model_weight_train_loss_0.22638_test_loss_1.18198_epoch_351.pth',\n",
       " 'model_weight_train_loss_0.22720_test_loss_0.56201_epoch_336.pth',\n",
       " 'model_weight_train_loss_0.22797_test_loss_0.86262_epoch_442.pth',\n",
       " 'model_weight_train_loss_0.22857_test_loss_0.33756_epoch_222.pth',\n",
       " 'model_weight_train_loss_0.22884_test_loss_1.74583_epoch_210.pth',\n",
       " 'model_weight_train_loss_0.22885_test_loss_0.92755_epoch_174.pth',\n",
       " 'model_weight_train_loss_0.22937_test_loss_0.44490_epoch_169.pth',\n",
       " 'model_weight_train_loss_0.23014_test_loss_1.10882_epoch_315.pth',\n",
       " 'model_weight_train_loss_0.23036_test_loss_1.13676_epoch_294.pth',\n",
       " 'model_weight_train_loss_0.23101_test_loss_0.26421_epoch_429.pth',\n",
       " 'model_weight_train_loss_0.23112_test_loss_1.19992_epoch_203.pth',\n",
       " 'model_weight_train_loss_0.23139_test_loss_1.72036_epoch_211.pth',\n",
       " 'model_weight_train_loss_0.23297_test_loss_1.05350_epoch_256.pth',\n",
       " 'model_weight_train_loss_0.23324_test_loss_0.88306_epoch_428.pth',\n",
       " 'model_weight_train_loss_0.23324_test_loss_1.36953_epoch_184.pth',\n",
       " 'model_weight_train_loss_0.23351_test_loss_1.11981_epoch_265.pth',\n",
       " 'model_weight_train_loss_0.23370_test_loss_0.89935_epoch_334.pth',\n",
       " 'model_weight_train_loss_0.23465_test_loss_1.41227_epoch_358.pth',\n",
       " 'model_weight_train_loss_0.23554_test_loss_1.35424_epoch_227.pth',\n",
       " 'model_weight_train_loss_0.23581_test_loss_0.54522_epoch_235.pth',\n",
       " 'model_weight_train_loss_0.23606_test_loss_0.62153_epoch_296.pth',\n",
       " 'model_weight_train_loss_0.23620_test_loss_1.17604_epoch_475.pth',\n",
       " 'model_weight_train_loss_0.23626_test_loss_1.44315_epoch_182.pth',\n",
       " 'model_weight_train_loss_0.23677_test_loss_1.35155_epoch_426.pth',\n",
       " 'model_weight_train_loss_0.23681_test_loss_1.17361_epoch_346.pth',\n",
       " 'model_weight_train_loss_0.23701_test_loss_0.83956_epoch_396.pth',\n",
       " 'model_weight_train_loss_0.23776_test_loss_1.06569_epoch_201.pth',\n",
       " 'model_weight_train_loss_0.23913_test_loss_0.63448_epoch_192.pth',\n",
       " 'model_weight_train_loss_0.23932_test_loss_0.34865_epoch_458.pth',\n",
       " 'model_weight_train_loss_0.23938_test_loss_1.82115_epoch_252.pth',\n",
       " 'model_weight_train_loss_0.23952_test_loss_1.07208_epoch_159.pth',\n",
       " 'model_weight_train_loss_0.24018_test_loss_1.13852_epoch_278.pth',\n",
       " 'model_weight_train_loss_0.24061_test_loss_1.12030_epoch_410.pth',\n",
       " 'model_weight_train_loss_0.24156_test_loss_0.95724_epoch_489.pth',\n",
       " 'model_weight_train_loss_0.24190_test_loss_0.76387_epoch_224.pth',\n",
       " 'model_weight_train_loss_0.24236_test_loss_0.63234_epoch_258.pth',\n",
       " 'model_weight_train_loss_0.24276_test_loss_0.90233_epoch_172.pth',\n",
       " 'model_weight_train_loss_0.24402_test_loss_1.55783_epoch_202.pth',\n",
       " 'model_weight_train_loss_0.24521_test_loss_0.37984_epoch_419.pth',\n",
       " 'model_weight_train_loss_0.24547_test_loss_0.62862_epoch_209.pth',\n",
       " 'model_weight_train_loss_0.24641_test_loss_0.95173_epoch_243.pth',\n",
       " 'model_weight_train_loss_0.24649_test_loss_0.48255_epoch_188.pth',\n",
       " 'model_weight_train_loss_0.24704_test_loss_0.36965_epoch_267.pth',\n",
       " 'model_weight_train_loss_0.24723_test_loss_1.24685_epoch_459.pth',\n",
       " 'model_weight_train_loss_0.24735_test_loss_0.89381_epoch_295.pth',\n",
       " 'model_weight_train_loss_0.24759_test_loss_1.03867_epoch_250.pth',\n",
       " 'model_weight_train_loss_0.24766_test_loss_0.67936_epoch_318.pth',\n",
       " 'model_weight_train_loss_0.24776_test_loss_1.92494_epoch_395.pth',\n",
       " 'model_weight_train_loss_0.24818_test_loss_1.66354_epoch_301.pth',\n",
       " 'model_weight_train_loss_0.24860_test_loss_0.95798_epoch_196.pth',\n",
       " 'model_weight_train_loss_0.24863_test_loss_0.69522_epoch_233.pth',\n",
       " 'model_weight_train_loss_0.24873_test_loss_0.66945_epoch_452.pth',\n",
       " 'model_weight_train_loss_0.24903_test_loss_1.41682_epoch_374.pth',\n",
       " 'model_weight_train_loss_0.24926_test_loss_1.36784_epoch_193.pth',\n",
       " 'model_weight_train_loss_0.25009_test_loss_1.50401_epoch_207.pth',\n",
       " 'model_weight_train_loss_0.25032_test_loss_1.27174_epoch_220.pth',\n",
       " 'model_weight_train_loss_0.25099_test_loss_0.52990_epoch_238.pth',\n",
       " 'model_weight_train_loss_0.25171_test_loss_0.61046_epoch_173.pth',\n",
       " 'model_weight_train_loss_0.25174_test_loss_0.79968_epoch_226.pth',\n",
       " 'model_weight_train_loss_0.25414_test_loss_1.32122_epoch_145.pth',\n",
       " 'model_weight_train_loss_0.25470_test_loss_1.48526_epoch_254.pth',\n",
       " 'model_weight_train_loss_0.25484_test_loss_0.81809_epoch_283.pth',\n",
       " 'model_weight_train_loss_0.25519_test_loss_1.66411_epoch_204.pth',\n",
       " 'model_weight_train_loss_0.25595_test_loss_0.56736_epoch_249.pth',\n",
       " 'model_weight_train_loss_0.25605_test_loss_1.12501_epoch_325.pth',\n",
       " 'model_weight_train_loss_0.25623_test_loss_0.66772_epoch_215.pth',\n",
       " 'model_weight_train_loss_0.25641_test_loss_1.07982_epoch_417.pth',\n",
       " 'model_weight_train_loss_0.25651_test_loss_0.44372_epoch_326.pth',\n",
       " 'model_weight_train_loss_0.25735_test_loss_0.55719_epoch_147.pth',\n",
       " 'model_weight_train_loss_0.25795_test_loss_0.62179_epoch_416.pth',\n",
       " 'model_weight_train_loss_0.25799_test_loss_0.51122_epoch_198.pth',\n",
       " 'model_weight_train_loss_0.25834_test_loss_1.45411_epoch_171.pth',\n",
       " 'model_weight_train_loss_0.25839_test_loss_1.63768_epoch_247.pth',\n",
       " 'model_weight_train_loss_0.25886_test_loss_0.76712_epoch_393.pth',\n",
       " 'model_weight_train_loss_0.25892_test_loss_0.59782_epoch_384.pth',\n",
       " 'model_weight_train_loss_0.26049_test_loss_1.07746_epoch_194.pth',\n",
       " 'model_weight_train_loss_0.26086_test_loss_0.55411_epoch_125.pth',\n",
       " 'model_weight_train_loss_0.26126_test_loss_0.60585_epoch_229.pth',\n",
       " 'model_weight_train_loss_0.26158_test_loss_1.55521_epoch_293.pth',\n",
       " 'model_weight_train_loss_0.26173_test_loss_0.62401_epoch_312.pth',\n",
       " 'model_weight_train_loss_0.26212_test_loss_0.83812_epoch_277.pth',\n",
       " 'model_weight_train_loss_0.26238_test_loss_0.40894_epoch_424.pth',\n",
       " 'model_weight_train_loss_0.26256_test_loss_1.03139_epoch_185.pth',\n",
       " 'model_weight_train_loss_0.26273_test_loss_0.80377_epoch_131.pth',\n",
       " 'model_weight_train_loss_0.26278_test_loss_0.39928_epoch_165.pth',\n",
       " 'model_weight_train_loss_0.26430_test_loss_1.38150_epoch_214.pth',\n",
       " 'model_weight_train_loss_0.26499_test_loss_1.03588_epoch_332.pth',\n",
       " 'model_weight_train_loss_0.26548_test_loss_1.60183_epoch_99.pth',\n",
       " 'model_weight_train_loss_0.26557_test_loss_0.50445_epoch_330.pth',\n",
       " 'model_weight_train_loss_0.26636_test_loss_0.59895_epoch_153.pth',\n",
       " 'model_weight_train_loss_0.26657_test_loss_0.30017_epoch_297.pth',\n",
       " 'model_weight_train_loss_0.26743_test_loss_0.68877_epoch_106.pth',\n",
       " 'model_weight_train_loss_0.26779_test_loss_0.81740_epoch_381.pth',\n",
       " 'model_weight_train_loss_0.26785_test_loss_1.52617_epoch_154.pth',\n",
       " 'model_weight_train_loss_0.26877_test_loss_1.19076_epoch_302.pth',\n",
       " 'model_weight_train_loss_0.26934_test_loss_0.78167_epoch_166.pth',\n",
       " 'model_weight_train_loss_0.26945_test_loss_1.26220_epoch_276.pth',\n",
       " 'model_weight_train_loss_0.26968_test_loss_1.56856_epoch_124.pth',\n",
       " 'model_weight_train_loss_0.26985_test_loss_1.59713_epoch_100.pth',\n",
       " 'model_weight_train_loss_0.27004_test_loss_0.42234_epoch_290.pth',\n",
       " 'model_weight_train_loss_0.27027_test_loss_0.96975_epoch_95.pth',\n",
       " 'model_weight_train_loss_0.27090_test_loss_0.43785_epoch_411.pth',\n",
       " 'model_weight_train_loss_0.27119_test_loss_0.57613_epoch_190.pth',\n",
       " 'model_weight_train_loss_0.27121_test_loss_0.95525_epoch_285.pth',\n",
       " 'model_weight_train_loss_0.27213_test_loss_0.83604_epoch_287.pth',\n",
       " 'model_weight_train_loss_0.27235_test_loss_0.72790_epoch_167.pth',\n",
       " 'model_weight_train_loss_0.27253_test_loss_0.50496_epoch_403.pth',\n",
       " 'model_weight_train_loss_0.27327_test_loss_0.50980_epoch_113.pth',\n",
       " 'model_weight_train_loss_0.27385_test_loss_0.64615_epoch_246.pth',\n",
       " 'model_weight_train_loss_0.27391_test_loss_0.77589_epoch_213.pth',\n",
       " 'model_weight_train_loss_0.27403_test_loss_0.85068_epoch_200.pth',\n",
       " 'model_weight_train_loss_0.27520_test_loss_0.49037_epoch_175.pth',\n",
       " 'model_weight_train_loss_0.27618_test_loss_1.36936_epoch_231.pth',\n",
       " 'model_weight_train_loss_0.27627_test_loss_0.40982_epoch_274.pth',\n",
       " 'model_weight_train_loss_0.27662_test_loss_0.51915_epoch_93.pth',\n",
       " 'model_weight_train_loss_0.27672_test_loss_0.61968_epoch_282.pth',\n",
       " 'model_weight_train_loss_0.27783_test_loss_1.85119_epoch_134.pth',\n",
       " 'model_weight_train_loss_0.27808_test_loss_1.62132_epoch_195.pth',\n",
       " 'model_weight_train_loss_0.27823_test_loss_0.59989_epoch_288.pth',\n",
       " 'model_weight_train_loss_0.27830_test_loss_0.21213_epoch_365.pth',\n",
       " 'model_weight_train_loss_0.27858_test_loss_0.43152_epoch_259.pth',\n",
       " 'model_weight_train_loss_0.27873_test_loss_1.46969_epoch_208.pth',\n",
       " 'model_weight_train_loss_0.27879_test_loss_0.61883_epoch_427.pth',\n",
       " 'model_weight_train_loss_0.27892_test_loss_0.52681_epoch_187.pth',\n",
       " 'model_weight_train_loss_0.27925_test_loss_0.96349_epoch_101.pth',\n",
       " 'model_weight_train_loss_0.27949_test_loss_1.73940_epoch_170.pth',\n",
       " 'model_weight_train_loss_0.27982_test_loss_0.49683_epoch_181.pth',\n",
       " 'model_weight_train_loss_0.28003_test_loss_1.60593_epoch_98.pth',\n",
       " 'model_weight_train_loss_0.28004_test_loss_1.09487_epoch_137.pth',\n",
       " 'model_weight_train_loss_0.28046_test_loss_0.33774_epoch_319.pth',\n",
       " 'model_weight_train_loss_0.28075_test_loss_0.42402_epoch_107.pth',\n",
       " 'model_weight_train_loss_0.28118_test_loss_0.50476_epoch_412.pth',\n",
       " 'model_weight_train_loss_0.28121_test_loss_1.66824_epoch_199.pth',\n",
       " 'model_weight_train_loss_0.28158_test_loss_0.75056_epoch_272.pth',\n",
       " 'model_weight_train_loss_0.28184_test_loss_1.44839_epoch_96.pth',\n",
       " 'model_weight_train_loss_0.28238_test_loss_0.54310_epoch_232.pth',\n",
       " 'model_weight_train_loss_0.28250_test_loss_0.73049_epoch_280.pth',\n",
       " 'model_weight_train_loss_0.28409_test_loss_1.72535_epoch_183.pth',\n",
       " 'model_weight_train_loss_0.28521_test_loss_1.23681_epoch_464.pth',\n",
       " 'model_weight_train_loss_0.28598_test_loss_0.95669_epoch_105.pth',\n",
       " 'model_weight_train_loss_0.28600_test_loss_1.11466_epoch_97.pth',\n",
       " 'model_weight_train_loss_0.28631_test_loss_0.49438_epoch_128.pth',\n",
       " 'model_weight_train_loss_0.28755_test_loss_1.62039_epoch_176.pth',\n",
       " 'model_weight_train_loss_0.28756_test_loss_1.28417_epoch_94.pth',\n",
       " 'model_weight_train_loss_0.28898_test_loss_0.41656_epoch_237.pth',\n",
       " 'model_weight_train_loss_0.28914_test_loss_1.67606_epoch_313.pth',\n",
       " 'model_weight_train_loss_0.28923_test_loss_1.17723_epoch_110.pth',\n",
       " 'model_weight_train_loss_0.28952_test_loss_1.32960_epoch_129.pth',\n",
       " 'model_weight_train_loss_0.29051_test_loss_0.43121_epoch_140.pth',\n",
       " 'model_weight_train_loss_0.29075_test_loss_1.18234_epoch_178.pth',\n",
       " 'model_weight_train_loss_0.29125_test_loss_0.82265_epoch_161.pth',\n",
       " 'model_weight_train_loss_0.29214_test_loss_1.61515_epoch_149.pth',\n",
       " 'model_weight_train_loss_0.29274_test_loss_0.68042_epoch_108.pth',\n",
       " 'model_weight_train_loss_0.29352_test_loss_0.48333_epoch_158.pth',\n",
       " 'model_weight_train_loss_0.29355_test_loss_0.78371_epoch_354.pth',\n",
       " 'model_weight_train_loss_0.29460_test_loss_1.44028_epoch_79.pth',\n",
       " 'model_weight_train_loss_0.29525_test_loss_1.58363_epoch_103.pth',\n",
       " 'model_weight_train_loss_0.29545_test_loss_1.35407_epoch_228.pth',\n",
       " 'model_weight_train_loss_0.29562_test_loss_0.67424_epoch_130.pth',\n",
       " 'model_weight_train_loss_0.29749_test_loss_0.81780_epoch_66.pth',\n",
       " 'model_weight_train_loss_0.29842_test_loss_1.46432_epoch_251.pth',\n",
       " 'model_weight_train_loss_0.29847_test_loss_1.50987_epoch_102.pth',\n",
       " 'model_weight_train_loss_0.29900_test_loss_0.95296_epoch_155.pth',\n",
       " 'model_weight_train_loss_0.29952_test_loss_0.73548_epoch_242.pth',\n",
       " 'model_weight_train_loss_0.30016_test_loss_0.76720_epoch_65.pth',\n",
       " 'model_weight_train_loss_0.30061_test_loss_0.58144_epoch_136.pth',\n",
       " 'model_weight_train_loss_0.30101_test_loss_1.03804_epoch_163.pth',\n",
       " 'model_weight_train_loss_0.30199_test_loss_0.62328_epoch_73.pth',\n",
       " 'model_weight_train_loss_0.30204_test_loss_0.77136_epoch_418.pth',\n",
       " 'model_weight_train_loss_0.30227_test_loss_0.91752_epoch_177.pth',\n",
       " 'model_weight_train_loss_0.30312_test_loss_0.90210_epoch_216.pth',\n",
       " 'model_weight_train_loss_0.30366_test_loss_0.41054_epoch_341.pth',\n",
       " 'model_weight_train_loss_0.30423_test_loss_0.53332_epoch_112.pth',\n",
       " 'model_weight_train_loss_0.30475_test_loss_1.69900_epoch_76.pth',\n",
       " 'model_weight_train_loss_0.30538_test_loss_1.15521_epoch_80.pth',\n",
       " 'model_weight_train_loss_0.30539_test_loss_0.71309_epoch_88.pth',\n",
       " 'model_weight_train_loss_0.30727_test_loss_0.45164_epoch_368.pth',\n",
       " 'model_weight_train_loss_0.30800_test_loss_0.62291_epoch_142.pth',\n",
       " 'model_weight_train_loss_0.30806_test_loss_1.18050_epoch_104.pth',\n",
       " 'model_weight_train_loss_0.30834_test_loss_1.51047_epoch_164.pth',\n",
       " 'model_weight_train_loss_0.30850_test_loss_1.52281_epoch_144.pth',\n",
       " 'model_weight_train_loss_0.30856_test_loss_1.26729_epoch_328.pth',\n",
       " 'model_weight_train_loss_0.31018_test_loss_0.80373_epoch_255.pth',\n",
       " 'model_weight_train_loss_0.31030_test_loss_0.44085_epoch_223.pth',\n",
       " 'model_weight_train_loss_0.31063_test_loss_0.67614_epoch_89.pth',\n",
       " 'model_weight_train_loss_0.31118_test_loss_1.58693_epoch_123.pth',\n",
       " 'model_weight_train_loss_0.31134_test_loss_0.43394_epoch_257.pth',\n",
       " 'model_weight_train_loss_0.31190_test_loss_0.71103_epoch_69.pth',\n",
       " 'model_weight_train_loss_0.31218_test_loss_0.81197_epoch_126.pth',\n",
       " 'model_weight_train_loss_0.31293_test_loss_1.73964_epoch_217.pth',\n",
       " 'model_weight_train_loss_0.31517_test_loss_1.27664_epoch_64.pth',\n",
       " 'model_weight_train_loss_0.31530_test_loss_0.92168_epoch_146.pth',\n",
       " 'model_weight_train_loss_0.31538_test_loss_0.59241_epoch_70.pth',\n",
       " 'model_weight_train_loss_0.31617_test_loss_0.84555_epoch_160.pth',\n",
       " 'model_weight_train_loss_0.31633_test_loss_0.99641_epoch_422.pth',\n",
       " 'model_weight_train_loss_0.31690_test_loss_1.14792_epoch_71.pth',\n",
       " 'model_weight_train_loss_0.31701_test_loss_1.12167_epoch_461.pth',\n",
       " 'model_weight_train_loss_0.31857_test_loss_1.55699_epoch_114.pth',\n",
       " 'model_weight_train_loss_0.31858_test_loss_0.99058_epoch_241.pth',\n",
       " 'model_weight_train_loss_0.31900_test_loss_0.91588_epoch_72.pth',\n",
       " 'model_weight_train_loss_0.31909_test_loss_1.47648_epoch_111.pth',\n",
       " 'model_weight_train_loss_0.31915_test_loss_0.64059_epoch_143.pth',\n",
       " 'model_weight_train_loss_0.31934_test_loss_0.97367_epoch_116.pth',\n",
       " 'model_weight_train_loss_0.31993_test_loss_1.05248_epoch_78.pth',\n",
       " 'model_weight_train_loss_0.32007_test_loss_0.60654_epoch_81.pth',\n",
       " 'model_weight_train_loss_0.32047_test_loss_1.07940_epoch_75.pth',\n",
       " 'model_weight_train_loss_0.32064_test_loss_0.57267_epoch_218.pth',\n",
       " 'model_weight_train_loss_0.32156_test_loss_0.70910_epoch_86.pth',\n",
       " 'model_weight_train_loss_0.32242_test_loss_0.93373_epoch_63.pth',\n",
       " 'model_weight_train_loss_0.32370_test_loss_0.76993_epoch_281.pth',\n",
       " 'model_weight_train_loss_0.32408_test_loss_1.33437_epoch_162.pth',\n",
       " 'model_weight_train_loss_0.32440_test_loss_0.57623_epoch_109.pth',\n",
       " 'model_weight_train_loss_0.32584_test_loss_0.70836_epoch_179.pth',\n",
       " 'model_weight_train_loss_0.32608_test_loss_1.08524_epoch_62.pth',\n",
       " 'model_weight_train_loss_0.32635_test_loss_0.64311_epoch_90.pth',\n",
       " 'model_weight_train_loss_0.32673_test_loss_0.72083_epoch_85.pth',\n",
       " 'model_weight_train_loss_0.32707_test_loss_1.34083_epoch_151.pth',\n",
       " 'model_weight_train_loss_0.32759_test_loss_1.49662_epoch_82.pth',\n",
       " 'model_weight_train_loss_0.32912_test_loss_1.39867_epoch_74.pth',\n",
       " 'model_weight_train_loss_0.33006_test_loss_0.59625_epoch_206.pth',\n",
       " 'model_weight_train_loss_0.33022_test_loss_0.86143_epoch_132.pth',\n",
       " 'model_weight_train_loss_0.33043_test_loss_1.59431_epoch_68.pth',\n",
       " 'model_weight_train_loss_0.33078_test_loss_1.11598_epoch_60.pth',\n",
       " 'model_weight_train_loss_0.33083_test_loss_0.40267_epoch_266.pth',\n",
       " 'model_weight_train_loss_0.33135_test_loss_0.67677_epoch_115.pth',\n",
       " 'model_weight_train_loss_0.33433_test_loss_0.55293_epoch_138.pth',\n",
       " 'model_weight_train_loss_0.33454_test_loss_0.85448_epoch_225.pth',\n",
       " 'model_weight_train_loss_0.33663_test_loss_1.26645_epoch_186.pth',\n",
       " 'model_weight_train_loss_0.33812_test_loss_0.47470_epoch_152.pth',\n",
       " 'model_weight_train_loss_0.33834_test_loss_0.83393_epoch_323.pth',\n",
       " 'model_weight_train_loss_0.33863_test_loss_0.81956_epoch_67.pth',\n",
       " 'model_weight_train_loss_0.33879_test_loss_0.63994_epoch_121.pth',\n",
       " 'model_weight_train_loss_0.33932_test_loss_1.53705_epoch_139.pth',\n",
       " 'model_weight_train_loss_0.34160_test_loss_1.23221_epoch_236.pth',\n",
       " 'model_weight_train_loss_0.34240_test_loss_1.06324_epoch_345.pth',\n",
       " 'model_weight_train_loss_0.34243_test_loss_1.04194_epoch_87.pth',\n",
       " 'model_weight_train_loss_0.34302_test_loss_0.63241_epoch_120.pth',\n",
       " 'model_weight_train_loss_0.34410_test_loss_1.04198_epoch_133.pth',\n",
       " 'model_weight_train_loss_0.34459_test_loss_0.55854_epoch_91.pth',\n",
       " 'model_weight_train_loss_0.34623_test_loss_0.92130_epoch_180.pth',\n",
       " 'model_weight_train_loss_0.34685_test_loss_0.33433_epoch_320.pth',\n",
       " 'model_weight_train_loss_0.34858_test_loss_0.79249_epoch_58.pth',\n",
       " 'model_weight_train_loss_0.35017_test_loss_1.18746_epoch_57.pth',\n",
       " 'model_weight_train_loss_0.35226_test_loss_0.93240_epoch_83.pth',\n",
       " 'model_weight_train_loss_0.35499_test_loss_1.42087_epoch_59.pth',\n",
       " 'model_weight_train_loss_0.35561_test_loss_0.40118_epoch_127.pth',\n",
       " 'model_weight_train_loss_0.35576_test_loss_0.55297_epoch_135.pth',\n",
       " 'model_weight_train_loss_0.35833_test_loss_0.85644_epoch_61.pth',\n",
       " 'model_weight_train_loss_0.35839_test_loss_0.47543_epoch_300.pth',\n",
       " 'model_weight_train_loss_0.35935_test_loss_1.07816_epoch_54.pth',\n",
       " 'model_weight_train_loss_0.35959_test_loss_0.87299_epoch_52.pth',\n",
       " 'model_weight_train_loss_0.36147_test_loss_0.94068_epoch_150.pth',\n",
       " 'model_weight_train_loss_0.36203_test_loss_1.42813_epoch_77.pth',\n",
       " 'model_weight_train_loss_0.36239_test_loss_0.95756_epoch_56.pth',\n",
       " 'model_weight_train_loss_0.36917_test_loss_1.35834_epoch_49.pth',\n",
       " 'model_weight_train_loss_0.37074_test_loss_1.17747_epoch_141.pth',\n",
       " 'model_weight_train_loss_0.37235_test_loss_0.79044_epoch_344.pth',\n",
       " 'model_weight_train_loss_0.37329_test_loss_1.22099_epoch_51.pth',\n",
       " 'model_weight_train_loss_0.37784_test_loss_1.08713_epoch_50.pth',\n",
       " 'model_weight_train_loss_0.37949_test_loss_1.24674_epoch_48.pth',\n",
       " 'model_weight_train_loss_0.38157_test_loss_0.63137_epoch_314.pth',\n",
       " 'model_weight_train_loss_0.38339_test_loss_0.98250_epoch_286.pth',\n",
       " 'model_weight_train_loss_0.38454_test_loss_0.92842_epoch_55.pth',\n",
       " 'model_weight_train_loss_0.38560_test_loss_1.04728_epoch_53.pth',\n",
       " 'model_weight_train_loss_0.38628_test_loss_1.41255_epoch_92.pth',\n",
       " 'model_weight_train_loss_0.38709_test_loss_1.11108_epoch_47.pth',\n",
       " 'model_weight_train_loss_0.38886_test_loss_1.10756_epoch_84.pth',\n",
       " 'model_weight_train_loss_0.38997_test_loss_0.72390_epoch_156.pth',\n",
       " 'model_weight_train_loss_0.39420_test_loss_1.25986_epoch_46.pth',\n",
       " 'model_weight_train_loss_0.40651_test_loss_1.34372_epoch_45.pth',\n",
       " 'model_weight_train_loss_0.40695_test_loss_1.06902_epoch_234.pth',\n",
       " 'model_weight_train_loss_0.40935_test_loss_1.14649_epoch_43.pth',\n",
       " 'model_weight_train_loss_0.41318_test_loss_1.04354_epoch_44.pth',\n",
       " 'model_weight_train_loss_0.41396_test_loss_1.39241_epoch_420.pth',\n",
       " 'model_weight_train_loss_0.41495_test_loss_1.22990_epoch_42.pth',\n",
       " 'model_weight_train_loss_0.41993_test_loss_1.08342_epoch_39.pth',\n",
       " 'model_weight_train_loss_0.42045_test_loss_1.03919_epoch_41.pth',\n",
       " 'model_weight_train_loss_0.42218_test_loss_1.29107_epoch_421.pth',\n",
       " 'model_weight_train_loss_0.43077_test_loss_1.02449_epoch_119.pth',\n",
       " 'model_weight_train_loss_0.43405_test_loss_1.34015_epoch_157.pth',\n",
       " 'model_weight_train_loss_0.43447_test_loss_1.18125_epoch_38.pth',\n",
       " 'model_weight_train_loss_0.43709_test_loss_0.98800_epoch_40.pth',\n",
       " 'model_weight_train_loss_0.44311_test_loss_1.24486_epoch_37.pth',\n",
       " 'model_weight_train_loss_0.44901_test_loss_1.32202_epoch_35.pth',\n",
       " 'model_weight_train_loss_0.45175_test_loss_0.96120_epoch_36.pth',\n",
       " 'model_weight_train_loss_0.45410_test_loss_1.33948_epoch_34.pth',\n",
       " 'model_weight_train_loss_0.45477_test_loss_1.06769_epoch_33.pth',\n",
       " 'model_weight_train_loss_0.45821_test_loss_1.08292_epoch_31.pth',\n",
       " 'model_weight_train_loss_0.45888_test_loss_0.97376_epoch_32.pth',\n",
       " 'model_weight_train_loss_0.46963_test_loss_1.03535_epoch_30.pth',\n",
       " 'model_weight_train_loss_0.47476_test_loss_1.51148_epoch_122.pth',\n",
       " 'model_weight_train_loss_0.47687_test_loss_1.18181_epoch_29.pth',\n",
       " 'model_weight_train_loss_0.48281_test_loss_1.11712_epoch_28.pth',\n",
       " 'model_weight_train_loss_0.48742_test_loss_1.06532_epoch_27.pth',\n",
       " 'model_weight_train_loss_0.48987_test_loss_0.70023_epoch_117.pth',\n",
       " 'model_weight_train_loss_0.49010_test_loss_0.71450_epoch_148.pth',\n",
       " 'model_weight_train_loss_0.49391_test_loss_1.15309_epoch_26.pth',\n",
       " 'model_weight_train_loss_0.49605_test_loss_1.06301_epoch_275.pth',\n",
       " 'model_weight_train_loss_0.50728_test_loss_1.66635_epoch_322.pth',\n",
       " 'model_weight_train_loss_0.53212_test_loss_1.45370_epoch_118.pth']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-12-27'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_cols[0].columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in weights:\n",
    "\n",
    "    # Create the model\n",
    "    model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "    model.load_state_dict(torch.load(f'../predict_model/weights/{weight}', map_location=\"cpu\"))\n",
    "\n",
    "    directory = f'./al_predicts/{weight}'\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    # 各グループを表示\n",
    "    for number, group in enumerate(grouped_cols):\n",
    "        x = group\n",
    "        # 2.DataFrameをnarrayに変換\n",
    "        x = x.to_numpy().astype(\"float32\")\n",
    "\n",
    "        # 3.ndarrayをTensorに変換\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "\n",
    "        # 4.TensorからDatasetを作成\n",
    "        dataset = torch.utils.data.TensorDataset(x)\n",
    "\n",
    "        # 6.DataLoaderに変換\n",
    "        batch_size = 20\n",
    "        test_loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "\n",
    "        # モデルを評価モードにする\n",
    "        model.eval()\n",
    "\n",
    "        # 正しい予測数、全体のデータ数を数えるカウンターの0初期化\n",
    "        total_correct = 0\n",
    "\n",
    "        name_list = []\n",
    "        index_y_list = []\n",
    "        ans_list = []\n",
    "        val_list = []\n",
    "\n",
    "        loss_mean = 0\n",
    "        m = nn.Softmax(dim=2)\n",
    "\n",
    "        for j, x in enumerate(test_loader):\n",
    "            x = torch.stack(x)  # リストをテンソルに変換\n",
    "            y = model(x)  # 順伝播（=予測）\n",
    "            # print(m(y))\n",
    "            \n",
    "            # ミニバッチごとの正答率と損失を求める\n",
    "            _, index_y = torch.max(y, axis=1)  # 最も確率が高いと予測したindex\n",
    "            # print(index_y, index_t)\n",
    "            res = m(y)\n",
    "            val, index_y = torch.max(m(y)[0], axis=1)\n",
    "            # print(val.tolist(), index_y.tolist())\n",
    "            for j, (i, ii) in enumerate(zip(val.tolist(), index_y.tolist())):\n",
    "                if ii == 1:\n",
    "                    name_list.append(df['name'][j])\n",
    "                    index_y_list.append(ii)\n",
    "                    val_list.append(i)\n",
    "\n",
    "        # リストをDataFrameに変換\n",
    "        result_df = pd.DataFrame({\n",
    "            'name': name_list,\n",
    "            'ai_predict': index_y_list,\n",
    "            'ai_confidence': val_list\n",
    "        })\n",
    "\n",
    "        result_df = result_df.sort_values(by='ai_confidence', ascending=False)\n",
    "\n",
    "        \"\"\"\n",
    "        予測するのはcsvの最後の日付の次の日、ただ、その次の日は休日の可能性があるので、\n",
    "        次のCSVの日付を予測日として保存する\n",
    "        最後のcsvの場合は、最後の日付の次の日を予測日として保存する\n",
    "        (このケースはエラーになる可能性があるので注意)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            date_str = grouped_cols[number+1].columns[-1]\n",
    "            date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            new_date_str = date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "        except Exception as e:\n",
    "            date_str = group.columns[-1]\n",
    "            date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            new_date_obj = date_obj + timedelta(days=1)\n",
    "            new_date_str = new_date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "        result_df.to_csv(f'./al_predicts/{weight}/{new_date_str}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.9630, 0.6251, 0.6053,  ..., 0.6916, 0.6741, 0.5742], device='cuda:0',\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1, 1, 0,  ..., 1, 1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(m(y)[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ai_predict</th>\n",
       "      <th>ai_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AEI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.988075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ADTX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.987467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ACRX</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AGFY</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ADD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>SGH</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>VC</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>TOMZ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>HLIT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>CNSL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1519 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  ai_predict  ai_confidence\n",
       "29     AEI           1       0.988075\n",
       "28    ADTX           1       0.987467\n",
       "19    ACRX           1       0.984830\n",
       "37    AGFY           1       0.982886\n",
       "22     ADD           1       0.982705\n",
       "...    ...         ...            ...\n",
       "1171   SGH           1       0.501737\n",
       "1393    VC           1       0.501423\n",
       "1330  TOMZ           1       0.501298\n",
       "570   HLIT           1       0.500268\n",
       "272   CNSL           1       0.500106\n",
       "\n",
       "[1519 rows x 3 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AADI'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['0'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.944132</td>\n",
       "      <td>0.912860</td>\n",
       "      <td>0.946943</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.950808</td>\n",
       "      <td>0.940970</td>\n",
       "      <td>0.956079</td>\n",
       "      <td>0.921644</td>\n",
       "      <td>0.936753</td>\n",
       "      <td>0.975755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892481</td>\n",
       "      <td>0.904076</td>\n",
       "      <td>0.907238</td>\n",
       "      <td>0.880534</td>\n",
       "      <td>0.873507</td>\n",
       "      <td>0.908292</td>\n",
       "      <td>0.864020</td>\n",
       "      <td>0.846451</td>\n",
       "      <td>0.837316</td>\n",
       "      <td>0.873858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.906110</td>\n",
       "      <td>0.901617</td>\n",
       "      <td>0.904313</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>0.897574</td>\n",
       "      <td>0.891285</td>\n",
       "      <td>0.877359</td>\n",
       "      <td>0.876909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.761456</td>\n",
       "      <td>0.777179</td>\n",
       "      <td>0.814016</td>\n",
       "      <td>0.820305</td>\n",
       "      <td>0.820305</td>\n",
       "      <td>0.816262</td>\n",
       "      <td>0.832884</td>\n",
       "      <td>0.810871</td>\n",
       "      <td>0.811770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.960089</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>0.997783</td>\n",
       "      <td>0.971175</td>\n",
       "      <td>0.955654</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.960089</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>0.920177</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758315</td>\n",
       "      <td>0.789357</td>\n",
       "      <td>0.691796</td>\n",
       "      <td>0.691796</td>\n",
       "      <td>0.687361</td>\n",
       "      <td>0.711752</td>\n",
       "      <td>0.678492</td>\n",
       "      <td>0.694013</td>\n",
       "      <td>0.638581</td>\n",
       "      <td>0.616408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.445736</td>\n",
       "      <td>0.438631</td>\n",
       "      <td>0.439922</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.474160</td>\n",
       "      <td>0.466408</td>\n",
       "      <td>0.462532</td>\n",
       "      <td>0.479328</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351421</td>\n",
       "      <td>0.366279</td>\n",
       "      <td>0.347545</td>\n",
       "      <td>0.353359</td>\n",
       "      <td>0.352713</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.354005</td>\n",
       "      <td>0.342377</td>\n",
       "      <td>0.345607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.644453</td>\n",
       "      <td>0.636428</td>\n",
       "      <td>0.626557</td>\n",
       "      <td>0.619085</td>\n",
       "      <td>0.621760</td>\n",
       "      <td>0.639195</td>\n",
       "      <td>0.645745</td>\n",
       "      <td>0.653586</td>\n",
       "      <td>0.659213</td>\n",
       "      <td>0.650726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716707</td>\n",
       "      <td>0.721151</td>\n",
       "      <td>0.713096</td>\n",
       "      <td>0.721336</td>\n",
       "      <td>0.727724</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.738927</td>\n",
       "      <td>0.734668</td>\n",
       "      <td>0.738001</td>\n",
       "      <td>0.733002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.688631</td>\n",
       "      <td>0.590439</td>\n",
       "      <td>0.574289</td>\n",
       "      <td>0.495478</td>\n",
       "      <td>0.560078</td>\n",
       "      <td>0.524548</td>\n",
       "      <td>0.510982</td>\n",
       "      <td>0.550388</td>\n",
       "      <td>0.544574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.601421</td>\n",
       "      <td>0.618863</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>0.664083</td>\n",
       "      <td>0.711886</td>\n",
       "      <td>0.708656</td>\n",
       "      <td>0.728036</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>0.910362</td>\n",
       "      <td>0.914211</td>\n",
       "      <td>0.933452</td>\n",
       "      <td>0.974470</td>\n",
       "      <td>0.984607</td>\n",
       "      <td>0.971654</td>\n",
       "      <td>0.960860</td>\n",
       "      <td>0.948376</td>\n",
       "      <td>0.956355</td>\n",
       "      <td>0.949597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862988</td>\n",
       "      <td>0.798990</td>\n",
       "      <td>0.802796</td>\n",
       "      <td>0.804769</td>\n",
       "      <td>0.782779</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>0.810549</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>0.803923</td>\n",
       "      <td>0.781651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>0.965996</td>\n",
       "      <td>0.969916</td>\n",
       "      <td>0.970680</td>\n",
       "      <td>0.987478</td>\n",
       "      <td>0.986154</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.983100</td>\n",
       "      <td>0.977043</td>\n",
       "      <td>0.981166</td>\n",
       "      <td>0.985594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894513</td>\n",
       "      <td>0.886561</td>\n",
       "      <td>0.890944</td>\n",
       "      <td>0.897520</td>\n",
       "      <td>0.876519</td>\n",
       "      <td>0.868720</td>\n",
       "      <td>0.870046</td>\n",
       "      <td>0.872696</td>\n",
       "      <td>0.885643</td>\n",
       "      <td>0.878762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>0.669834</td>\n",
       "      <td>0.687958</td>\n",
       "      <td>0.675013</td>\n",
       "      <td>0.692581</td>\n",
       "      <td>0.726424</td>\n",
       "      <td>0.747137</td>\n",
       "      <td>0.735301</td>\n",
       "      <td>0.735301</td>\n",
       "      <td>0.755459</td>\n",
       "      <td>0.751576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705712</td>\n",
       "      <td>0.691842</td>\n",
       "      <td>0.687588</td>\n",
       "      <td>0.699794</td>\n",
       "      <td>0.688698</td>\n",
       "      <td>0.688513</td>\n",
       "      <td>0.698725</td>\n",
       "      <td>0.712837</td>\n",
       "      <td>0.701325</td>\n",
       "      <td>0.686656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>0.425832</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.403835</td>\n",
       "      <td>0.425832</td>\n",
       "      <td>0.426396</td>\n",
       "      <td>0.448393</td>\n",
       "      <td>0.435420</td>\n",
       "      <td>0.442752</td>\n",
       "      <td>0.431472</td>\n",
       "      <td>0.441060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286520</td>\n",
       "      <td>0.276932</td>\n",
       "      <td>0.282008</td>\n",
       "      <td>0.269036</td>\n",
       "      <td>0.265087</td>\n",
       "      <td>0.265651</td>\n",
       "      <td>0.262831</td>\n",
       "      <td>0.259447</td>\n",
       "      <td>0.251551</td>\n",
       "      <td>0.247039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3536 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             2         3         4         5         6         7         8  \\\n",
       "0     0.944132  0.912860  0.946943  0.958538  0.950808  0.940970  0.956079   \n",
       "1     0.906110  0.901617  0.904313  0.911500  0.880952  0.882300  0.897574   \n",
       "2     0.960089  0.995565  0.997783  0.971175  0.955654  0.975610  0.960089   \n",
       "3     0.445736  0.438631  0.439922  0.436047  0.465116  0.474160  0.466408   \n",
       "4     0.644453  0.636428  0.626557  0.619085  0.621760  0.639195  0.645745   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3531  0.616279  0.688631  0.590439  0.574289  0.495478  0.560078  0.524548   \n",
       "3532  0.910362  0.914211  0.933452  0.974470  0.984607  0.971654  0.960860   \n",
       "3533  0.965996  0.969916  0.970680  0.987478  0.986154  0.993128  0.983100   \n",
       "3534  0.669834  0.687958  0.675013  0.692581  0.726424  0.747137  0.735301   \n",
       "3535  0.425832  0.418500  0.403835  0.425832  0.426396  0.448393  0.435420   \n",
       "\n",
       "             9        10        11  ...        51        52        53  \\\n",
       "0     0.921644  0.936753  0.975755  ...  0.892481  0.904076  0.907238   \n",
       "1     0.891285  0.877359  0.876909  ...  0.742138  0.761456  0.777179   \n",
       "2     0.942350  0.920177  0.942350  ...  0.758315  0.789357  0.691796   \n",
       "3     0.462532  0.479328  0.478682  ...  0.351421  0.366279  0.347545   \n",
       "4     0.653586  0.659213  0.650726  ...  0.716707  0.721151  0.713096   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3531  0.510982  0.550388  0.544574  ...  0.635659  0.601421  0.618863   \n",
       "3532  0.948376  0.956355  0.949597  ...  0.862988  0.798990  0.802796   \n",
       "3533  0.977043  0.981166  0.985594  ...  0.894513  0.886561  0.890944   \n",
       "3534  0.735301  0.755459  0.751576  ...  0.705712  0.691842  0.687588   \n",
       "3535  0.442752  0.431472  0.441060  ...  0.286520  0.276932  0.282008   \n",
       "\n",
       "            54        55        56        57        58        59        60  \n",
       "0     0.880534  0.873507  0.908292  0.864020  0.846451  0.837316  0.873858  \n",
       "1     0.814016  0.820305  0.820305  0.816262  0.832884  0.810871  0.811770  \n",
       "2     0.691796  0.687361  0.711752  0.678492  0.694013  0.638581  0.616408  \n",
       "3     0.353359  0.352713  0.364341  0.361111  0.354005  0.342377  0.345607  \n",
       "4     0.721336  0.727724  0.727447  0.738927  0.734668  0.738001  0.733002  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3531  0.606589  0.629845  0.664083  0.711886  0.708656  0.728036  0.666667  \n",
       "3532  0.804769  0.782779  0.801668  0.810549  0.801668  0.803923  0.781651  \n",
       "3533  0.897520  0.876519  0.868720  0.870046  0.872696  0.885643  0.878762  \n",
       "3534  0.699794  0.688698  0.688513  0.698725  0.712837  0.701325  0.686656  \n",
       "3535  0.269036  0.265087  0.265651  0.262831  0.259447  0.251551  0.247039  \n",
       "\n",
       "[3536 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.DataFrameを説明変数と目的変数に分ける\n",
    "x = df.iloc[:, 2:]\n",
    "t = pd.get_dummies(df['1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.944132</td>\n",
       "      <td>0.912860</td>\n",
       "      <td>0.946943</td>\n",
       "      <td>0.958538</td>\n",
       "      <td>0.950808</td>\n",
       "      <td>0.940970</td>\n",
       "      <td>0.956079</td>\n",
       "      <td>0.921644</td>\n",
       "      <td>0.936753</td>\n",
       "      <td>0.975755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892481</td>\n",
       "      <td>0.904076</td>\n",
       "      <td>0.907238</td>\n",
       "      <td>0.880534</td>\n",
       "      <td>0.873507</td>\n",
       "      <td>0.908292</td>\n",
       "      <td>0.864020</td>\n",
       "      <td>0.846451</td>\n",
       "      <td>0.837316</td>\n",
       "      <td>0.873858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.906110</td>\n",
       "      <td>0.901617</td>\n",
       "      <td>0.904313</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>0.897574</td>\n",
       "      <td>0.891285</td>\n",
       "      <td>0.877359</td>\n",
       "      <td>0.876909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.761456</td>\n",
       "      <td>0.777179</td>\n",
       "      <td>0.814016</td>\n",
       "      <td>0.820305</td>\n",
       "      <td>0.820305</td>\n",
       "      <td>0.816262</td>\n",
       "      <td>0.832884</td>\n",
       "      <td>0.810871</td>\n",
       "      <td>0.811770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.960089</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>0.997783</td>\n",
       "      <td>0.971175</td>\n",
       "      <td>0.955654</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.960089</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>0.920177</td>\n",
       "      <td>0.942350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758315</td>\n",
       "      <td>0.789357</td>\n",
       "      <td>0.691796</td>\n",
       "      <td>0.691796</td>\n",
       "      <td>0.687361</td>\n",
       "      <td>0.711752</td>\n",
       "      <td>0.678492</td>\n",
       "      <td>0.694013</td>\n",
       "      <td>0.638581</td>\n",
       "      <td>0.616408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.445736</td>\n",
       "      <td>0.438631</td>\n",
       "      <td>0.439922</td>\n",
       "      <td>0.436047</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.474160</td>\n",
       "      <td>0.466408</td>\n",
       "      <td>0.462532</td>\n",
       "      <td>0.479328</td>\n",
       "      <td>0.478682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351421</td>\n",
       "      <td>0.366279</td>\n",
       "      <td>0.347545</td>\n",
       "      <td>0.353359</td>\n",
       "      <td>0.352713</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.354005</td>\n",
       "      <td>0.342377</td>\n",
       "      <td>0.345607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.644453</td>\n",
       "      <td>0.636428</td>\n",
       "      <td>0.626557</td>\n",
       "      <td>0.619085</td>\n",
       "      <td>0.621760</td>\n",
       "      <td>0.639195</td>\n",
       "      <td>0.645745</td>\n",
       "      <td>0.653586</td>\n",
       "      <td>0.659213</td>\n",
       "      <td>0.650726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716707</td>\n",
       "      <td>0.721151</td>\n",
       "      <td>0.713096</td>\n",
       "      <td>0.721336</td>\n",
       "      <td>0.727724</td>\n",
       "      <td>0.727447</td>\n",
       "      <td>0.738927</td>\n",
       "      <td>0.734668</td>\n",
       "      <td>0.738001</td>\n",
       "      <td>0.733002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.688631</td>\n",
       "      <td>0.590439</td>\n",
       "      <td>0.574289</td>\n",
       "      <td>0.495478</td>\n",
       "      <td>0.560078</td>\n",
       "      <td>0.524548</td>\n",
       "      <td>0.510982</td>\n",
       "      <td>0.550388</td>\n",
       "      <td>0.544574</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.601421</td>\n",
       "      <td>0.618863</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>0.629845</td>\n",
       "      <td>0.664083</td>\n",
       "      <td>0.711886</td>\n",
       "      <td>0.708656</td>\n",
       "      <td>0.728036</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>0.910362</td>\n",
       "      <td>0.914211</td>\n",
       "      <td>0.933452</td>\n",
       "      <td>0.974470</td>\n",
       "      <td>0.984607</td>\n",
       "      <td>0.971654</td>\n",
       "      <td>0.960860</td>\n",
       "      <td>0.948376</td>\n",
       "      <td>0.956355</td>\n",
       "      <td>0.949597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862988</td>\n",
       "      <td>0.798990</td>\n",
       "      <td>0.802796</td>\n",
       "      <td>0.804769</td>\n",
       "      <td>0.782779</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>0.810549</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>0.803923</td>\n",
       "      <td>0.781651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>0.965996</td>\n",
       "      <td>0.969916</td>\n",
       "      <td>0.970680</td>\n",
       "      <td>0.987478</td>\n",
       "      <td>0.986154</td>\n",
       "      <td>0.993128</td>\n",
       "      <td>0.983100</td>\n",
       "      <td>0.977043</td>\n",
       "      <td>0.981166</td>\n",
       "      <td>0.985594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894513</td>\n",
       "      <td>0.886561</td>\n",
       "      <td>0.890944</td>\n",
       "      <td>0.897520</td>\n",
       "      <td>0.876519</td>\n",
       "      <td>0.868720</td>\n",
       "      <td>0.870046</td>\n",
       "      <td>0.872696</td>\n",
       "      <td>0.885643</td>\n",
       "      <td>0.878762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>0.669834</td>\n",
       "      <td>0.687958</td>\n",
       "      <td>0.675013</td>\n",
       "      <td>0.692581</td>\n",
       "      <td>0.726424</td>\n",
       "      <td>0.747137</td>\n",
       "      <td>0.735301</td>\n",
       "      <td>0.735301</td>\n",
       "      <td>0.755459</td>\n",
       "      <td>0.751576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705712</td>\n",
       "      <td>0.691842</td>\n",
       "      <td>0.687588</td>\n",
       "      <td>0.699794</td>\n",
       "      <td>0.688698</td>\n",
       "      <td>0.688513</td>\n",
       "      <td>0.698725</td>\n",
       "      <td>0.712837</td>\n",
       "      <td>0.701325</td>\n",
       "      <td>0.686656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>0.425832</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.403835</td>\n",
       "      <td>0.425832</td>\n",
       "      <td>0.426396</td>\n",
       "      <td>0.448393</td>\n",
       "      <td>0.435420</td>\n",
       "      <td>0.442752</td>\n",
       "      <td>0.431472</td>\n",
       "      <td>0.441060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286520</td>\n",
       "      <td>0.276932</td>\n",
       "      <td>0.282008</td>\n",
       "      <td>0.269036</td>\n",
       "      <td>0.265087</td>\n",
       "      <td>0.265651</td>\n",
       "      <td>0.262831</td>\n",
       "      <td>0.259447</td>\n",
       "      <td>0.251551</td>\n",
       "      <td>0.247039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3536 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             2         3         4         5         6         7         8  \\\n",
       "0     0.944132  0.912860  0.946943  0.958538  0.950808  0.940970  0.956079   \n",
       "1     0.906110  0.901617  0.904313  0.911500  0.880952  0.882300  0.897574   \n",
       "2     0.960089  0.995565  0.997783  0.971175  0.955654  0.975610  0.960089   \n",
       "3     0.445736  0.438631  0.439922  0.436047  0.465116  0.474160  0.466408   \n",
       "4     0.644453  0.636428  0.626557  0.619085  0.621760  0.639195  0.645745   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3531  0.616279  0.688631  0.590439  0.574289  0.495478  0.560078  0.524548   \n",
       "3532  0.910362  0.914211  0.933452  0.974470  0.984607  0.971654  0.960860   \n",
       "3533  0.965996  0.969916  0.970680  0.987478  0.986154  0.993128  0.983100   \n",
       "3534  0.669834  0.687958  0.675013  0.692581  0.726424  0.747137  0.735301   \n",
       "3535  0.425832  0.418500  0.403835  0.425832  0.426396  0.448393  0.435420   \n",
       "\n",
       "             9        10        11  ...        51        52        53  \\\n",
       "0     0.921644  0.936753  0.975755  ...  0.892481  0.904076  0.907238   \n",
       "1     0.891285  0.877359  0.876909  ...  0.742138  0.761456  0.777179   \n",
       "2     0.942350  0.920177  0.942350  ...  0.758315  0.789357  0.691796   \n",
       "3     0.462532  0.479328  0.478682  ...  0.351421  0.366279  0.347545   \n",
       "4     0.653586  0.659213  0.650726  ...  0.716707  0.721151  0.713096   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3531  0.510982  0.550388  0.544574  ...  0.635659  0.601421  0.618863   \n",
       "3532  0.948376  0.956355  0.949597  ...  0.862988  0.798990  0.802796   \n",
       "3533  0.977043  0.981166  0.985594  ...  0.894513  0.886561  0.890944   \n",
       "3534  0.735301  0.755459  0.751576  ...  0.705712  0.691842  0.687588   \n",
       "3535  0.442752  0.431472  0.441060  ...  0.286520  0.276932  0.282008   \n",
       "\n",
       "            54        55        56        57        58        59        60  \n",
       "0     0.880534  0.873507  0.908292  0.864020  0.846451  0.837316  0.873858  \n",
       "1     0.814016  0.820305  0.820305  0.816262  0.832884  0.810871  0.811770  \n",
       "2     0.691796  0.687361  0.711752  0.678492  0.694013  0.638581  0.616408  \n",
       "3     0.353359  0.352713  0.364341  0.361111  0.354005  0.342377  0.345607  \n",
       "4     0.721336  0.727724  0.727447  0.738927  0.734668  0.738001  0.733002  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3531  0.606589  0.629845  0.664083  0.711886  0.708656  0.728036  0.666667  \n",
       "3532  0.804769  0.782779  0.801668  0.810549  0.801668  0.803923  0.781651  \n",
       "3533  0.897520  0.876519  0.868720  0.870046  0.872696  0.885643  0.878762  \n",
       "3534  0.699794  0.688698  0.688513  0.698725  0.712837  0.701325  0.686656  \n",
       "3535  0.269036  0.265087  0.265651  0.262831  0.259447  0.251551  0.247039  \n",
       "\n",
       "[3536 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3536 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1\n",
       "0     1  0\n",
       "1     1  0\n",
       "2     1  0\n",
       "3     1  0\n",
       "4     1  0\n",
       "...  .. ..\n",
       "3531  1  0\n",
       "3532  0  1\n",
       "3533  1  0\n",
       "3534  1  0\n",
       "3535  1  0\n",
       "\n",
       "[3536 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.DataFrameをnarrayに変換\n",
    "x = x.to_numpy().astype(\"float32\")\n",
    "\n",
    "# 3.ndarrayをTensorに変換\n",
    "x = torch.from_numpy(x).to(device)\n",
    "\n",
    "# 4.TensorからDatasetを作成\n",
    "dataset = torch.utils.data.TensorDataset(x)\n",
    "\n",
    "# 6.DataLoaderに変換\n",
    "batch_size = 20\n",
    "test_loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size = 64,\n",
    "                            hidden_size = 100,\n",
    "                            batch_first = True)\n",
    "        self.output_layer = nn.Linear(20, 2)\n",
    "\n",
    "        # 損失関数と最適化関数\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion_2 = nn.HingeEmbeddingLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, inputs, hidden0=None):\n",
    "        output, (hidden, cell) = self.rnn(inputs, hidden0) #LSTM層\n",
    "        output = self.output_layer(output[:, -1]) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        # 損失関数と最適化関数\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion_2 = nn.HingeEmbeddingLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x, hidden0=None):        \n",
    "        # Forward pass\n",
    "        out, (hidden, cell) = self.lstm(x, hidden0)\n",
    "        # Index hidden state of last time step\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層を6つに増やす\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.dropout1 = nn.Dropout(0.3)  # ドロップアウトを追加\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)  # ドロップアウトを追加\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.dropout3 = nn.Dropout(0.3)  # ドロップアウトを追加\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        self.dropout4 = nn.Dropout(0.3)  # ドロップアウトを追加\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.dropout5 = nn.Dropout(0.3)  # ドロップアウトを追加\n",
    "        self.fc6 = nn.Linear(32, 2)\n",
    "\n",
    "        # 損失関数と最適化関数\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.criterion_2 = nn.HingeEmbeddingLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)  # ドロップアウトを適用\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)  # ドロップアウトを適用\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)  # ドロップアウトを適用\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout4(x)  # ドロップアウトを適用\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout5(x)  # ドロップアウトを適用\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    # モデルを評価モードにする\n",
    "    model.eval()\n",
    "\n",
    "    # 正しい予測数、全体のデータ数を数えるカウンターの0初期化\n",
    "    total_data_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    loss_mean = 0\n",
    "\n",
    "    for j, (x, t) in enumerate(data_loader):\n",
    "        y = model(x)  # 順伝播（=予測）\n",
    "        loss = model.criterion(y, t)  # 損失を計算\n",
    "        loss_mean += loss.item()\n",
    "\n",
    "        # ミニバッチごとの正答率と損失を求める\n",
    "        _, index_y = torch.max(y, axis=1)  # 最も確率が高いと予測したindex\n",
    "        _, index_t = torch.max(t, axis=1)  # 正解のindex\n",
    "        for i in range(len(t)):  # データ一つずつループ,ミニバッチの中身出しきるまで\n",
    "            total_data_len += 1  # 全データ数を集計\n",
    "            if index_y[i] == index_t[i]:\n",
    "                total_correct += 1 # 正解のデータ数を集計\n",
    "\n",
    "    loss_mean = loss_mean / (j+1)\n",
    "\n",
    "    return total_correct, total_data_len, loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 59\n",
    "hidden_size = 2\n",
    "num_layers = 3\n",
    "num_classes = 2\n",
    "\n",
    "# Create the model\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('../predict_model/weights/model_weight3.pth', map_location=\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6188809275627136, 0.5900458097457886, 0.7202277779579163, 0.6217108964920044, 0.6510367393493652, 0.5682734847068787, 0.6661288738250732, 0.6205351948738098, 0.6994403004646301, 0.6248250603675842, 0.741827130317688, 0.6049798130989075, 0.6640167832374573, 0.5857458710670471, 0.6653698086738586, 0.5793137550354004, 0.7054375410079956, 0.5996900200843811, 0.741631805896759, 0.6484270095825195, 0.6529103517532349, 0.6232622861862183, 0.661768913269043, 0.638318657875061, 0.6749601364135742, 0.6101622581481934, 0.7435817718505859, 0.5827338695526123, 0.6395730376243591, 0.6243159174919128, 0.6500524282455444, 0.630545973777771, 0.6717182397842407, 0.6300795674324036, 0.744522750377655, 0.6389649510383606, 0.6072825789451599, 0.6068164110183716, 0.639148473739624, 0.6837903261184692, 0.6729153990745544, 0.6194990277290344, 0.7362712621688843, 0.6448964476585388, 0.6151716113090515, 0.615625262260437, 0.6318235993385315, 0.7192761301994324, 0.6588127017021179, 0.6537846922874451, 0.7466453909873962, 0.6301459074020386, 0.582138180732727, 0.5843393206596375, 0.643070638179779, 0.71971595287323, 0.6639701128005981, 0.6251702308654785, 0.7170479893684387, 0.6879172921180725, 0.5850285887718201, 0.5836828351020813, 0.6261469721794128, 0.7176050543785095, 0.6620755195617676, 0.6541290879249573, 0.7321796417236328, 0.6261886358261108, 0.5442793369293213, 0.5508010983467102, 0.6614452600479126, 0.698452353477478, 0.6351765990257263, 0.6443497538566589, 0.7184931635856628, 0.6240540146827698, 0.5452926158905029, 0.5372400283813477, 0.6523147821426392, 0.6755829453468323, 0.6759458780288696, 0.6310089826583862, 0.7228508591651917, 0.7033169269561768, 0.5405028462409973, 0.5509690046310425, 0.6647056341171265, 0.6949101090431213, 0.6564694046974182, 0.6182043552398682, 0.71977299451828, 0.7188035249710083, 0.5420209169387817, 0.5225370526313782, 0.6482368111610413, 0.6896039247512817, 0.6489840745925903, 0.61286860704422, 0.7089027166366577, 0.7683650851249695, 0.5501206517219543, 0.537811279296875, 0.6359955072402954, 0.6931706666946411, 0.6650418639183044, 0.5984352827072144, 0.709315836429596, 0.7542588710784912, 0.5271076560020447, 0.5338031649589539, 0.6275439858436584, 0.7169259190559387, 0.7035353779792786, 0.6123398542404175, 0.7327211499214172, 0.7246542572975159, 0.541241466999054, 0.5338842868804932, 0.6324885487556458, 0.7261238694190979, 0.7158358097076416, 0.6163589358329773, 0.7305244207382202, 0.7697680592536926, 0.5377449989318848, 0.5531243085861206, 0.6346842646598816, 0.7531428933143616, 0.7297870516777039, 0.6445977091789246, 0.7563598155975342, 0.8002501726150513, 0.5352982878684998, 0.5291931629180908, 0.6446464657783508, 0.7642977237701416, 0.7158705592155457, 0.6598920822143555, 0.7657285332679749, 0.8200783729553223, 0.5151111483573914, 0.5192533135414124, 0.6524879336357117, 0.7625429630279541, 0.7078192234039307, 0.6650782227516174, 0.7619328498840332, 0.8175404071807861, 0.531208872795105, 0.514477014541626, 0.6488847136497498, 0.757978081703186, 0.7276349067687988, 0.70001220703125, 0.7644863724708557, 0.7589167952537537, 0.5071659684181213, 0.5061700940132141, 0.668761134147644, 0.7622278332710266, 0.7162649035453796, 0.696371853351593, 0.7424048185348511, 0.6839566826820374, 0.5120861530303955, 0.5095718502998352, 0.6626303791999817, 0.7483530640602112, 0.7056489586830139, 0.7206912040710449, 0.7334740161895752, 0.697638213634491, 0.5476821660995483, 0.50750333070755, 0.6831739544868469, 0.7464570999145508, 0.6516870260238647, 0.7112853527069092, 0.7329408526420593, 0.7222248911857605, 0.5722709894180298, 0.5127662420272827, 0.6600933074951172, 0.7378674745559692, 0.6001578569412231, 0.6847046613693237, 0.7290841937065125, 0.7523622512817383, 0.5587476491928101, 0.5012367963790894, 0.6569334864616394, 0.727695882320404, 0.550926685333252, 0.6885842680931091, 0.7226860523223877, 0.7455725073814392, 0.5637697577476501, 0.5304279923439026, 0.6619259715080261, 0.743302583694458, 0.5594468712806702, 0.6510652899742126, 0.726524829864502, 0.733723521232605, 0.5664381384849548, 0.5288309454917908, 0.6467183828353882, 0.7274110913276672, 0.5331388115882874, 0.6560875773429871, 0.70523601770401, 0.7009928822517395, 0.571757972240448, 0.5441735982894897, 0.6609250903129578, 0.7245985865592957, 0.6074718832969666, 0.6248472332954407, 0.7114596962928772, 0.734156608581543, 0.5799687504768372, 0.5236824750900269, 0.6447254419326782, 0.7254342436790466, 0.6402820944786072, 0.6054311394691467, 0.7238190770149231, 0.7413068413734436, 0.5895230174064636, 0.5168936252593994, 0.632825493812561, 0.7019113302230835, 0.6464861035346985, 0.6193551421165466, 0.7175094485282898, 0.7320346832275391, 0.5751662850379944, 0.5052071809768677, 0.6322205066680908, 0.7005453705787659, 0.6648236513137817, 0.6160238981246948, 0.7248778939247131, 0.7442116737365723, 0.6073610782623291, 0.5507489442825317, 0.6398482322692871, 0.6900192499160767, 0.662463903427124, 0.589714527130127, 0.718744158744812, 0.7229956984519958, 0.5898579359054565, 0.5329416990280151, 0.6367194652557373, 0.674465000629425, 0.6320356130599976, 0.5892279148101807, 0.7244224548339844, 0.7297919392585754, 0.5952708721160889, 0.5553150177001953, 0.6464802622795105, 0.6851493716239929, 0.6589179635047913, 0.5533219575881958, 0.7113463878631592, 0.7333505749702454, 0.5943724513053894, 0.5504964590072632, 0.5946002006530762, 0.6729037165641785, 0.6313451528549194, 0.5222486853599548, 0.6884123682975769, 0.7456187009811401, 0.5776785016059875, 0.5734844207763672, 0.5845429301261902, 0.6863518953323364, 0.6379117369651794, 0.5421513915061951, 0.6906256079673767, 0.763167679309845, 0.5981168150901794, 0.5927412509918213, 0.5828301310539246, 0.7132260799407959, 0.6323129534721375, 0.5321679711341858, 0.6791403293609619, 0.7509526610374451, 0.591143012046814, 0.5635747909545898, 0.5545302033424377, 0.7125566005706787, 0.5925402045249939, 0.5205610394477844, 0.6664620041847229, 0.7328746914863586, 0.5866910219192505, 0.5596002340316772, 0.5948867797851562, 0.7180188894271851, 0.6163821816444397, 0.5381726026535034, 0.6371840834617615, 0.7134470343589783, 0.5932847261428833, 0.5449458956718445, 0.589492678642273, 0.6987744569778442, 0.5773406028747559, 0.5471521615982056, 0.6387393474578857, 0.7294973134994507, 0.6053626537322998, 0.5640897750854492, 0.5944392681121826, 0.7046964168548584, 0.6321913003921509, 0.5753041505813599, 0.6466688513755798, 0.7805831432342529, 0.6126654744148254, 0.5917468667030334, 0.5974292159080505, 0.7122485637664795, 0.6380991339683533, 0.600381076335907, 0.6494210362434387, 0.8093957304954529, 0.6259799003601074, 0.6065253019332886, 0.5954316854476929, 0.7031283378601074, 0.6478018760681152, 0.5879455208778381, 0.6572995185852051, 0.8128166198730469, 0.6200217604637146, 0.6125253438949585, 0.6112522482872009, 0.7063432335853577, 0.644606351852417, 0.5951119065284729, 0.6551306247711182, 0.8219819068908691, 0.620385468006134, 0.6107069253921509, 0.5983346700668335, 0.6973287463188171, 0.6474176645278931, 0.5582683086395264, 0.6554511189460754, 0.8348023295402527, 0.6154534220695496, 0.6074050664901733, 0.5848953127861023, 0.684406042098999, 0.6671643257141113, 0.5543832778930664, 0.6390582323074341, 0.8338530659675598, 0.6166521310806274, 0.6154525876045227, 0.5983688831329346, 0.6974805593490601, 0.6889629364013672, 0.5854324102401733, 0.6448362469673157, 0.8528395891189575, 0.6250984072685242, 0.6206687092781067, 0.6006234288215637, 0.699548602104187, 0.7071242928504944, 0.5962662100791931, 0.6376374363899231, 0.8657267093658447, 0.6211277842521667, 0.6271136403083801, 0.6065187454223633, 0.7175974249839783, 0.6882091164588928, 0.6345865726470947, 0.6369821429252625, 0.8669523000717163, 0.6204296946525574, 0.6168981790542603, 0.6230785250663757, 0.7329666018486023, 0.6601951122283936, 0.6372958421707153, 0.6296269297599792, 0.8653725385665894, 0.6085319519042969, 0.6020686626434326, 0.6250190734863281, 0.7283549904823303, 0.6413233876228333, 0.6389427185058594, 0.6146212816238403, 0.8264045715332031, 0.6073240637779236, 0.602544367313385, 0.6259069442749023, 0.7371983528137207, 0.6777074933052063, 0.6578435897827148, 0.5867705345153809, 0.8324949741363525, 0.612328052520752, 0.6082140803337097, 0.6369631290435791, 0.755415678024292, 0.668549656867981, 0.6639424562454224, 0.5662293434143066, 0.8640874624252319, 0.6132053732872009, 0.641917884349823, 0.6340073347091675, 0.7539178133010864, 0.6876564621925354, 0.6793500781059265, 0.5992971658706665, 0.8778706789016724, 0.6253063082695007, 0.651496171951294, 0.6488412022590637, 0.7626941800117493, 0.6859304904937744, 0.656010091304779, 0.5894672274589539, 0.8887094259262085, 0.6192942261695862, 0.6532210111618042, 0.6560844779014587, 0.7339624762535095, 0.6511132121086121, 0.5966752767562866, 0.5479188561439514, 0.8716048002243042, 0.5993396639823914, 0.6239907145500183, 0.6303571462631226, 0.7026086449623108, 0.6511643528938293, 0.5563008189201355, 0.520622193813324, 0.8703974485397339, 0.5873711705207825, 0.615395188331604, 0.6341104507446289, 0.675713837146759, 0.6414484977722168, 0.5344932079315186, 0.5647713541984558, 0.878091037273407, 0.5864558219909668, 0.6122849583625793, 0.6201842427253723, 0.6689728498458862, 0.6404629349708557, 0.5489740371704102, 0.5851738452911377, 0.8779830932617188, 0.5884383916854858, 0.6124522686004639, 0.643319845199585, 0.6443253755569458, 0.6556255221366882, 0.550836980342865, 0.5702214241027832, 0.8772809505462646, 0.6014041304588318, 0.5909593105316162, 0.6495174169540405, 0.5997419953346252, 0.6005288362503052, 0.5585627555847168, 0.5728917717933655, 0.8728357553482056, 0.5854339599609375, 0.5590481758117676, 0.6484211087226868, 0.5349782705307007, 0.5893383026123047, 0.6015985012054443, 0.5448638796806335, 0.8522404432296753, 0.5859025120735168, 0.5544068217277527, 0.6925439834594727, 0.5073000192642212, 0.5831320285797119, 0.6002306938171387, 0.5357698202133179, 0.800580620765686, 0.5751180052757263, 0.5311869978904724, 0.700832724571228, 0.500020444393158, 0.5643649101257324, 0.6097611784934998, 0.5581462979316711, 0.7935788035392761, 0.5681845545768738, 0.5589978694915771, 0.7058061957359314, 0.5003342032432556, 0.6008790731430054, 0.582088053226471, 0.567171573638916, 0.7443402409553528, 0.5696261525154114, 0.5501901507377625, 0.6940271854400635, 0.5016906261444092, 0.6126251220703125, 0.5796850919723511, 0.6107901930809021, 0.784597635269165, 0.5537428259849548, 0.5832486152648926, 0.6812066435813904, 0.505717933177948, 0.6273159980773926, 0.588657796382904, 0.6296650171279907, 0.806989848613739, 0.5564761161804199, 0.6005203127861023, 0.6908333897590637, 0.518356204032898, 0.6306895613670349, 0.5614820718765259, 0.6227896809577942, 0.7939691543579102, 0.5369741916656494, 0.5985749959945679, 0.6969079971313477, 0.5028720498085022, 0.6154660582542419, 0.5286970138549805, 0.6396636366844177, 0.8248094916343689, 0.5099684000015259, 0.6007546782493591, 0.6964127421379089, 0.5348864197731018, 0.5572448372840881, 0.5224303603172302, 0.693760871887207, 0.7641943693161011, 0.536369264125824, 0.569674015045166, 0.6847234964370728, 0.5978047251701355, 0.5285032391548157, 0.5244438052177429, 0.7135639786720276, 0.7482526898384094, 0.5286237001419067, 0.575383722782135, 0.6883506178855896, 0.5884421467781067, 0.5085261464118958, 0.5110745429992676, 0.7413177490234375, 0.7590323686599731, 0.5284106731414795, 0.576557457447052, 0.6777267456054688, 0.6070000529289246, 0.5690934658050537, 0.537246823310852, 0.712433397769928, 0.7837810516357422, 0.5019826889038086, 0.5722410678863525, 0.7039644718170166, 0.614411473274231, 0.5713517665863037, 0.5485143065452576, 0.7066349387168884, 0.8402880430221558, 0.5113538503646851, 0.5476518273353577, 0.7077073454856873, 0.6192609071731567, 0.5374108552932739, 0.515367329120636, 0.7256613969802856, 0.8351789712905884, 0.5154057741165161, 0.5452030897140503, 0.6972841620445251, 0.6443912386894226, 0.536514401435852, 0.5191654562950134, 0.6893427968025208, 0.8438622951507568, 0.5012186169624329, 0.5417347550392151, 0.7082461714744568, 0.6290239691734314, 0.5164090394973755, 0.5235621333122253, 0.6730611324310303, 0.8522228002548218, 0.5083841681480408, 0.5600333213806152, 0.6746217012405396, 0.6077340841293335, 0.5478767156600952, 0.5049015283584595, 0.6645983457565308, 0.8866685628890991, 0.5033291578292847, 0.5861793160438538, 0.668180525302887, 0.5667766332626343, 0.5690554976463318, 0.5086339712142944, 0.6680120229721069, 0.9064222574234009, 0.505143940448761, 0.5954724550247192, 0.6535882949829102, 0.520782470703125, 0.5360642671585083, 0.5005007386207581, 0.6437122225761414, 0.8972492218017578, 0.5348461270332336, 0.6076931953430176, 0.6473197937011719, 0.5032631158828735, 0.5023022890090942, 0.5433452725410461, 0.6364153623580933, 0.9045709371566772, 0.5543887615203857, 0.598698616027832, 0.6561237573623657, 0.5198656916618347, 0.5189312696456909, 0.5894818305969238, 0.6427879929542542, 0.9096918702125549, 0.5737446546554565, 0.5912744402885437, 0.6392702460289001, 0.5215623378753662, 0.5018039345741272, 0.5656676888465881, 0.6361385583877563, 0.917854905128479, 0.5809859037399292, 0.599578320980072, 0.6521149277687073, 0.5269640684127808, 0.5166301131248474, 0.5009188055992126, 0.6828576922416687, 0.9357681274414062, 0.5639492273330688, 0.619753360748291, 0.6603606939315796, 0.540216863155365, 0.5626216530799866, 0.5763965249061584, 0.6471184492111206, 0.9378949999809265, 0.5321061015129089, 0.6345458030700684, 0.6753462553024292, 0.5386422872543335, 0.5511137843132019, 0.5228666663169861, 0.6498628854751587, 0.941918671131134, 0.5298088192939758, 0.630862832069397, 0.6635063290596008, 0.5323144197463989, 0.5528299808502197, 0.5318465828895569, 0.6590160727500916, 0.9431778788566589, 0.524110734462738, 0.6117389798164368, 0.6645469665527344, 0.5302220582962036, 0.5499147772789001, 0.5219143629074097, 0.6375346779823303, 0.9430009126663208, 0.5109999775886536, 0.6171777844429016, 0.6652264595031738, 0.5462120771408081, 0.5398516058921814, 0.502892255783081, 0.6994933485984802, 0.9515602588653564, 0.5043919682502747, 0.6133261919021606, 0.6525701880455017, 0.5025367736816406, 0.5463932156562805, 0.520735502243042, 0.6405004858970642, 0.9586344957351685, 0.5349031090736389, 0.6017588376998901, 0.6474065184593201, 0.5118474364280701, 0.5600353479385376, 0.5633740425109863, 0.6028770208358765, 0.9566890001296997, 0.5074954032897949, 0.578701913356781, 0.6344670057296753, 0.5523920059204102, 0.5777761340141296, 0.5033999085426331, 0.6294969320297241, 0.9603058695793152, 0.5085645914077759, 0.5836876630783081, 0.625141441822052, 0.5628799200057983, 0.5735447406768799, 0.5111878514289856, 0.6233432292938232, 0.9607218503952026, 0.5263518691062927, 0.5569582581520081, 0.6199021935462952, 0.5756685733795166, 0.6041695475578308, 0.508154571056366, 0.6402876377105713, 0.9585875272750854, 0.5538586974143982, 0.5678485631942749, 0.6296999454498291, 0.5494995713233948, 0.5896012187004089, 0.5191649794578552, 0.6971978545188904, 0.9625261425971985, 0.5488643050193787, 0.5479834079742432, 0.6349250078201294, 0.6286574006080627, 0.5936946868896484, 0.5000832080841064, 0.6790342330932617, 0.9588927030563354, 0.5497488975524902, 0.5686516165733337, 0.6430258750915527, 0.5431102514266968, 0.6069801449775696, 0.5492006540298462, 0.7375097274780273, 0.9622741937637329, 0.5423632264137268, 0.6147520542144775, 0.6412913799285889, 0.5275691151618958, 0.559380829334259, 0.5740576386451721, 0.6947152018547058, 0.9683411717414856, 0.5008893609046936, 0.6174162030220032, 0.6329573392868042, 0.512502908706665, 0.6067651510238647, 0.5564916133880615, 0.6276981234550476, 0.9668615460395813, 0.5172112584114075, 0.6371222138404846, 0.636935830116272, 0.5160049796104431, 0.562523365020752, 0.5069704055786133, 0.6584154367446899, 0.9674330353736877, 0.5006488561630249, 0.6255680918693542, 0.6156014800071716, 0.527510404586792, 0.502936601638794, 0.5372975468635559, 0.6925228834152222, 0.9627139568328857, 0.5221405625343323, 0.6420965790748596, 0.6064238548278809, 0.5714127421379089, 0.5143203139305115, 0.633124589920044, 0.6866567134857178, 0.9608091115951538, 0.5413196086883545, 0.6335989236831665, 0.5916942954063416, 0.6100490689277649, 0.5303539633750916, 0.6026687622070312, 0.6661536693572998, 0.964219868183136, 0.5179711580276489, 0.6369747519493103, 0.5575714111328125, 0.5928385853767395, 0.5011016130447388, 0.7221977710723877, 0.646004855632782, 0.9661054015159607, 0.5200213193893433, 0.6108001470565796, 0.556297779083252, 0.5972808599472046, 0.5948624610900879, 0.5544782280921936, 0.6256353855133057, 0.9626373052597046, 0.5220068693161011, 0.6215165853500366, 0.5735613703727722, 0.526220440864563, 0.5512984991073608, 0.5709525346755981, 0.6523801684379578, 0.9623358845710754, 0.5744293928146362, 0.6124233603477478, 0.5690606832504272, 0.5759045481681824, 0.5514429807662964, 0.5781605243682861, 0.6565610766410828, 0.9580599069595337, 0.5969073176383972, 0.5842282772064209, 0.5910236239433289, 0.5143795609474182, 0.5627613663673401, 0.5205168724060059, 0.6164050102233887, 0.9573284983634949, 0.5798311829566956, 0.5875779390335083, 0.579093873500824, 0.5012414455413818, 0.5266728401184082, 0.5285720229148865, 0.6481989026069641, 0.9620469808578491, 0.573993444442749, 0.5809480547904968, 0.5679125785827637, 0.5339357256889343, 0.5897126197814941, 0.5361666083335876, 0.599506676197052, 0.9542966485023499, 0.5543981790542603, 0.618345320224762, 0.5775920152664185, 0.5413629412651062, 0.5216019749641418, 0.5005265474319458, 0.590600311756134, 0.9449096322059631, 0.5953083038330078, 0.5903235673904419, 0.5495215654373169, 0.5048271417617798, 0.5113500356674194, 0.5167686343193054, 0.5503852367401123, 0.9401313662528992, 0.58779376745224, 0.6114193797111511, 0.5461831092834473, 0.5068773627281189, 0.5413125157356262, 0.5425931215286255, 0.5686653852462769, 0.9296393394470215, 0.6056882739067078, 0.6231245994567871, 0.5520923137664795, 0.51829993724823, 0.5246121883392334, 0.5233319997787476, 0.5074049234390259, 0.9418405890464783, 0.6093060374259949, 0.652444064617157, 0.5414736866950989, 0.5174842476844788, 0.5059629678726196, 0.5183529257774353, 0.5474992990493774, 0.9348536133766174, 0.6236537098884583, 0.6677207350730896, 0.5459887385368347, 0.5371001362800598, 0.5131105780601501, 0.6231729984283447, 0.5126600861549377, 0.9365159869194031, 0.6096549034118652, 0.682475745677948, 0.5539063215255737, 0.5080770254135132, 0.5171994566917419, 0.626005232334137, 0.5654550790786743, 0.9471811056137085, 0.5827025175094604, 0.6728176474571228, 0.5538729429244995, 0.5400084853172302, 0.5703200101852417, 0.682452380657196, 0.5002726912498474, 0.9495975375175476, 0.5302421450614929, 0.6851639151573181, 0.577130913734436, 0.5061661005020142, 0.5851995944976807, 0.7096270322799683, 0.5663694739341736, 0.9510200023651123, 0.5325374603271484, 0.6833961606025696, 0.5735061168670654, 0.5352960228919983, 0.5573710203170776, 0.6694538593292236, 0.5100498795509338, 0.9457597136497498, 0.5401496887207031, 0.6746714115142822, 0.565832793712616, 0.5575692653656006, 0.5793609023094177, 0.7063636183738708, 0.532220721244812, 0.9374055862426758, 0.5175784230232239, 0.6770970821380615, 0.5641002655029297, 0.558933436870575, 0.5626093745231628, 0.6810133457183838, 0.5506197810173035, 0.9291677474975586, 0.5440833568572998, 0.6529435515403748, 0.5508036017417908, 0.5793411135673523, 0.595936119556427, 0.7080357670783997, 0.5422679781913757, 0.9208317995071411, 0.5317239761352539, 0.6648457050323486, 0.5361824035644531, 0.5631365180015564, 0.5708101987838745, 0.687789261341095, 0.562531590461731, 0.9030740857124329, 0.59075528383255, 0.6396485567092896, 0.5005900263786316, 0.6197270750999451, 0.5968234539031982, 0.6620411276817322, 0.5876453518867493, 0.9006513357162476, 0.614223062992096, 0.6282885074615479, 0.5021616816520691, 0.6372432708740234, 0.5946117043495178, 0.6053559184074402, 0.5880727171897888, 0.9167073965072632, 0.6303122639656067, 0.6228412985801697, 0.5361267328262329, 0.6811285614967346, 0.6066591739654541, 0.6295144557952881, 0.6114526391029358, 0.9219009280204773, 0.6527168154716492, 0.6232224702835083, 0.5042871832847595, 0.6800559759140015, 0.5764805674552917, 0.6303754448890686, 0.6347751617431641, 0.9312875270843506, 0.6281201243400574, 0.620104193687439, 0.5071343779563904, 0.7290176749229431, 0.533924400806427, 0.6933544278144836, 0.6332550644874573, 0.9366618990898132, 0.6324308514595032, 0.6297959685325623, 0.5127912163734436, 0.7131359577178955, 0.594969630241394, 0.7148592472076416, 0.6492907404899597, 0.9355401992797852, 0.6321134567260742, 0.6479412913322449, 0.5400816202163696, 0.7436003088951111, 0.5871151089668274, 0.710604727268219, 0.6218443512916565, 0.9317614436149597, 0.622215986251831, 0.6413281559944153, 0.5288306474685669, 0.7464357614517212, 0.567509114742279, 0.7347210645675659, 0.6153706908226013, 0.9117304086685181, 0.6276357769966125, 0.6523181796073914, 0.5430199503898621, 0.7729026079177856, 0.5063284635543823, 0.7136643528938293, 0.5740336775779724, 0.8904784321784973, 0.6148098111152649, 0.6271264553070068, 0.5629697442054749, 0.8232538104057312, 0.5409085750579834, 0.7379019260406494, 0.5598668456077576, 0.860994815826416, 0.5675792098045349, 0.6354316473007202, 0.559400200843811, 0.8565319776535034, 0.6009817719459534, 0.7053783535957336, 0.5358737111091614, 0.820284366607666, 0.5532202124595642, 0.5960665345191956, 0.5316365361213684, 0.8744682669639587, 0.5572221279144287, 0.7065823674201965, 0.5025580525398254, 0.8120314478874207, 0.5525488257408142, 0.589047908782959, 0.5368195176124573, 0.8425827622413635, 0.5647026896476746, 0.6752328872680664, 0.5035395622253418, 0.7811497449874878, 0.5921812653541565, 0.5737351775169373, 0.5035297870635986, 0.8586444854736328, 0.5789293646812439, 0.6597628593444824, 0.5123729705810547, 0.7370755076408386, 0.6441879272460938, 0.5458237528800964, 0.5367573499679565, 0.8481815457344055, 0.5014106035232544, 0.6531734466552734, 0.5015067458152771, 0.6631255745887756, 0.6133254170417786, 0.5495700836181641, 0.5530818104743958, 0.8167357444763184, 0.5338391065597534, 0.6417657732963562, 0.5220723748207092, 0.532084584236145, 0.6269423961639404, 0.5484673380851746, 0.541252076625824, 0.7705577611923218, 0.5140827894210815, 0.6526373624801636, 0.5001559257507324, 0.5021375417709351, 0.6123369336128235, 0.5757723450660706, 0.5705788135528564, 0.7354623079299927, 0.5089166164398193, 0.646591067314148, 0.520101010799408, 0.6233710050582886, 0.6137776374816895, 0.5745863318443298, 0.5701664686203003, 0.7412784099578857, 0.532497227191925, 0.6610772013664246, 0.5509493947029114, 0.7130511999130249, 0.6193386912345886, 0.5643287897109985, 0.5718811750411987, 0.7312572598457336, 0.5320919156074524, 0.6201817989349365, 0.526151716709137, 0.8229535818099976, 0.5968176126480103, 0.5634762048721313, 0.564483106136322, 0.7217048406600952, 0.5044804215431213, 0.6349365711212158, 0.5347057580947876, 0.8585442900657654, 0.5604641437530518, 0.5676755309104919, 0.5604470372200012, 0.7454456090927124, 0.5633850693702698, 0.5850175619125366, 0.5176452398300171, 0.904732346534729, 0.555054247379303, 0.5560342073440552, 0.5421786308288574, 0.8323713541030884, 0.604056179523468, 0.6475995182991028, 0.5326317548751831, 0.9281355738639832, 0.5025550127029419, 0.5631684064865112, 0.5645836591720581, 0.8562571406364441, 0.5905493497848511, 0.6362324953079224, 0.5306344032287598, 0.9397173523902893, 0.5224828720092773, 0.5298457741737366, 0.5389364361763, 0.9129382371902466, 0.5499567985534668, 0.6349610090255737, 0.5485332608222961, 0.9533554315567017, 0.5099157094955444, 0.5180524587631226, 0.526526927947998, 0.8669700622558594, 0.5385560989379883, 0.6367753148078918, 0.5487992167472839, 0.9585128426551819, 0.5024550557136536, 0.5258351564407349, 0.5458301305770874, 0.8534355163574219, 0.5266374945640564, 0.6027517914772034, 0.5104992389678955, 0.964419960975647, 0.5514761805534363, 0.5081418752670288, 0.5460026860237122, 0.8379423022270203, 0.5116401314735413, 0.6058170795440674, 0.516450047492981, 0.9683588743209839, 0.516218900680542, 0.504891037940979, 0.5608119964599609, 0.816856861114502, 0.5202844142913818, 0.584665834903717, 0.5228381752967834, 0.9720840454101562, 0.5713750720024109, 0.5328747630119324, 0.547592282295227, 0.7781552672386169, 0.5048282742500305, 0.5626892447471619, 0.5164275765419006, 0.9772031307220459, 0.5771293044090271, 0.5438743233680725, 0.5371909737586975, 0.7749578356742859, 0.5369859337806702, 0.5542891025543213, 0.5449487566947937, 0.9810020327568054, 0.5909802913665771, 0.5505070686340332, 0.5396177768707275, 0.7858741879463196, 0.523419976234436, 0.549720048904419, 0.5039878487586975, 0.9841722846031189, 0.6367303729057312, 0.5549347400665283, 0.5605245232582092, 0.8248614072799683, 0.5981954336166382, 0.5043489933013916, 0.5465548038482666, 0.9864857196807861, 0.6553736329078674, 0.5872516632080078, 0.5287942290306091, 0.8209556937217712, 0.5961176156997681, 0.5556493401527405, 0.5352131128311157, 0.9878877401351929, 0.6774783730506897, 0.5771847367286682, 0.5551291108131409, 0.825376570224762, 0.5907307267189026, 0.5348707437515259, 0.5143506526947021, 0.9885912537574768, 0.658778727054596, 0.5847567319869995, 0.5361632704734802, 0.82745361328125, 0.6235777735710144, 0.5694504976272583, 0.5207599997520447, 0.9892856478691101, 0.6475067734718323, 0.5630102753639221, 0.5270602703094482, 0.8328713178634644, 0.5096192359924316, 0.6047224402427673, 0.5166842937469482, 0.989626407623291, 0.6305893063545227, 0.5516608357429504, 0.5302525162696838, 0.8150079846382141, 0.500633955001831, 0.5632620453834534, 0.5145857930183411, 0.9900797605514526, 0.6822366118431091, 0.5763106346130371, 0.5147848129272461, 0.8154031038284302, 0.5446394085884094, 0.589606761932373, 0.5203683972358704, 0.9904068112373352, 0.634899377822876, 0.5550370812416077, 0.514528751373291, 0.7873321771621704, 0.5178022384643555, 0.5632824301719666, 0.5331297516822815, 0.9908117055892944, 0.64326411485672, 0.5799199938774109, 0.5179186463356018, 0.7704020142555237, 0.5105363130569458, 0.5561065077781677, 0.5060524344444275, 0.9909387826919556, 0.6058872938156128, 0.5494706034660339, 0.5492219924926758, 0.7746491432189941, 0.5094844102859497, 0.5649781823158264, 0.5299704074859619, 0.9910683631896973, 0.5544624924659729, 0.5422348380088806, 0.5495418310165405, 0.68465656042099, 0.5302447080612183, 0.538898229598999, 0.5430214405059814, 0.9910938143730164, 0.5845953822135925, 0.5488697290420532, 0.5516963005065918, 0.7765944004058838, 0.52181077003479, 0.5298340320587158, 0.5442975759506226, 0.9911380410194397, 0.5557405352592468, 0.5410171151161194, 0.5282818078994751, 0.6687614321708679, 0.5733727216720581, 0.5112327337265015, 0.515177845954895, 0.9911012649536133, 0.5930559039115906, 0.5783265233039856, 0.5355695486068726, 0.7111259698867798, 0.5767850875854492, 0.524080216884613, 0.5458375811576843, 0.9909452199935913, 0.5852660536766052, 0.5707386136054993, 0.5386294722557068, 0.684866189956665, 0.5692565441131592, 0.5600325465202332, 0.5108506083488464, 0.9908088445663452, 0.5607767105102539, 0.5805875062942505, 0.5313195586204529, 0.6528058052062988, 0.5823272466659546, 0.5914019346237183, 0.5665666460990906, 0.9909263849258423, 0.5898518562316895, 0.5941144824028015, 0.5180042386054993, 0.7152143716812134, 0.5753305554389954, 0.5834513306617737, 0.5437000393867493, 0.9908787608146667, 0.609268307685852, 0.5692086219787598, 0.5111711025238037, 0.6354734301567078, 0.5834885239601135, 0.5603612661361694, 0.5064302086830139, 0.9907329082489014, 0.6254386901855469, 0.5823575854301453, 0.5057694315910339, 0.6057024598121643, 0.5689027905464172, 0.5717725157737732, 0.502726674079895, 0.9904950857162476, 0.683881938457489, 0.5700875520706177, 0.5222284197807312, 0.5304042100906372, 0.5459068417549133, 0.5300660133361816, 0.5253142714500427, 0.9902955889701843, 0.6361081600189209, 0.5378136038780212, 0.5036463141441345, 0.5058349370956421, 0.602316677570343, 0.5450996160507202, 0.5108148455619812, 0.9901643395423889, 0.6611564755439758, 0.544579029083252, 0.5184184908866882, 0.5022680759429932, 0.603872537612915, 0.5259075164794922, 0.5121544599533081, 0.9903119802474976, 0.6394235491752625, 0.5306522846221924, 0.536526083946228, 0.5369153618812561, 0.6111969351768494, 0.520315408706665, 0.5552746653556824, 0.9902978539466858, 0.603141188621521, 0.5092145204544067, 0.5133014917373657, 0.513161838054657, 0.6589221954345703, 0.565125584602356, 0.5617620944976807, 0.9908005595207214, 0.6837671995162964, 0.5641014575958252, 0.539948046207428, 0.5464721322059631, 0.5829650163650513, 0.5579835772514343, 0.6528886556625366, 0.9911909699440002, 0.6591600775718689, 0.5116490721702576, 0.5418255925178528, 0.571214497089386, 0.6206617951393127, 0.5763007402420044, 0.5826722383499146, 0.9912102222442627, 0.6473739147186279, 0.5497145056724548, 0.5238308310508728, 0.5815317034721375, 0.5831629633903503, 0.5903951525688171, 0.6622587442398071, 0.9916083216667175, 0.6407648921012878, 0.5444461703300476, 0.5211083292961121, 0.5814604759216309, 0.5520583987236023, 0.5056154727935791, 0.6105309128761292, 0.9916531443595886, 0.5788936614990234, 0.5078995823860168, 0.5134425759315491, 0.5144299268722534, 0.5603223443031311, 0.5268393754959106, 0.5752139091491699, 0.9916728138923645, 0.5684319734573364, 0.5050881505012512, 0.5238671898841858, 0.5519502758979797, 0.5583445429801941, 0.5613698363304138, 0.5646570920944214, 0.9916313290596008, 0.5922960042953491, 0.5165418386459351, 0.5025321841239929, 0.5608099699020386, 0.5839911103248596, 0.518626868724823, 0.5266028642654419, 0.9914804697036743, 0.5636233687400818, 0.5390723943710327, 0.5412200689315796, 0.5183523297309875, 0.5622108578681946, 0.521447479724884, 0.5191975831985474, 0.9917128086090088, 0.5789491534233093, 0.5036069750785828, 0.5317527055740356, 0.5221205353736877, 0.5984179973602295, 0.518454909324646, 0.5213999152183533, 0.9913950562477112, 0.5621164441108704, 0.5271008014678955, 0.5458285808563232, 0.533211350440979, 0.5775591731071472, 0.5597555041313171, 0.5172263383865356, 0.9913723468780518, 0.5382265448570251, 0.5517898797988892, 0.5600162148475647, 0.5387405157089233, 0.6032370924949646, 0.5063696503639221, 0.5013663172721863, 0.9910462498664856, 0.5950555205345154, 0.5297631621360779, 0.5198544859886169, 0.5138435363769531, 0.618293285369873, 0.5565502643585205, 0.5431397557258606, 0.9909458160400391, 0.5545868277549744, 0.5756544470787048, 0.5433786511421204, 0.5061575174331665, 0.6199786067008972, 0.5658838152885437, 0.5705165863037109, 0.9911256432533264, 0.5815883278846741, 0.5556977987289429, 0.5490451455116272, 0.5142090320587158, 0.6059722900390625, 0.5560867190361023, 0.5876240134239197, 0.9907763600349426, 0.5755617618560791, 0.5927820205688477, 0.5580747127532959, 0.5007967948913574, 0.6309077739715576, 0.5813562273979187, 0.5088967680931091, 0.990467369556427, 0.5311854481697083, 0.6071789264678955, 0.570250928401947, 0.5056692361831665, 0.6295363903045654, 0.5159403085708618, 0.5019915699958801, 0.9902335405349731, 0.5579767227172852, 0.6108853220939636, 0.537222683429718, 0.5595407485961914, 0.6283665895462036, 0.5111117362976074, 0.5072113871574402, 0.9898332357406616, 0.526400625705719, 0.6183392405509949, 0.5378293991088867, 0.509652316570282, 0.6615116596221924, 0.5138437151908875, 0.5060358047485352, 0.9892248511314392, 0.5387958884239197, 0.6026856303215027, 0.5367575287818909, 0.5579037070274353, 0.6115763187408447, 0.5082398056983948, 0.5649639368057251, 0.988055944442749, 0.5458627343177795, 0.6139713525772095, 0.540123701095581, 0.505064070224762, 0.6522623896598816, 0.5447006225585938, 0.5112629532814026, 0.9870991110801697, 0.5131377577781677, 0.627812922000885, 0.5787459015846252, 0.517155647277832, 0.6217076182365417, 0.5582063794136047, 0.5472277402877808, 0.9864765405654907, 0.5177172422409058, 0.6580620408058167, 0.6138721704483032, 0.511083722114563, 0.5957078337669373, 0.5448102951049805, 0.5097535848617554, 0.9867728352546692, 0.5626301169395447, 0.6538184881210327, 0.6251669526100159, 0.5440595746040344, 0.6337571740150452, 0.5565446019172668, 0.5164915323257446, 0.9867161512374878, 0.5624472498893738, 0.6546472311019897, 0.6296993494033813, 0.510084331035614, 0.5742929577827454, 0.5262682437896729, 0.5236776471138, 0.9865009188652039, 0.538203239440918, 0.6465343832969666, 0.6260367035865784, 0.5009763240814209, 0.5944507718086243, 0.5093457698822021, 0.5362400412559509, 0.9853575825691223, 0.5556592345237732, 0.6611308455467224, 0.6238889098167419, 0.5194445848464966, 0.5457902550697327, 0.5110222101211548, 0.5151510834693909, 0.9839293956756592, 0.5501521229743958, 0.668001115322113, 0.64234459400177, 0.5256983041763306, 0.5164605975151062, 0.5236648917198181, 0.5134803652763367, 0.9834989905357361, 0.5381469130516052, 0.6611879467964172, 0.6160855293273926, 0.5041034817695618, 0.5085890889167786, 0.5028602480888367, 0.5385095477104187, 0.9822024703025818, 0.5515022277832031, 0.6539151668548584, 0.6200462579727173, 0.545179545879364, 0.5424178242683411, 0.548804521560669, 0.5011503100395203, 0.9823271036148071, 0.530838131904602, 0.6237954497337341, 0.5952227115631104, 0.5167407393455505, 0.5199204683303833, 0.5543243288993835, 0.5111786723136902, 0.9807482957839966, 0.523686945438385, 0.6377424597740173, 0.587664783000946, 0.5663607716560364, 0.555427074432373, 0.5245023369789124, 0.5030896663665771, 0.9761050939559937, 0.501723051071167, 0.6426755785942078, 0.6173759698867798, 0.5886377692222595, 0.5437537431716919, 0.5162675976753235, 0.5506988763809204, 0.9716111421585083, 0.5193356275558472, 0.6427502036094666, 0.6014147996902466, 0.5562053322792053, 0.533981204032898, 0.5081782341003418, 0.5029056668281555, 0.9584288001060486, 0.5300729870796204, 0.6654090285301208, 0.6130194067955017, 0.5720704197883606, 0.5589983463287354, 0.5041571259498596, 0.5186629295349121, 0.9470584392547607, 0.5079810619354248, 0.6363071799278259, 0.6013833284378052, 0.5362409949302673, 0.5670359134674072, 0.5037829279899597, 0.5207135081291199, 0.9209901094436646, 0.5011703372001648, 0.6464217901229858, 0.5960244536399841, 0.5416789650917053, 0.5847111940383911, 0.5025627017021179, 0.508963406085968, 0.9036551117897034, 0.5223456025123596, 0.6607714891433716, 0.6011272668838501, 0.5328545570373535, 0.5881079435348511, 0.5026645660400391, 0.5351347923278809, 0.9070740342140198, 0.5142179131507874, 0.6596425175666809, 0.5975362062454224, 0.5228673219680786, 0.6288431286811829, 0.5121276378631592, 0.506152331829071, 0.8737770318984985, 0.5181061029434204, 0.6696118712425232, 0.5982204079627991, 0.5523685812950134, 0.596538782119751, 0.5009711384773254, 0.5058644413948059, 0.8403624296188354, 0.5071290731430054, 0.6658668518066406, 0.6058242917060852, 0.5020607709884644, 0.6269570589065552, 0.5277336239814758, 0.5205016136169434, 0.684265673160553, 0.5477117896080017, 0.6434418559074402, 0.5996866226196289, 0.5132193565368652, 0.6177508234977722, 0.5082768797874451, 0.5060827136039734, 0.5623586773872375, 0.5743491053581238, 0.6510096192359924, 0.6053051352500916, 0.5051054954528809, 0.6466891765594482, 0.5358565449714661, 0.5116453766822815, 0.5085780024528503, 0.6325749754905701, 0.6287710666656494, 0.58659827709198, 0.5522558093070984, 0.6760506629943848, 0.583591878414154, 0.5237424969673157, 0.6972321271896362, 0.7055214643478394, 0.5666735172271729, 0.5327702760696411, 0.6443666815757751, 0.6604650616645813, 0.5701679587364197, 0.5004265904426575, 0.6144458651542664, 0.6846526861190796, 0.582587718963623, 0.5436160564422607, 0.6331793069839478, 0.6321246027946472, 0.5884881615638733, 0.5212140679359436, 0.7094213366508484, 0.7557534575462341, 0.5488458871841431, 0.5276749730110168, 0.6916301846504211, 0.6395606398582458, 0.5809330344200134, 0.5190052390098572, 0.6715589165687561, 0.7152040600776672, 0.5527244806289673, 0.5333431959152222, 0.6985167860984802, 0.6303712129592896, 0.6071833372116089, 0.5156275033950806, 0.6624082922935486, 0.7648687958717346, 0.5387768745422363, 0.5203296542167664, 0.7381786108016968, 0.5958008170127869, 0.6124041676521301, 0.5190829634666443, 0.7326682806015015, 0.7568007707595825, 0.5066836476325989, 0.5025808215141296, 0.7500311136245728, 0.6184746623039246, 0.5944879651069641, 0.54423588514328, 0.681052029132843, 0.717694878578186, 0.5397098660469055, 0.5126634836196899, 0.7261292338371277, 0.5706933736801147, 0.5462487936019897, 0.5039658546447754, 0.803454577922821, 0.7682393193244934, 0.5614683628082275, 0.520068347454071, 0.701848030090332, 0.6034287810325623, 0.5503066778182983, 0.506386399269104, 0.8726630806922913, 0.6898435354232788, 0.585824728012085, 0.5246397256851196, 0.6725329756736755, 0.6101469397544861, 0.5670599937438965, 0.5215517282485962, 0.8716926574707031, 0.7183938026428223, 0.5888523459434509, 0.5226122140884399, 0.6408311724662781, 0.6074641942977905, 0.5676263570785522, 0.5204293131828308, 0.8653128147125244, 0.7164685726165771, 0.5983262062072754, 0.5199536085128784, 0.5950177907943726, 0.6257436275482178, 0.5677974820137024, 0.5502192378044128, 0.866603434085846, 0.7139156460762024, 0.5779418349266052, 0.5291134715080261, 0.5859598517417908, 0.6048162579536438, 0.5828975439071655, 0.524333655834198, 0.8810088634490967, 0.7645606398582458, 0.5629518628120422, 0.5363878607749939, 0.5779877305030823, 0.6103837490081787, 0.6085418462753296, 0.5254231691360474, 0.8952476382255554, 0.7539519667625427, 0.5428056120872498, 0.536579430103302, 0.5917986035346985, 0.6097632050514221, 0.590217113494873, 0.5027103424072266, 0.8912437558174133, 0.7395862936973572, 0.5451803207397461, 0.5599796772003174, 0.5903116464614868, 0.6111775636672974, 0.5704414248466492, 0.502848207950592, 0.8460655808448792, 0.761767566204071, 0.5545756816864014, 0.5806734561920166, 0.6150315999984741, 0.6008558869361877, 0.580100953578949, 0.5336413979530334, 0.8113568425178528, 0.7634369730949402, 0.5633288621902466, 0.6030982136726379, 0.5839362144470215, 0.640466570854187, 0.6222656965255737, 0.5460028648376465, 0.7596603035926819, 0.794825553894043, 0.5132580995559692, 0.5898742079734802, 0.6279318928718567, 0.6134850382804871, 0.6269960403442383, 0.5144256353378296, 0.7188583016395569, 0.8097365498542786, 0.5070663094520569, 0.5708757638931274, 0.6723566055297852, 0.6076213121414185, 0.6297426819801331, 0.5535959005355835, 0.7095305323600769, 0.7903287410736084, 0.5382232069969177, 0.5706608891487122, 0.6114718317985535, 0.5992911458015442, 0.5910822749137878, 0.543575644493103, 0.699783444404602, 0.7963478565216064, 0.5440121293067932, 0.577687680721283, 0.6612873077392578, 0.5719884037971497, 0.6039833426475525, 0.5504810214042664, 0.7103427052497864, 0.7869417071342468, 0.5675111413002014, 0.5698827505111694, 0.6249846816062927, 0.553037166595459, 0.5870293378829956, 0.5379459857940674, 0.6289519667625427, 0.7861076593399048, 0.5353694558143616, 0.5745031833648682, 0.5978453755378723, 0.5479035377502441, 0.5923632979393005, 0.5519009232521057, 0.5124567151069641, 0.7776334881782532, 0.5548079609870911, 0.6002625823020935, 0.6428377032279968, 0.5058210492134094, 0.5739294290542603, 0.5502705574035645, 0.5135233402252197, 0.7540067434310913, 0.5971434712409973, 0.604836106300354, 0.5582429766654968, 0.5019108057022095, 0.5494583249092102, 0.5169317126274109, 0.5638673901557922, 0.7286701202392578, 0.6129243969917297, 0.6409144997596741, 0.5363402366638184, 0.5196629762649536, 0.5541512966156006, 0.5006939172744751, 0.5661560297012329, 0.7587732076644897, 0.6042274236679077, 0.6200642585754395, 0.5516884326934814, 0.5008112788200378, 0.5573092699050903, 0.5169572234153748, 0.6153577566146851, 0.7359183430671692, 0.6084776520729065, 0.6396027207374573, 0.532523512840271, 0.5413498878479004, 0.5878028273582458, 0.5338969230651855, 0.6196103096008301, 0.757385790348053, 0.5959135890007019, 0.6457095742225647, 0.5790356993675232, 0.5253534913063049, 0.5562673807144165, 0.521318793296814, 0.5844783186912537, 0.73414146900177, 0.6154934167861938, 0.659027099609375, 0.5248362421989441, 0.5410140156745911, 0.5953570604324341, 0.5664443969726562, 0.5797979235649109, 0.7459179162979126, 0.6121518611907959, 0.6543952822685242, 0.5161685943603516, 0.5567179322242737, 0.5520684719085693, 0.5310735106468201, 0.5687678456306458, 0.7746285796165466, 0.5818613767623901, 0.6445320844650269, 0.5098922252655029, 0.5787873268127441, 0.5675326585769653, 0.5693919658660889, 0.6176360249519348, 0.7276430130004883, 0.5922453999519348, 0.6469389796257019, 0.517514705657959, 0.5923953056335449, 0.5459704995155334, 0.5529358983039856, 0.6764498949050903, 0.7695390582084656, 0.5816627740859985, 0.6649122834205627, 0.524357795715332, 0.5945575833320618, 0.5378432869911194, 0.500602662563324, 0.6979103684425354, 0.6768360733985901, 0.5823490023612976, 0.6786118149757385, 0.5030717253684998, 0.5717692375183105, 0.5571736097335815, 0.540264904499054, 0.758011519908905, 0.7227524518966675, 0.5816280245780945, 0.6829147338867188, 0.5357514023780823, 0.5890061259269714, 0.5285446643829346, 0.5228521227836609, 0.7994470000267029, 0.6456233859062195, 0.5788910388946533, 0.6960980296134949, 0.508233904838562, 0.5884907841682434, 0.5449309349060059, 0.5526126623153687, 0.8455098867416382, 0.5985788106918335, 0.5967568159103394, 0.6809525489807129, 0.5144525170326233, 0.560612678527832, 0.537010669708252, 0.5053719282150269, 0.8872783184051514, 0.6143345236778259, 0.6176676750183105, 0.68503338098526, 0.5004585981369019, 0.5629732012748718, 0.5643340945243835, 0.5238444209098816, 0.9212338328361511, 0.5303153991699219, 0.6242411136627197, 0.6818116307258606, 0.5066334009170532, 0.5768646597862244, 0.5803217887878418, 0.5125330090522766, 0.9420759677886963, 0.5046530961990356, 0.6024000644683838, 0.6762756109237671, 0.5284885168075562, 0.5867542028427124, 0.5944148898124695, 0.5406899452209473, 0.9523687958717346, 0.5053028464317322, 0.6158931851387024, 0.69219970703125, 0.5130996108055115, 0.5970603227615356, 0.6073460578918457, 0.559803307056427, 0.9659273624420166, 0.5101215839385986, 0.6165648698806763, 0.6839353442192078, 0.558364748954773, 0.5924449563026428, 0.6157846450805664, 0.5463337898254395, 0.9711335897445679, 0.5085532069206238, 0.6196116805076599, 0.6855796575546265, 0.5839696526527405, 0.5792894959449768, 0.604521632194519, 0.5224577784538269, 0.9745050668716431, 0.5152753591537476, 0.628682553768158, 0.6791828870773315, 0.607177197933197, 0.6107568740844727, 0.6230443716049194, 0.5433926582336426, 0.9786617755889893, 0.5282914042472839, 0.605246901512146, 0.6702245473861694, 0.6109460592269897, 0.6283438801765442, 0.6202441453933716, 0.5719127655029297, 0.9806453585624695, 0.544821560382843, 0.5917294025421143, 0.6781414151191711, 0.5817756652832031, 0.6416710615158081, 0.6158172488212585, 0.5663749575614929, 0.9825233221054077, 0.5669325590133667, 0.5729673504829407, 0.6838021278381348, 0.6097002029418945, 0.633897602558136, 0.5970948934555054, 0.5406413674354553, 0.9844484329223633, 0.5345994830131531, 0.5527273416519165, 0.7013823986053467, 0.6289210319519043, 0.5992998480796814, 0.5623282194137573, 0.5303624868392944, 0.9846439361572266, 0.511241614818573, 0.5528944730758667, 0.7150174975395203, 0.628391683101654, 0.5853172540664673, 0.5663829445838928, 0.547341525554657, 0.9849625825881958, 0.5545494556427002, 0.5407625436782837, 0.7066539525985718, 0.6310203075408936, 0.5742456316947937, 0.5774308443069458, 0.5392274856567383, 0.9854170680046082, 0.5761606097221375, 0.5272728204727173, 0.7161316275596619, 0.6171807050704956, 0.543727457523346, 0.5603018403053284, 0.5319923758506775, 0.986517071723938, 0.576708197593689, 0.5250299572944641, 0.7066054344177246, 0.6248958706855774, 0.5495156645774841, 0.5669782161712646, 0.5351718068122864, 0.9870708584785461, 0.5786608457565308, 0.5338304042816162, 0.71198570728302, 0.6576864719390869, 0.5649807453155518, 0.5812506079673767, 0.539345383644104, 0.9873765707015991, 0.5983262062072754, 0.5450634956359863, 0.706089973449707, 0.6439409255981445, 0.5593987107276917, 0.612795889377594, 0.5524299740791321, 0.9879173636436462, 0.5807799100875854, 0.551756739616394, 0.679135799407959, 0.6507177352905273, 0.567172110080719, 0.6419976949691772, 0.5352636575698853, 0.9884472489356995, 0.5803591012954712, 0.5369505882263184, 0.6640865206718445, 0.6304979920387268, 0.5365797877311707, 0.6227413415908813, 0.5283944606781006, 0.9886870384216309, 0.5762695074081421, 0.5283756852149963, 0.6563111543655396, 0.6207306981086731, 0.5522972941398621, 0.6323777437210083, 0.5367437601089478, 0.9888856410980225, 0.5643123984336853, 0.5196552276611328, 0.6534478664398193, 0.6284365057945251, 0.5447713136672974, 0.6228839159011841, 0.5270755290985107, 0.989130973815918, 0.6034494638442993, 0.5191293358802795, 0.6612576842308044, 0.6090877652168274, 0.5416107177734375, 0.6249927878379822, 0.5410663485527039, 0.9892145991325378, 0.6021801233291626, 0.5235065221786499, 0.6624396443367004, 0.6375402808189392, 0.5286620259284973, 0.6108527183532715, 0.5491191744804382, 0.9895188212394714, 0.6066944003105164, 0.5213361382484436, 0.6639798879623413, 0.6423787474632263, 0.5383442640304565, 0.5851874947547913, 0.5294550657272339, 0.9897796511650085, 0.6261749863624573, 0.525458037853241, 0.6815480589866638, 0.6456695795059204, 0.5402648448944092, 0.605823278427124, 0.5505207180976868, 0.9897049069404602, 0.5876951217651367, 0.5003734827041626, 0.6715748310089111, 0.6397244334220886, 0.5263625383377075, 0.6235558986663818, 0.5300528407096863, 0.9899700284004211, 0.6051033139228821, 0.5112238526344299, 0.6850509643554688, 0.633204460144043, 0.5348339676856995, 0.6258420944213867, 0.5446411967277527, 0.990048885345459, 0.5981485247612, 0.5098208785057068, 0.6828235387802124, 0.6495553851127625, 0.5271472334861755, 0.6196784973144531, 0.5523703098297119, 0.9901533722877502, 0.5940019488334656, 0.5063914656639099, 0.6851482391357422, 0.6627011895179749, 0.5307605862617493, 0.6322327256202698, 0.5271176099777222, 0.9902492165565491, 0.5805506706237793, 0.5249282717704773, 0.6839858889579773, 0.634918212890625, 0.5390510559082031, 0.6780154705047607, 0.562202513217926, 0.990159809589386, 0.5468391180038452, 0.5151702761650085, 0.6503251791000366, 0.6147329211235046, 0.513740062713623, 0.6809681057929993, 0.5179786086082458, 0.9904552102088928, 0.5264374017715454, 0.5049703121185303, 0.6555184721946716, 0.5937994718551636, 0.5015019774436951, 0.68329918384552, 0.5396663546562195, 0.9904903173446655, 0.515082836151123, 0.5250615477561951, 0.6518572568893433, 0.587654173374176, 0.5114246010780334, 0.6773635149002075, 0.5372222065925598, 0.9901589751243591, 0.5221739411354065, 0.5325530767440796, 0.6542888283729553, 0.5808407068252563, 0.5078394412994385, 0.6704197525978088, 0.5650409460067749, 0.9898965358734131, 0.5507423281669617, 0.5115786790847778, 0.6563795208930969, 0.5491146445274353, 0.5147925019264221, 0.6825748682022095, 0.541576623916626, 0.989903450012207, 0.5054138898849487, 0.5208923816680908, 0.6475914120674133, 0.5497417449951172, 0.5417917966842651, 0.6817808747291565, 0.5538315176963806, 0.9899622201919556, 0.5085552334785461, 0.5179916024208069, 0.6360673904418945, 0.5584433078765869, 0.566647469997406, 0.6504613161087036, 0.5788983702659607, 0.9898815751075745, 0.5052137970924377, 0.5109800100326538, 0.6477078795433044, 0.5716631412506104, 0.5628710985183716, 0.6904869675636292, 0.5445249080657959, 0.9898389577865601, 0.54813152551651, 0.5757855176925659, 0.6197778582572937, 0.5252769589424133, 0.5753099322319031, 0.6828677654266357, 0.5380316376686096, 0.9895113110542297, 0.5489408373832703, 0.572303295135498, 0.6439009308815002, 0.525928795337677, 0.5714408159255981, 0.6674057245254517, 0.5325452089309692, 0.9894611239433289, 0.5613466501235962, 0.5958948135375977, 0.6549532413482666, 0.5297821164131165, 0.6041941046714783, 0.6608238816261292, 0.5260912179946899, 0.989300549030304, 0.5485197305679321, 0.603103518486023, 0.663897693157196, 0.5622415542602539, 0.663764476776123, 0.606752336025238, 0.5671669840812683, 0.9885081648826599, 0.5205305218696594, 0.5464385151863098, 0.7133608460426331, 0.5945044755935669, 0.6714227199554443, 0.5958912372589111, 0.5194809436798096, 0.9881072640419006, 0.5109929442405701, 0.5812409520149231, 0.7038505673408508, 0.5995235443115234, 0.6630935668945312, 0.5928705930709839, 0.5543214678764343, 0.9876104593276978, 0.5005753040313721, 0.570610523223877, 0.7352142930030823, 0.5693575739860535, 0.6787757277488708, 0.5615034699440002, 0.5653039813041687, 0.9869148135185242, 0.5136174559593201, 0.5882484316825867, 0.7321803569793701, 0.5792537331581116, 0.6897660493850708, 0.5397903323173523, 0.608456552028656, 0.9859433174133301, 0.5090533494949341, 0.5854703783988953, 0.7324774861335754, 0.5837490558624268, 0.7195530533790588, 0.5724229216575623, 0.6291379332542419, 0.9838793873786926, 0.5052138566970825, 0.5802463293075562, 0.7194390296936035, 0.5810862183570862, 0.7141203284263611, 0.5612519383430481, 0.6220147609710693, 0.9823170900344849, 0.5170600414276123, 0.5875718593597412, 0.7284322381019592, 0.6031935811042786, 0.6920831799507141, 0.606658399105072, 0.5830691456794739, 0.9800312519073486, 0.5956190228462219, 0.6009027361869812, 0.7115968465805054, 0.5623563528060913, 0.7257147431373596, 0.577438473701477, 0.5964703559875488, 0.9730381965637207, 0.5699918866157532, 0.6069461107254028, 0.6989175081253052, 0.5745505690574646, 0.746789276599884, 0.5547850728034973, 0.5599686503410339, 0.9613155722618103, 0.6050533652305603, 0.6264179944992065, 0.7034907341003418, 0.5660248398780823, 0.7764880061149597, 0.5663262605667114, 0.5427244901657104, 0.9413922429084778, 0.5476982593536377, 0.6085768938064575, 0.6907492280006409, 0.5525604486465454, 0.7596322298049927, 0.5393527746200562, 0.5489237904548645, 0.9102527499198914, 0.5136719942092896, 0.613540768623352, 0.7071261405944824, 0.569831907749176, 0.7472211122512817, 0.5392034649848938, 0.511299729347229, 0.8527365326881409, 0.5382189154624939, 0.5944705009460449, 0.6749058961868286, 0.5599607825279236, 0.7762715816497803, 0.5287462472915649, 0.5650011897087097, 0.7780706882476807, 0.5052515864372253, 0.5639582276344299, 0.7051889300346375, 0.5892336964607239, 0.776535153388977, 0.5471286177635193, 0.5463446378707886, 0.6830897927284241, 0.5172315835952759, 0.594217836856842, 0.7094014286994934, 0.6080562472343445, 0.7766309976577759, 0.5820140242576599, 0.5416339039802551, 0.5492116212844849, 0.50138920545578, 0.5715094804763794, 0.737831175327301, 0.628473162651062, 0.7565139532089233, 0.588710367679596, 0.5532807111740112, 0.5042192339897156, 0.5284925699234009, 0.580171525478363, 0.7367745637893677, 0.6334689855575562, 0.7318970561027527, 0.597389280796051, 0.5438896417617798, 0.5117084383964539, 0.5474147796630859, 0.5735203623771667, 0.7194790840148926, 0.6331629157066345, 0.754694938659668, 0.6009413599967957, 0.5726462602615356, 0.5158340334892273, 0.5417149066925049, 0.5435371994972229, 0.7350565791130066, 0.6537694334983826, 0.76076340675354, 0.5757838487625122, 0.5872469544410706, 0.630972146987915, 0.5724664330482483, 0.5356042385101318, 0.7358778715133667, 0.6484476327896118, 0.7448891997337341, 0.5711666345596313, 0.5519952774047852, 0.6617594957351685, 0.5806209444999695, 0.5288237929344177, 0.736064612865448, 0.6402016282081604, 0.7611435651779175, 0.5735877156257629, 0.5706311464309692, 0.6014084219932556, 0.60148686170578, 0.518897294998169, 0.7204201221466064, 0.6363776326179504, 0.7608615159988403, 0.5774908661842346, 0.5728691816329956, 0.5783989429473877, 0.6203091144561768, 0.5197253227233887, 0.7196998596191406, 0.6315548419952393, 0.7773085832595825, 0.5930061340332031, 0.5564529895782471, 0.5251744985580444, 0.6538136005401611, 0.5002708435058594, 0.7139576077461243, 0.654559314250946, 0.7661559581756592, 0.5837141275405884, 0.5999721884727478, 0.5166754722595215, 0.6429429650306702, 0.5018419027328491, 0.7101207971572876, 0.6486318707466125, 0.7475556135177612, 0.5569878220558167, 0.5633986592292786, 0.5436775088310242, 0.6901562809944153, 0.500822901725769, 0.6771828532218933, 0.6315727829933167, 0.7642173171043396, 0.5756943225860596, 0.5682087540626526, 0.5982738137245178, 0.653998851776123, 0.508338212966919, 0.6970927715301514, 0.6416512727737427, 0.7630692720413208, 0.5663054585456848, 0.5795302391052246, 0.5479196310043335, 0.623149573802948, 0.5217728614807129, 0.7099739909172058, 0.6391595005989075, 0.7533941268920898, 0.5631036758422852, 0.5322954058647156, 0.5349886417388916, 0.6138523817062378, 0.5231301188468933, 0.7193823456764221, 0.6504113078117371, 0.7623255848884583, 0.5716277360916138, 0.5438107252120972, 0.5017064213752747, 0.5452257394790649, 0.5244945883750916, 0.7212744355201721, 0.6532962918281555, 0.7542942762374878, 0.5666200518608093, 0.5458131432533264, 0.5323347449302673, 0.5623595118522644, 0.5076342821121216, 0.7282221913337708, 0.6534016132354736, 0.7831324934959412, 0.580569326877594, 0.5150495171546936, 0.521943986415863, 0.542927622795105, 0.5038768649101257, 0.7330214381217957, 0.6627651453018188, 0.7593650817871094, 0.594276487827301, 0.5549431443214417, 0.6193890571594238, 0.5275627970695496, 0.5068233609199524, 0.7380375862121582, 0.6600176095962524, 0.7661280632019043, 0.5670053362846375, 0.5246486663818359, 0.6152033805847168, 0.5292707681655884, 0.5142423510551453, 0.7337982058525085, 0.6374329924583435, 0.7798783779144287, 0.5748557448387146, 0.5322888493537903, 0.6359093189239502, 0.5260547995567322, 0.5145973563194275, 0.7283167839050293, 0.6332816481590271, 0.7616972923278809, 0.5824800729751587, 0.5746597051620483, 0.6325069069862366, 0.5170244574546814, 0.5075967311859131, 0.7305700182914734, 0.6325448751449585, 0.7707709074020386, 0.5952385663986206, 0.5619320273399353, 0.7304328083992004, 0.5196825861930847, 0.5022377967834473, 0.7350082993507385, 0.6361434459686279, 0.7174857258796692, 0.6160499453544617, 0.6111405491828918, 0.7817059755325317, 0.5119732022285461, 0.5046440958976746, 0.7440588474273682, 0.6461462378501892, 0.6791375875473022, 0.5836367607116699, 0.5880941152572632, 0.7764739990234375, 0.546014130115509, 0.5024522542953491, 0.7319195866584778, 0.6311272382736206, 0.7273833155632019, 0.557660698890686, 0.5688874125480652, 0.723992109298706, 0.5046465396881104, 0.5019669532775879, 0.7228144407272339, 0.6235348582267761, 0.6974350214004517, 0.5316433310508728, 0.5868889689445496, 0.7149159908294678, 0.5832528471946716, 0.5314520597457886, 0.7250517010688782, 0.6407753229141235, 0.7206498980522156, 0.5093411803245544, 0.5578275918960571, 0.6488324403762817, 0.5541036128997803, 0.5233901739120483, 0.7244018316268921, 0.6414251327514648, 0.673430323600769, 0.5029122233390808, 0.5651968717575073, 0.5670193433761597, 0.5112899541854858, 0.5322917103767395, 0.7203201651573181, 0.6067777872085571, 0.6952692866325378, 0.5010279417037964, 0.5635482668876648, 0.503614604473114, 0.570512592792511, 0.5128419399261475, 0.7180387377738953, 0.6200658679008484, 0.7190169095993042, 0.5153362154960632, 0.5391519665718079, 0.5025506615638733, 0.5609889626502991, 0.5207207798957825, 0.7229754328727722, 0.6263821125030518, 0.702454149723053, 0.5058059096336365, 0.5384681820869446, 0.501642644405365, 0.6203163266181946, 0.5351369380950928, 0.7129569053649902, 0.6344056129455566, 0.6686422824859619, 0.5021055936813354, 0.5147048830986023, 0.5533434152603149, 0.6462502479553223, 0.533481776714325, 0.7039393186569214, 0.6323959231376648, 0.6160838603973389, 0.5130175948143005, 0.5188816785812378, 0.5092648863792419, 0.6886575222015381, 0.5409029126167297, 0.6971678137779236, 0.6003225445747375, 0.5155497193336487, 0.5203226208686829, 0.5352137088775635, 0.6062299013137817, 0.6946111917495728, 0.5154775977134705, 0.6804665327072144, 0.5855503082275391, 0.5112848877906799, 0.5360916256904602, 0.5447197556495667, 0.6493435502052307, 0.715725302696228, 0.5032205581665039, 0.6801113486289978, 0.5764967799186707, 0.5759838223457336, 0.5583721399307251, 0.5604979991912842, 0.7536965608596802, 0.759933352470398, 0.5243643522262573, 0.6785207390785217, 0.540012538433075, 0.6735774278640747, 0.5690929889678955, 0.5586077570915222, 0.8428614139556885, 0.7421835660934448, 0.5237151980400085, 0.6620824337005615, 0.5156314373016357, 0.7147074341773987, 0.5600404143333435, 0.5298914313316345, 0.8900159001350403, 0.7450124025344849, 0.5198323130607605, 0.6500410437583923, 0.520962655544281, 0.7508598566055298, 0.5552706718444824, 0.5426856279373169, 0.9226807951927185, 0.7242697477340698, 0.5069072842597961, 0.6243383884429932, 0.5128350257873535, 0.7496964335441589, 0.5203539729118347, 0.5331149101257324, 0.9383342862129211, 0.689547061920166, 0.5044792890548706, 0.605213463306427, 0.5150426626205444, 0.7287572026252747, 0.520714282989502, 0.5251110196113586, 0.9563905000686646, 0.7186002731323242, 0.5139519572257996, 0.5953722596168518, 0.5536299347877502, 0.7249761819839478, 0.5117847919464111, 0.5214836001396179, 0.9696569442749023, 0.6868781447410583, 0.5269857048988342, 0.5823999643325806, 0.5627604722976685, 0.7142134308815002, 0.5129146575927734, 0.5038219094276428, 0.977220356464386, 0.6883300542831421, 0.5310220122337341, 0.5730616450309753, 0.5831107497215271, 0.7225688099861145, 0.5003418326377869, 0.5063431262969971, 0.981782853603363, 0.6738314032554626, 0.5263482332229614, 0.5685595870018005, 0.5408027172088623, 0.7720901966094971, 0.5296539068222046, 0.5074518918991089, 0.9859100580215454, 0.6390227675437927, 0.519841194152832, 0.5353261828422546, 0.5288316011428833, 0.7799025177955627, 0.5475980639457703, 0.5235776305198669, 0.9879122376441956, 0.6360978484153748, 0.5161192417144775, 0.5229895710945129, 0.5495643019676208, 0.7873515486717224, 0.5614898204803467, 0.5137057304382324, 0.9892078042030334, 0.6330007314682007, 0.5249374508857727, 0.5082387328147888, 0.5554007291793823, 0.7761092185974121, 0.5873587727546692, 0.5047532320022583, 0.9902693033218384, 0.6087440848350525, 0.5123547315597534, 0.5020964741706848, 0.5610319972038269, 0.773918628692627, 0.5880529880523682, 0.5218417644500732, 0.9907875657081604, 0.630128026008606, 0.5147684216499329, 0.5025339126586914, 0.5668114423751831, 0.8042739629745483, 0.5748012661933899, 0.5023881196975708, 0.991519033908844, 0.6238282918930054, 0.5118979215621948, 0.502780020236969, 0.5755955576896667, 0.7922258973121643, 0.6089849472045898, 0.522417426109314, 0.9918820261955261, 0.6316524744033813, 0.5104207992553711, 0.5005865693092346, 0.6156958937644958, 0.7948364019393921, 0.6009144186973572, 0.5077093243598938, 0.9921178221702576, 0.6271916031837463, 0.5226775407791138, 0.5049479603767395, 0.6056070327758789, 0.7645411491394043, 0.5928328037261963, 0.5029079914093018, 0.9922366738319397, 0.6365211606025696, 0.5086625814437866, 0.5497122406959534, 0.5664709806442261, 0.7302963137626648, 0.6096102595329285, 0.5012217164039612, 0.9923438429832458, 0.6503268480300903, 0.5138976573944092, 0.5300125479698181, 0.6007558107376099, 0.7143081426620483, 0.5749337077140808, 0.5258125066757202, 0.9925398230552673, 0.6532253623008728, 0.5124650597572327, 0.5629411935806274, 0.6023592948913574, 0.6940805912017822, 0.6109100580215454, 0.5156430006027222, 0.9925864338874817, 0.6480980515480042, 0.511763870716095, 0.5343444347381592, 0.6009800434112549, 0.6897026896476746, 0.6088883876800537, 0.5021610260009766, 0.9925593137741089, 0.6399505138397217, 0.5120593309402466, 0.5286884903907776, 0.5998362302780151, 0.6863071918487549, 0.5945230722427368, 0.5025678873062134, 0.9925674200057983, 0.6172378659248352, 0.5079143643379211, 0.5258328914642334, 0.5485634207725525, 0.6614189743995667, 0.6057175397872925, 0.50254887342453, 0.9926689863204956, 0.6265848875045776, 0.5034242868423462, 0.5027552247047424, 0.5804016590118408, 0.584463894367218, 0.6073126792907715, 0.5068273544311523, 0.9927055239677429, 0.6243986487388611, 0.5062032341957092, 0.5044017434120178, 0.5608125925064087, 0.5125205516815186, 0.6617199778556824, 0.5015491843223572, 0.9927389025688171, 0.6026719212532043, 0.5074365139007568, 0.5332286953926086, 0.5135278701782227, 0.5431532263755798, 0.6920832395553589, 0.5156971216201782, 0.9927405714988708, 0.6529801487922668, 0.5074320435523987, 0.5193610787391663, 0.5501263737678528, 0.5955537557601929, 0.6816583275794983, 0.5164206624031067, 0.9927489161491394, 0.6370292901992798, 0.5021924376487732, 0.5453976392745972, 0.5264209508895874, 0.6864980459213257, 0.7280207872390747, 0.5137766599655151, 0.9928476810455322, 0.631385326385498, 0.5102033019065857, 0.5662755370140076, 0.5515745878219604, 0.6952686309814453, 0.7355414032936096, 0.5170310139656067, 0.9929220676422119, 0.6391412615776062, 0.5067253112792969, 0.5784427523612976, 0.5788296461105347, 0.7235159873962402, 0.7283721566200256, 0.5347283482551575, 0.9930900931358337, 0.6210763454437256, 0.5019725561141968, 0.5553289651870728, 0.5639042258262634, 0.7427555322647095, 0.7224518060684204, 0.5099405646324158, 0.9930862188339233, 0.6594420671463013, 0.5001690983772278, 0.5611191391944885, 0.5981810688972473, 0.7218102216720581, 0.693329930305481, 0.5158647894859314, 0.9931528568267822, 0.6356769800186157, 0.5095900297164917, 0.5396419167518616, 0.6354118585586548, 0.7667317390441895, 0.7090756297111511, 0.5049135684967041, 0.9931675791740417, 0.6057193875312805, 0.5047652721405029, 0.5857083201408386, 0.6678181290626526, 0.7502919435501099, 0.7245761156082153, 0.5016974210739136, 0.9931825995445251, 0.5997450947761536, 0.5034536719322205, 0.5724096298217773, 0.6762104034423828, 0.769201934337616, 0.7122328877449036, 0.5277872681617737, 0.9932470321655273, 0.5791171789169312, 0.5000892281532288, 0.5691521763801575, 0.6917429566383362, 0.8205894827842712, 0.7460025548934937, 0.5131616592407227, 0.9931657910346985, 0.5798317790031433, 0.504908561706543, 0.6097025871276855, 0.7137762308120728, 0.830720841884613, 0.7057350873947144, 0.5017896294593811, 0.9932030439376831, 0.5938096642494202, 0.5209070444107056, 0.5623597502708435, 0.7003796100616455, 0.8482452034950256, 0.6974495053291321, 0.5042033791542053, 0.9931847453117371, 0.5400376915931702, 0.5164816379547119, 0.6335031390190125, 0.711218535900116, 0.8383399248123169, 0.7179269194602966, 0.5107753276824951, 0.9931729435920715, 0.5438036918640137, 0.5046414732933044, 0.6496366858482361, 0.7201331853866577, 0.8206570744514465, 0.6781069040298462, 0.5137311220169067, 0.9931542873382568, 0.5671422481536865, 0.5098793506622314, 0.6550784111022949, 0.7129712104797363, 0.85293048620224, 0.7000380158424377, 0.517460286617279, 0.9931422472000122, 0.5504176616668701, 0.500847339630127, 0.6968137621879578, 0.7280541658401489, 0.8761246800422668, 0.7003999948501587, 0.5184096097946167, 0.9931265115737915, 0.5873916149139404, 0.5057026147842407, 0.6832219958305359, 0.7435654401779175, 0.8858822584152222, 0.6851980686187744, 0.5217840671539307, 0.9932208061218262, 0.5827488303184509, 0.5203069448471069, 0.7173420190811157, 0.7153980135917664, 0.8874096274375916, 0.6706183552742004, 0.5357015132904053, 0.9932431578636169, 0.5850708484649658, 0.5189139246940613, 0.7113157510757446, 0.7817510962486267, 0.8585118055343628, 0.639873743057251, 0.5331666469573975, 0.9932234287261963, 0.5920596718788147, 0.5288956761360168, 0.6911865472793579, 0.7462008595466614, 0.8619362711906433, 0.6545721888542175, 0.5128986835479736, 0.9931744933128357, 0.5780834555625916, 0.5449074506759644, 0.7183359265327454, 0.7581144571304321, 0.8770245909690857, 0.6733382344245911, 0.5271351337432861, 0.99305659532547, 0.5437202453613281, 0.544615626335144, 0.7204633355140686, 0.7669356465339661, 0.8797428011894226, 0.6500452756881714, 0.5135623216629028, 0.9931941032409668, 0.5502463579177856, 0.5357000827789307, 0.699053168296814, 0.7328851819038391, 0.8966724276542664, 0.6215218305587769, 0.5109009742736816, 0.993123471736908, 0.5107064843177795, 0.5383512377738953, 0.7201191186904907, 0.749271035194397, 0.9015716910362244, 0.5826526284217834, 0.5308673977851868, 0.993077278137207, 0.5168284773826599, 0.5506489276885986, 0.6497328877449036, 0.6971094608306885, 0.9019887447357178, 0.6046763062477112, 0.5177386999130249, 0.9931158423423767, 0.5005768537521362, 0.5618830323219299, 0.6712685823440552, 0.6862804889678955, 0.9043091535568237, 0.6031439900398254, 0.5298432111740112, 0.9931282997131348, 0.5113539099693298, 0.5762118101119995, 0.6594259738922119, 0.6182147860527039, 0.9104577302932739, 0.6022956967353821, 0.5236301422119141, 0.9931706190109253, 0.5438669323921204, 0.5712703466415405, 0.6502598524093628, 0.5566316246986389, 0.9101765751838684, 0.6128419041633606, 0.522254228591919, 0.9931888580322266, 0.5287737846374512, 0.5792110562324524, 0.6586073637008667, 0.5250422358512878, 0.9126839637756348, 0.5897490978240967, 0.5308656096458435, 0.9931644797325134, 0.574010968208313, 0.5846565961837769, 0.6135695576667786, 0.5489630699157715, 0.8841220140457153, 0.6088987588882446, 0.506478488445282, 0.9932730197906494, 0.5679398775100708, 0.5939119458198547, 0.6511194705963135, 0.614477276802063, 0.8889535069465637, 0.5929892063140869, 0.5032184720039368, 0.9932500123977661, 0.5778629183769226, 0.5986690521240234, 0.6518120169639587, 0.6256048679351807, 0.889837920665741, 0.5899483561515808, 0.5128414034843445, 0.9932900667190552, 0.5629292130470276, 0.5968866944313049, 0.6267253160476685, 0.6264012455940247, 0.8857681751251221, 0.5754215717315674, 0.502261221408844, 0.993230938911438, 0.5608798861503601, 0.6183608770370483, 0.6412824392318726, 0.6416126489639282, 0.8737998604774475, 0.5885937809944153, 0.5404254198074341, 0.9931275844573975, 0.5535646677017212, 0.6061174273490906, 0.6273502111434937, 0.5822710990905762, 0.8657698035240173, 0.5524659156799316, 0.5322958827018738, 0.9933226704597473, 0.5585106611251831, 0.6245187520980835, 0.6289239525794983, 0.588779091835022, 0.8390044569969177, 0.5611444711685181, 0.5023037195205688, 0.9932211637496948, 0.5641560554504395, 0.6124558448791504, 0.6222941875457764, 0.5734644532203674, 0.7811209559440613, 0.5241511464118958, 0.5155180096626282, 0.9932709336280823, 0.5663678646087646, 0.606624186038971, 0.5607384443283081, 0.5941535234451294, 0.7320313453674316, 0.5041234493255615, 0.5339107513427734, 0.9933018684387207, 0.5924152135848999, 0.628007709980011, 0.5615995526313782, 0.586371898651123, 0.6180023550987244, 0.5030462741851807, 0.5280443429946899, 0.9932929277420044, 0.5976786017417908, 0.6241474747657776, 0.5702303647994995, 0.5777779221534729, 0.5110785365104675, 0.5714133381843567, 0.5606457591056824, 0.9933035969734192, 0.6357645392417908, 0.6428779363632202, 0.5711074471473694, 0.5428990721702576, 0.5424811840057373, 0.5924244523048401, 0.5654340386390686, 0.9933562278747559, 0.6044771671295166, 0.6354678869247437, 0.5881800651550293, 0.5275135040283203, 0.5441725254058838, 0.6314959526062012, 0.5529423952102661, 0.9931433200836182, 0.602652907371521, 0.6344585418701172, 0.5630474090576172, 0.5279225707054138, 0.5323183536529541, 0.6308091282844543, 0.5726633667945862, 0.9928576946258545, 0.5424417853355408, 0.6442160606384277, 0.5900688171386719, 0.532516360282898, 0.5431787371635437, 0.6378993988037109, 0.5590727925300598, 0.9916955828666687, 0.5305658578872681, 0.647979199886322, 0.5953311920166016, 0.5182175636291504, 0.581484854221344, 0.6503937244415283, 0.5545127391815186, 0.9884278178215027, 0.5169510841369629, 0.6503232717514038, 0.5943241119384766, 0.5567540526390076, 0.5224637389183044, 0.6379137635231018, 0.5834216475486755, 0.9845368266105652, 0.507683515548706, 0.6636084318161011, 0.6129586696624756, 0.5574102401733398, 0.5121274590492249, 0.6598679423332214, 0.5231642723083496, 0.9654149413108826, 0.5778471827507019, 0.6552339792251587, 0.5977353453636169, 0.5221898555755615, 0.5413338541984558, 0.6603280305862427, 0.5525319576263428, 0.9511011242866516, 0.5659794211387634, 0.6648054122924805, 0.6174736022949219, 0.5773178935050964, 0.604526162147522, 0.688809871673584, 0.555931031703949, 0.9151790738105774, 0.612418532371521, 0.6804882287979126, 0.6266442537307739, 0.5649244785308838, 0.6815177202224731, 0.700066089630127, 0.5429056882858276, 0.9053828120231628, 0.5943899154663086, 0.6753279566764832, 0.5916184186935425, 0.5995705127716064, 0.677904486656189, 0.7026689648628235, 0.612091600894928, 0.8683476448059082, 0.6453832983970642, 0.6910334825515747, 0.566181480884552, 0.5859553813934326, 0.666702151298523, 0.7118070125579834, 0.5870270133018494, 0.9574011564254761, 0.6368851661682129, 0.7035196423530579, 0.5590087175369263, 0.5910261273384094, 0.66034996509552, 0.7343848347663879, 0.6129342317581177, 0.8583344221115112, 0.6971549391746521, 0.7137444615364075, 0.5447967648506165, 0.625236988067627, 0.6034564971923828, 0.7194437980651855, 0.6531282663345337, 0.8788819909095764, 0.70207279920578, 0.7194896340370178, 0.566743016242981, 0.6438759565353394, 0.5705652832984924, 0.7243918180465698, 0.6146897077560425, 0.9531077146530151, 0.6915671229362488, 0.7158570885658264, 0.5300812721252441, 0.6523441076278687, 0.553398609161377, 0.7030134201049805, 0.6212944388389587, 0.9337633848190308, 0.6866728663444519, 0.7036401629447937, 0.532457709312439, 0.6923333406448364, 0.5451590418815613, 0.6925572156906128, 0.6337755918502808, 0.967769980430603, 0.6464226245880127, 0.7014654874801636, 0.5734509825706482, 0.7025248408317566, 0.5089573860168457, 0.7009664177894592, 0.6086298823356628, 0.9795762300491333, 0.6719598174095154, 0.694410502910614, 0.5493555665016174, 0.7267121076583862, 0.5664470791816711, 0.6915209293365479, 0.6382849812507629, 0.9746860265731812, 0.6904097199440002, 0.6922449469566345, 0.540303111076355, 0.7599575519561768, 0.6464493274688721, 0.7021298408508301, 0.5893188118934631, 0.8260852098464966, 0.7118959426879883, 0.6855311989784241, 0.5020470023155212, 0.7707143425941467, 0.6652922034263611, 0.6879724264144897, 0.6060724854469299, 0.8323308229446411, 0.699715793132782, 0.6956461668014526, 0.5176111459732056, 0.7935103178024292, 0.7024239301681519, 0.6749365329742432, 0.6439418792724609, 0.7584492564201355, 0.7057164907455444, 0.704741895198822, 0.5334858298301697, 0.7938462495803833, 0.7545217275619507, 0.6597794890403748, 0.6427555680274963, 0.828456461429596, 0.7100902199745178, 0.7036697864532471, 0.5545904636383057, 0.8062671422958374] [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# モデルを評価モードにする\n",
    "model.eval()\n",
    "\n",
    "# 正しい予測数、全体のデータ数を数えるカウンターの0初期化\n",
    "total_data_len = 0\n",
    "total_correct = 0\n",
    "\n",
    "name_list = []\n",
    "index_y_list = []\n",
    "ans_list = []\n",
    "val_list = []\n",
    "\n",
    "loss_mean = 0\n",
    "m = nn.Softmax(dim=2)\n",
    "\n",
    "for j, x in enumerate(test_loader):\n",
    "    x = torch.stack(x)  # リストをテンソルに変換\n",
    "    y = model(x)  # 順伝播（=予測）\n",
    "    # print(m(y))\n",
    "    \n",
    "    \n",
    "    # ミニバッチごとの正答率と損失を求める\n",
    "    _, index_y = torch.max(y, axis=1)  # 最も確率が高いと予測したindex\n",
    "    # print(index_y, index_t)\n",
    "    for i in range(len(t)):  # データ一つずつループ,ミニバッチの中身出しきるまで\n",
    "        total_data_len += 1  # 全データ数を集計\n",
    "    res = m(y)\n",
    "    val, index_y = torch.max(m(y)[0], axis=1)\n",
    "    print(val.tolist(), index_y.tolist())\n",
    "    for j, (i, ii) in enumerate(zip(val.tolist(), index_y.tolist())):\n",
    "        name_list.append(df['0'][j])\n",
    "        index_y_list.append(ii)\n",
    "        val_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストをDataFrameに変換\n",
    "result_df = pd.DataFrame({\n",
    "    'name': name_list,\n",
    "    'index_y': index_y_list,\n",
    "    'ai_confidence': val_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name  index_y  ai_confidence\n",
      "2  AAME        0       0.720228\n",
      "    name  index_y  ai_confidence\n",
      "10  AAME        0       0.741827\n",
      "    name  index_y  ai_confidence\n",
      "18  AAME        0       0.741632\n",
      "    name  index_y  ai_confidence\n",
      "26  AAME        0       0.743582\n",
      "    name  index_y  ai_confidence\n",
      "34  AAME        0       0.744523\n",
      "    name  index_y  ai_confidence\n",
      "42  AAME        0       0.736271\n",
      "    name  index_y  ai_confidence\n",
      "50  AAME        0       0.746645\n",
      "    name  index_y  ai_confidence\n",
      "63  ABCL        0       0.717605\n",
      "    name  index_y  ai_confidence\n",
      "66  AAME        0        0.73218\n",
      "    name  index_y  ai_confidence\n",
      "74  AAME        0       0.718493\n",
      "    name  index_y  ai_confidence\n",
      "82  AAME        0       0.722851\n",
      "    name  index_y  ai_confidence\n",
      "90  AAME        0       0.719773\n",
      "    name  index_y  ai_confidence\n",
      "99  AAOI        0       0.768365\n",
      "     name  index_y  ai_confidence\n",
      "107  AAOI        0       0.754259\n",
      "     name  index_y  ai_confidence\n",
      "114  AAME        0       0.732721\n",
      "     name  index_y  ai_confidence\n",
      "123  AAOI        0       0.769768\n",
      "     name  index_y  ai_confidence\n",
      "131  AAOI        0        0.80025\n",
      "     name  index_y  ai_confidence\n",
      "139  AAOI        0       0.820078\n",
      "     name  index_y  ai_confidence\n",
      "147  AAOI        0        0.81754\n",
      "     name  index_y  ai_confidence\n",
      "154  AAME        0       0.764486\n",
      "     name  index_y  ai_confidence\n",
      "167  ABCL        0       0.748353\n",
      "     name  index_y  ai_confidence\n",
      "175  ABCL        0       0.746457\n",
      "     name  index_y  ai_confidence\n",
      "183  ABCL        0       0.737867\n",
      "     name  index_y  ai_confidence\n",
      "187  AAOI        0       0.752362\n",
      "     name  index_y  ai_confidence\n",
      "195  AAOI        0       0.745573\n",
      "     name  index_y  ai_confidence\n",
      "203  AAOI        0       0.733724\n",
      "     name  index_y  ai_confidence\n",
      "215  ABCL        0       0.724599\n",
      "     name  index_y  ai_confidence\n",
      "219  AAOI        0       0.734157\n",
      "     name  index_y  ai_confidence\n",
      "227  AAOI        0       0.741307\n",
      "     name  index_y  ai_confidence\n",
      "235  AAOI        0       0.732035\n",
      "     name  index_y  ai_confidence\n",
      "243  AAOI        0       0.744212\n",
      "     name  index_y  ai_confidence\n",
      "251  AAOI        0       0.722996\n",
      "     name  index_y  ai_confidence\n",
      "259  AAOI        0       0.729792\n",
      "     name  index_y  ai_confidence\n",
      "267  AAOI        0       0.733351\n",
      "     name  index_y  ai_confidence\n",
      "275  AAOI        0       0.745619\n",
      "     name  index_y  ai_confidence\n",
      "283  AAOI        0       0.763168\n",
      "     name  index_y  ai_confidence\n",
      "291  AAOI        0       0.750953\n",
      "     name  index_y  ai_confidence\n",
      "299  AAOI        0       0.732875\n",
      "     name  index_y  ai_confidence\n",
      "307  AAOI        0       0.713447\n",
      "     name  index_y  ai_confidence\n",
      "315  AAOI        0       0.729497\n",
      "     name  index_y  ai_confidence\n",
      "323  AAOI        0       0.780583\n",
      "     name  index_y  ai_confidence\n",
      "331  AAOI        0       0.809396\n",
      "     name  index_y  ai_confidence\n",
      "339  AAOI        0       0.812817\n",
      "     name  index_y  ai_confidence\n",
      "347  AAOI        0       0.821982\n",
      "     name  index_y  ai_confidence\n",
      "355  AAOI        0       0.834802\n",
      "     name  index_y  ai_confidence\n",
      "363  AAOI        0       0.833853\n",
      "     name  index_y  ai_confidence\n",
      "371  AAOI        0        0.85284\n",
      "     name  index_y  ai_confidence\n",
      "379  AAOI        0       0.865727\n",
      "     name  index_y  ai_confidence\n",
      "387  AAOI        0       0.866952\n",
      "     name  index_y  ai_confidence\n",
      "395  AAOI        0       0.865373\n",
      "     name  index_y  ai_confidence\n",
      "403  AAOI        0       0.826405\n",
      "     name  index_y  ai_confidence\n",
      "411  AAOI        0       0.832495\n",
      "     name  index_y  ai_confidence\n",
      "419  AAOI        0       0.864087\n",
      "     name  index_y  ai_confidence\n",
      "427  AAOI        0       0.877871\n",
      "     name  index_y  ai_confidence\n",
      "435  AAOI        0       0.888709\n",
      "     name  index_y  ai_confidence\n",
      "443  AAOI        0       0.871605\n",
      "     name  index_y  ai_confidence\n",
      "451  AAOI        0       0.870397\n",
      "     name  index_y  ai_confidence\n",
      "459  AAOI        0       0.878091\n",
      "     name  index_y  ai_confidence\n",
      "467  AAOI        0       0.877983\n",
      "     name  index_y  ai_confidence\n",
      "475  AAOI        0       0.877281\n",
      "     name  index_y  ai_confidence\n",
      "483  AAOI        0       0.872836\n",
      "     name  index_y  ai_confidence\n",
      "491  AAOI        0        0.85224\n",
      "     name  index_y  ai_confidence\n",
      "499  AAOI        0       0.800581\n",
      "     name  index_y  ai_confidence\n",
      "507  AAOI        0       0.793579\n",
      "     name  index_y  ai_confidence\n",
      "515  AAOI        0        0.74434\n",
      "     name  index_y  ai_confidence\n",
      "523  AAOI        0       0.784598\n",
      "     name  index_y  ai_confidence\n",
      "531  AAOI        0        0.80699\n",
      "     name  index_y  ai_confidence\n",
      "539  AAOI        0       0.793969\n",
      "     name  index_y  ai_confidence\n",
      "547  AAOI        0       0.824809\n",
      "     name  index_y  ai_confidence\n",
      "555  AAOI        0       0.764194\n",
      "     name  index_y  ai_confidence\n",
      "563  AAOI        0       0.748253\n",
      "     name  index_y  ai_confidence\n",
      "571  AAOI        0       0.759032\n",
      "     name  index_y  ai_confidence\n",
      "579  AAOI        0       0.783781\n",
      "     name  index_y  ai_confidence\n",
      "587  AAOI        0       0.840288\n",
      "     name  index_y  ai_confidence\n",
      "595  AAOI        0       0.835179\n",
      "     name  index_y  ai_confidence\n",
      "603  AAOI        0       0.843862\n",
      "     name  index_y  ai_confidence\n",
      "611  AAOI        0       0.852223\n",
      "     name  index_y  ai_confidence\n",
      "619  AAOI        0       0.886669\n",
      "     name  index_y  ai_confidence\n",
      "627  AAOI        0       0.906422\n",
      "     name  index_y  ai_confidence\n",
      "635  AAOI        0       0.897249\n",
      "     name  index_y  ai_confidence\n",
      "643  AAOI        0       0.904571\n",
      "     name  index_y  ai_confidence\n",
      "651  AAOI        0       0.909692\n",
      "     name  index_y  ai_confidence\n",
      "659  AAOI        0       0.917855\n",
      "     name  index_y  ai_confidence\n",
      "667  AAOI        0       0.935768\n",
      "     name  index_y  ai_confidence\n",
      "675  AAOI        0       0.937895\n",
      "     name  index_y  ai_confidence\n",
      "683  AAOI        0       0.941919\n",
      "     name  index_y  ai_confidence\n",
      "691  AAOI        0       0.943178\n",
      "     name  index_y  ai_confidence\n",
      "699  AAOI        0       0.943001\n",
      "     name  index_y  ai_confidence\n",
      "707  AAOI        0        0.95156\n",
      "     name  index_y  ai_confidence\n",
      "715  AAOI        0       0.958634\n",
      "     name  index_y  ai_confidence\n",
      "723  AAOI        0       0.956689\n",
      "     name  index_y  ai_confidence\n",
      "731  AAOI        0       0.960306\n",
      "     name  index_y  ai_confidence\n",
      "739  AAOI        0       0.960722\n",
      "     name  index_y  ai_confidence\n",
      "747  AAOI        0       0.958588\n",
      "     name  index_y  ai_confidence\n",
      "755  AAOI        0       0.962526\n",
      "     name  index_y  ai_confidence\n",
      "763  AAOI        0       0.958893\n",
      "     name  index_y  ai_confidence\n",
      "771  AAOI        0       0.962274\n",
      "     name  index_y  ai_confidence\n",
      "779  AAOI        0       0.968341\n",
      "     name  index_y  ai_confidence\n",
      "787  AAOI        0       0.966862\n",
      "     name  index_y  ai_confidence\n",
      "795  AAOI        0       0.967433\n",
      "     name  index_y  ai_confidence\n",
      "803  AAOI        0       0.962714\n",
      "     name  index_y  ai_confidence\n",
      "811  AAOI        0       0.960809\n",
      "     name  index_y  ai_confidence\n",
      "819  AAOI        0        0.96422\n",
      "     name  index_y  ai_confidence\n",
      "827  AAOI        0       0.966105\n",
      "     name  index_y  ai_confidence\n",
      "835  AAOI        0       0.962637\n",
      "     name  index_y  ai_confidence\n",
      "843  AAOI        0       0.962336\n",
      "     name  index_y  ai_confidence\n",
      "851  AAOI        0        0.95806\n",
      "     name  index_y  ai_confidence\n",
      "859  AAOI        0       0.957328\n",
      "     name  index_y  ai_confidence\n",
      "867  AAOI        0       0.962047\n",
      "     name  index_y  ai_confidence\n",
      "875  AAOI        0       0.954297\n",
      "     name  index_y  ai_confidence\n",
      "883  AAOI        0        0.94491\n",
      "     name  index_y  ai_confidence\n",
      "891  AAOI        0       0.940131\n",
      "     name  index_y  ai_confidence\n",
      "899  AAOI        0       0.929639\n",
      "     name  index_y  ai_confidence\n",
      "907  AAOI        0       0.941841\n",
      "     name  index_y  ai_confidence\n",
      "915  AAOI        0       0.934854\n",
      "     name  index_y  ai_confidence\n",
      "923  AAOI        0       0.936516\n",
      "     name  index_y  ai_confidence\n",
      "931  AAOI        0       0.947181\n",
      "     name  index_y  ai_confidence\n",
      "939  AAOI        0       0.949598\n",
      "     name  index_y  ai_confidence\n",
      "947  AAOI        0        0.95102\n",
      "     name  index_y  ai_confidence\n",
      "955  AAOI        0        0.94576\n",
      "     name  index_y  ai_confidence\n",
      "963  AAOI        0       0.937406\n",
      "     name  index_y  ai_confidence\n",
      "971  AAOI        0       0.929168\n",
      "     name  index_y  ai_confidence\n",
      "979  AAOI        0       0.920832\n",
      "     name  index_y  ai_confidence\n",
      "987  AAOI        0       0.903074\n",
      "     name  index_y  ai_confidence\n",
      "995  AAOI        0       0.900651\n",
      "      name  index_y  ai_confidence\n",
      "1003  AAOI        0       0.916707\n",
      "      name  index_y  ai_confidence\n",
      "1011  AAOI        0       0.921901\n",
      "      name  index_y  ai_confidence\n",
      "1019  AAOI        0       0.931288\n",
      "      name  index_y  ai_confidence\n",
      "1027  AAOI        0       0.936662\n",
      "      name  index_y  ai_confidence\n",
      "1035  AAOI        0        0.93554\n",
      "      name  index_y  ai_confidence\n",
      "1043  AAOI        0       0.931761\n",
      "      name  index_y  ai_confidence\n",
      "1051  AAOI        0        0.91173\n",
      "      name  index_y  ai_confidence\n",
      "1059  AAOI        0       0.890478\n",
      "      name  index_y  ai_confidence\n",
      "1067  AAOI        0       0.860995\n",
      "      name  index_y  ai_confidence\n",
      "1079  ABCL        1       0.874468\n",
      "      name  index_y  ai_confidence\n",
      "1087  ABCL        1       0.842583\n",
      "      name  index_y  ai_confidence\n",
      "1095  ABCL        1       0.858644\n",
      "      name  index_y  ai_confidence\n",
      "1103  ABCL        1       0.848182\n",
      "      name  index_y  ai_confidence\n",
      "1111  ABCL        1       0.816736\n",
      "      name  index_y  ai_confidence\n",
      "1119  ABCL        1       0.770558\n",
      "      name  index_y  ai_confidence\n",
      "1127  ABCL        1       0.735462\n",
      "      name  index_y  ai_confidence\n",
      "1135  ABCL        1       0.741278\n",
      "      name  index_y  ai_confidence\n",
      "1143  ABCL        1       0.731257\n",
      "      name  index_y  ai_confidence\n",
      "1147  AAOI        1       0.822954\n",
      "      name  index_y  ai_confidence\n",
      "1155  AAOI        1       0.858544\n",
      "      name  index_y  ai_confidence\n",
      "1163  AAOI        1       0.904732\n",
      "      name  index_y  ai_confidence\n",
      "1171  AAOI        1       0.928136\n",
      "      name  index_y  ai_confidence\n",
      "1179  AAOI        1       0.939717\n",
      "      name  index_y  ai_confidence\n",
      "1187  AAOI        1       0.953355\n",
      "      name  index_y  ai_confidence\n",
      "1195  AAOI        1       0.958513\n",
      "      name  index_y  ai_confidence\n",
      "1203  AAOI        1        0.96442\n",
      "      name  index_y  ai_confidence\n",
      "1211  AAOI        1       0.968359\n",
      "      name  index_y  ai_confidence\n",
      "1219  AAOI        1       0.972084\n",
      "      name  index_y  ai_confidence\n",
      "1227  AAOI        1       0.977203\n",
      "      name  index_y  ai_confidence\n",
      "1235  AAOI        1       0.981002\n",
      "      name  index_y  ai_confidence\n",
      "1243  AAOI        1       0.984172\n",
      "      name  index_y  ai_confidence\n",
      "1251  AAOI        1       0.986486\n",
      "      name  index_y  ai_confidence\n",
      "1259  AAOI        1       0.987888\n",
      "      name  index_y  ai_confidence\n",
      "1267  AAOI        1       0.988591\n",
      "      name  index_y  ai_confidence\n",
      "1275  AAOI        1       0.989286\n",
      "      name  index_y  ai_confidence\n",
      "1283  AAOI        1       0.989626\n",
      "      name  index_y  ai_confidence\n",
      "1291  AAOI        1        0.99008\n",
      "      name  index_y  ai_confidence\n",
      "1299  AAOI        1       0.990407\n",
      "      name  index_y  ai_confidence\n",
      "1307  AAOI        1       0.990812\n",
      "      name  index_y  ai_confidence\n",
      "1315  AAOI        1       0.990939\n",
      "      name  index_y  ai_confidence\n",
      "1323  AAOI        1       0.991068\n",
      "      name  index_y  ai_confidence\n",
      "1331  AAOI        1       0.991094\n",
      "      name  index_y  ai_confidence\n",
      "1339  AAOI        1       0.991138\n",
      "      name  index_y  ai_confidence\n",
      "1347  AAOI        1       0.991101\n",
      "      name  index_y  ai_confidence\n",
      "1355  AAOI        1       0.990945\n",
      "      name  index_y  ai_confidence\n",
      "1363  AAOI        1       0.990809\n",
      "      name  index_y  ai_confidence\n",
      "1371  AAOI        1       0.990926\n",
      "      name  index_y  ai_confidence\n",
      "1379  AAOI        1       0.990879\n",
      "      name  index_y  ai_confidence\n",
      "1387  AAOI        1       0.990733\n",
      "      name  index_y  ai_confidence\n",
      "1395  AAOI        1       0.990495\n",
      "      name  index_y  ai_confidence\n",
      "1403  AAOI        1       0.990296\n",
      "      name  index_y  ai_confidence\n",
      "1411  AAOI        1       0.990164\n",
      "      name  index_y  ai_confidence\n",
      "1419  AAOI        1       0.990312\n",
      "      name  index_y  ai_confidence\n",
      "1427  AAOI        1       0.990298\n",
      "      name  index_y  ai_confidence\n",
      "1435  AAOI        1       0.990801\n",
      "      name  index_y  ai_confidence\n",
      "1443  AAOI        1       0.991191\n",
      "      name  index_y  ai_confidence\n",
      "1451  AAOI        1        0.99121\n",
      "      name  index_y  ai_confidence\n",
      "1459  AAOI        1       0.991608\n",
      "      name  index_y  ai_confidence\n",
      "1467  AAOI        1       0.991653\n",
      "      name  index_y  ai_confidence\n",
      "1475  AAOI        1       0.991673\n",
      "      name  index_y  ai_confidence\n",
      "1483  AAOI        1       0.991631\n",
      "      name  index_y  ai_confidence\n",
      "1491  AAOI        1        0.99148\n",
      "      name  index_y  ai_confidence\n",
      "1499  AAOI        1       0.991713\n",
      "      name  index_y  ai_confidence\n",
      "1507  AAOI        1       0.991395\n",
      "      name  index_y  ai_confidence\n",
      "1515  AAOI        1       0.991372\n",
      "      name  index_y  ai_confidence\n",
      "1523  AAOI        1       0.991046\n",
      "      name  index_y  ai_confidence\n",
      "1531  AAOI        1       0.990946\n",
      "      name  index_y  ai_confidence\n",
      "1539  AAOI        1       0.991126\n",
      "      name  index_y  ai_confidence\n",
      "1547  AAOI        1       0.990776\n",
      "      name  index_y  ai_confidence\n",
      "1555  AAOI        1       0.990467\n",
      "      name  index_y  ai_confidence\n",
      "1563  AAOI        1       0.990234\n",
      "      name  index_y  ai_confidence\n",
      "1571  AAOI        1       0.989833\n",
      "      name  index_y  ai_confidence\n",
      "1579  AAOI        1       0.989225\n",
      "      name  index_y  ai_confidence\n",
      "1587  AAOI        1       0.988056\n",
      "      name  index_y  ai_confidence\n",
      "1595  AAOI        1       0.987099\n",
      "      name  index_y  ai_confidence\n",
      "1603  AAOI        1       0.986477\n",
      "      name  index_y  ai_confidence\n",
      "1611  AAOI        1       0.986773\n",
      "      name  index_y  ai_confidence\n",
      "1619  AAOI        1       0.986716\n",
      "      name  index_y  ai_confidence\n",
      "1627  AAOI        1       0.986501\n",
      "      name  index_y  ai_confidence\n",
      "1635  AAOI        1       0.985358\n",
      "      name  index_y  ai_confidence\n",
      "1643  AAOI        1       0.983929\n",
      "      name  index_y  ai_confidence\n",
      "1651  AAOI        1       0.983499\n",
      "      name  index_y  ai_confidence\n",
      "1659  AAOI        1       0.982202\n",
      "      name  index_y  ai_confidence\n",
      "1667  AAOI        1       0.982327\n",
      "      name  index_y  ai_confidence\n",
      "1675  AAOI        1       0.980748\n",
      "      name  index_y  ai_confidence\n",
      "1683  AAOI        1       0.976105\n",
      "      name  index_y  ai_confidence\n",
      "1691  AAOI        1       0.971611\n",
      "      name  index_y  ai_confidence\n",
      "1699  AAOI        1       0.958429\n",
      "      name  index_y  ai_confidence\n",
      "1707  AAOI        1       0.947058\n",
      "      name  index_y  ai_confidence\n",
      "1715  AAOI        1        0.92099\n",
      "      name  index_y  ai_confidence\n",
      "1723  AAOI        1       0.903655\n",
      "      name  index_y  ai_confidence\n",
      "1731  AAOI        1       0.907074\n",
      "      name  index_y  ai_confidence\n",
      "1739  AAOI        1       0.873777\n",
      "      name  index_y  ai_confidence\n",
      "1747  AAOI        1       0.840362\n",
      "      name  index_y  ai_confidence\n",
      "1755  AAOI        1       0.684266\n",
      "      name  index_y  ai_confidence\n",
      "1765  AAPL        0        0.65101\n",
      "      name  index_y  ai_confidence\n",
      "1768  AADI        1       0.646689\n",
      "      name  index_y  ai_confidence\n",
      "1780  AAON        1       0.705521\n",
      "      name  index_y  ai_confidence\n",
      "1788  AAON        1       0.684653\n",
      "      name  index_y  ai_confidence\n",
      "1796  AAON        1       0.755753\n",
      "      name  index_y  ai_confidence\n",
      "1804  AAON        1       0.715204\n",
      "      name  index_y  ai_confidence\n",
      "1812  AAON        1       0.764869\n",
      "      name  index_y  ai_confidence\n",
      "1820  AAON        1       0.756801\n",
      "      name  index_y  ai_confidence\n",
      "1831  ABCL        1       0.726129\n",
      "      name  index_y  ai_confidence\n",
      "1835  AAOI        0       0.803455\n",
      "      name  index_y  ai_confidence\n",
      "1843  AAOI        0       0.872663\n",
      "      name  index_y  ai_confidence\n",
      "1851  AAOI        0       0.871693\n",
      "      name  index_y  ai_confidence\n",
      "1859  AAOI        0       0.865313\n",
      "      name  index_y  ai_confidence\n",
      "1867  AAOI        0       0.866603\n",
      "      name  index_y  ai_confidence\n",
      "1875  AAOI        0       0.881009\n",
      "      name  index_y  ai_confidence\n",
      "1883  AAOI        0       0.895248\n",
      "      name  index_y  ai_confidence\n",
      "1891  AAOI        0       0.891244\n",
      "      name  index_y  ai_confidence\n",
      "1899  AAOI        0       0.846066\n",
      "      name  index_y  ai_confidence\n",
      "1907  AAOI        0       0.811357\n",
      "      name  index_y  ai_confidence\n",
      "1916  AAON        1       0.794826\n",
      "      name  index_y  ai_confidence\n",
      "1924  AAON        1       0.809737\n",
      "      name  index_y  ai_confidence\n",
      "1932  AAON        1       0.790329\n",
      "      name  index_y  ai_confidence\n",
      "1940  AAON        1       0.796348\n",
      "      name  index_y  ai_confidence\n",
      "1948  AAON        1       0.786942\n",
      "      name  index_y  ai_confidence\n",
      "1956  AAON        1       0.786108\n",
      "      name  index_y  ai_confidence\n",
      "1964  AAON        1       0.777633\n",
      "      name  index_y  ai_confidence\n",
      "1972  AAON        1       0.754007\n",
      "      name  index_y  ai_confidence\n",
      "1980  AAON        1        0.72867\n",
      "      name  index_y  ai_confidence\n",
      "1988  AAON        1       0.758773\n",
      "      name  index_y  ai_confidence\n",
      "1996  AAON        1       0.735918\n",
      "      name  index_y  ai_confidence\n",
      "2004  AAON        1       0.757386\n",
      "      name  index_y  ai_confidence\n",
      "2012  AAON        1       0.734141\n",
      "      name  index_y  ai_confidence\n",
      "2020  AAON        1       0.745918\n",
      "      name  index_y  ai_confidence\n",
      "2028  AAON        1       0.774629\n",
      "      name  index_y  ai_confidence\n",
      "2036  AAON        1       0.727643\n",
      "      name  index_y  ai_confidence\n",
      "2044  AAON        1       0.769539\n",
      "      name  index_y  ai_confidence\n",
      "2051  AAOI        1        0.69791\n",
      "      name  index_y  ai_confidence\n",
      "2059  AAOI        1       0.758012\n",
      "      name  index_y  ai_confidence\n",
      "2067  AAOI        1       0.799447\n",
      "      name  index_y  ai_confidence\n",
      "2075  AAOI        1        0.84551\n",
      "      name  index_y  ai_confidence\n",
      "2083  AAOI        1       0.887278\n",
      "      name  index_y  ai_confidence\n",
      "2091  AAOI        1       0.921234\n",
      "      name  index_y  ai_confidence\n",
      "2099  AAOI        1       0.942076\n",
      "      name  index_y  ai_confidence\n",
      "2107  AAOI        1       0.952369\n",
      "      name  index_y  ai_confidence\n",
      "2115  AAOI        1       0.965927\n",
      "      name  index_y  ai_confidence\n",
      "2123  AAOI        1       0.971134\n",
      "      name  index_y  ai_confidence\n",
      "2131  AAOI        1       0.974505\n",
      "      name  index_y  ai_confidence\n",
      "2139  AAOI        1       0.978662\n",
      "      name  index_y  ai_confidence\n",
      "2147  AAOI        1       0.980645\n",
      "      name  index_y  ai_confidence\n",
      "2155  AAOI        1       0.982523\n",
      "      name  index_y  ai_confidence\n",
      "2163  AAOI        1       0.984448\n",
      "      name  index_y  ai_confidence\n",
      "2171  AAOI        1       0.984644\n",
      "      name  index_y  ai_confidence\n",
      "2179  AAOI        1       0.984963\n",
      "      name  index_y  ai_confidence\n",
      "2187  AAOI        1       0.985417\n",
      "      name  index_y  ai_confidence\n",
      "2195  AAOI        1       0.986517\n",
      "      name  index_y  ai_confidence\n",
      "2203  AAOI        1       0.987071\n",
      "      name  index_y  ai_confidence\n",
      "2211  AAOI        1       0.987377\n",
      "      name  index_y  ai_confidence\n",
      "2219  AAOI        1       0.987917\n",
      "      name  index_y  ai_confidence\n",
      "2227  AAOI        1       0.988447\n",
      "      name  index_y  ai_confidence\n",
      "2235  AAOI        1       0.988687\n",
      "      name  index_y  ai_confidence\n",
      "2243  AAOI        1       0.988886\n",
      "      name  index_y  ai_confidence\n",
      "2251  AAOI        1       0.989131\n",
      "      name  index_y  ai_confidence\n",
      "2259  AAOI        1       0.989215\n",
      "      name  index_y  ai_confidence\n",
      "2267  AAOI        1       0.989519\n",
      "      name  index_y  ai_confidence\n",
      "2275  AAOI        1        0.98978\n",
      "      name  index_y  ai_confidence\n",
      "2283  AAOI        1       0.989705\n",
      "      name  index_y  ai_confidence\n",
      "2291  AAOI        1        0.98997\n",
      "      name  index_y  ai_confidence\n",
      "2299  AAOI        1       0.990049\n",
      "      name  index_y  ai_confidence\n",
      "2307  AAOI        1       0.990153\n",
      "      name  index_y  ai_confidence\n",
      "2315  AAOI        1       0.990249\n",
      "      name  index_y  ai_confidence\n",
      "2323  AAOI        1        0.99016\n",
      "      name  index_y  ai_confidence\n",
      "2331  AAOI        1       0.990455\n",
      "      name  index_y  ai_confidence\n",
      "2339  AAOI        1        0.99049\n",
      "      name  index_y  ai_confidence\n",
      "2347  AAOI        1       0.990159\n",
      "      name  index_y  ai_confidence\n",
      "2355  AAOI        1       0.989897\n",
      "      name  index_y  ai_confidence\n",
      "2363  AAOI        1       0.989903\n",
      "      name  index_y  ai_confidence\n",
      "2371  AAOI        1       0.989962\n",
      "      name  index_y  ai_confidence\n",
      "2379  AAOI        1       0.989882\n",
      "      name  index_y  ai_confidence\n",
      "2387  AAOI        1       0.989839\n",
      "      name  index_y  ai_confidence\n",
      "2395  AAOI        1       0.989511\n",
      "      name  index_y  ai_confidence\n",
      "2403  AAOI        1       0.989461\n",
      "      name  index_y  ai_confidence\n",
      "2411  AAOI        1       0.989301\n",
      "      name  index_y  ai_confidence\n",
      "2419  AAOI        1       0.988508\n",
      "      name  index_y  ai_confidence\n",
      "2427  AAOI        1       0.988107\n",
      "      name  index_y  ai_confidence\n",
      "2435  AAOI        1        0.98761\n",
      "      name  index_y  ai_confidence\n",
      "2443  AAOI        1       0.986915\n",
      "      name  index_y  ai_confidence\n",
      "2451  AAOI        1       0.985943\n",
      "      name  index_y  ai_confidence\n",
      "2459  AAOI        1       0.983879\n",
      "      name  index_y  ai_confidence\n",
      "2467  AAOI        1       0.982317\n",
      "      name  index_y  ai_confidence\n",
      "2475  AAOI        1       0.980031\n",
      "      name  index_y  ai_confidence\n",
      "2483  AAOI        1       0.973038\n",
      "      name  index_y  ai_confidence\n",
      "2491  AAOI        1       0.961316\n",
      "      name  index_y  ai_confidence\n",
      "2499  AAOI        1       0.941392\n",
      "      name  index_y  ai_confidence\n",
      "2507  AAOI        1       0.910253\n",
      "      name  index_y  ai_confidence\n",
      "2515  AAOI        1       0.852737\n",
      "      name  index_y  ai_confidence\n",
      "2523  AAOI        1       0.778071\n",
      "      name  index_y  ai_confidence\n",
      "2528  AADI        0       0.776535\n",
      "      name  index_y  ai_confidence\n",
      "2536  AADI        0       0.776631\n",
      "      name  index_y  ai_confidence\n",
      "2544  AADI        0       0.756514\n",
      "      name  index_y  ai_confidence\n",
      "2552  AADI        0       0.731897\n",
      "      name  index_y  ai_confidence\n",
      "2560  AADI        0       0.754695\n",
      "      name  index_y  ai_confidence\n",
      "2568  AADI        0       0.760763\n",
      "      name  index_y  ai_confidence\n",
      "2576  AADI        0       0.744889\n",
      "      name  index_y  ai_confidence\n",
      "2584  AADI        0       0.761144\n",
      "      name  index_y  ai_confidence\n",
      "2592  AADI        0       0.760862\n",
      "      name  index_y  ai_confidence\n",
      "2600  AADI        0       0.777309\n",
      "      name  index_y  ai_confidence\n",
      "2608  AADI        0       0.766156\n",
      "      name  index_y  ai_confidence\n",
      "2616  AADI        0       0.747556\n",
      "      name  index_y  ai_confidence\n",
      "2624  AADI        0       0.764217\n",
      "      name  index_y  ai_confidence\n",
      "2632  AADI        0       0.763069\n",
      "      name  index_y  ai_confidence\n",
      "2640  AADI        0       0.753394\n",
      "      name  index_y  ai_confidence\n",
      "2648  AADI        0       0.762326\n",
      "      name  index_y  ai_confidence\n",
      "2656  AADI        0       0.754294\n",
      "      name  index_y  ai_confidence\n",
      "2664  AADI        0       0.783132\n",
      "      name  index_y  ai_confidence\n",
      "2672  AADI        0       0.759365\n",
      "      name  index_y  ai_confidence\n",
      "2680  AADI        0       0.766128\n",
      "      name  index_y  ai_confidence\n",
      "2688  AADI        0       0.779878\n",
      "      name  index_y  ai_confidence\n",
      "2696  AADI        0       0.761697\n",
      "      name  index_y  ai_confidence\n",
      "2704  AADI        0       0.770771\n",
      "      name  index_y  ai_confidence\n",
      "2715  AAOI        0       0.781706\n",
      "      name  index_y  ai_confidence\n",
      "2723  AAOI        0       0.776474\n",
      "      name  index_y  ai_confidence\n",
      "2728  AADI        0       0.727383\n",
      "      name  index_y  ai_confidence\n",
      "2742  ABCB        0       0.725052\n",
      "      name  index_y  ai_confidence\n",
      "2750  ABCB        0       0.724402\n",
      "      name  index_y  ai_confidence\n",
      "2758  ABCB        0        0.72032\n",
      "      name  index_y  ai_confidence\n",
      "2766  ABCB        0       0.718039\n",
      "      name  index_y  ai_confidence\n",
      "2774  ABCB        0       0.722975\n",
      "      name  index_y  ai_confidence\n",
      "2782  ABCB        0       0.712957\n",
      "      name  index_y  ai_confidence\n",
      "2790  ABCB        0       0.703939\n",
      "      name  index_y  ai_confidence\n",
      "2798  ABCB        0       0.697168\n",
      "      name  index_y  ai_confidence\n",
      "2804  AAON        0       0.694611\n",
      "      name  index_y  ai_confidence\n",
      "2812  AAON        0       0.715725\n",
      "      name  index_y  ai_confidence\n",
      "2820  AAON        0       0.759933\n",
      "      name  index_y  ai_confidence\n",
      "2827  AAOI        1       0.842861\n",
      "      name  index_y  ai_confidence\n",
      "2835  AAOI        1       0.890016\n",
      "      name  index_y  ai_confidence\n",
      "2843  AAOI        1       0.922681\n",
      "      name  index_y  ai_confidence\n",
      "2851  AAOI        1       0.938334\n",
      "      name  index_y  ai_confidence\n",
      "2859  AAOI        1       0.956391\n",
      "      name  index_y  ai_confidence\n",
      "2867  AAOI        1       0.969657\n",
      "      name  index_y  ai_confidence\n",
      "2875  AAOI        1        0.97722\n",
      "      name  index_y  ai_confidence\n",
      "2883  AAOI        1       0.981783\n",
      "      name  index_y  ai_confidence\n",
      "2891  AAOI        1        0.98591\n",
      "      name  index_y  ai_confidence\n",
      "2899  AAOI        1       0.987912\n",
      "      name  index_y  ai_confidence\n",
      "2907  AAOI        1       0.989208\n",
      "      name  index_y  ai_confidence\n",
      "2915  AAOI        1       0.990269\n",
      "      name  index_y  ai_confidence\n",
      "2923  AAOI        1       0.990788\n",
      "      name  index_y  ai_confidence\n",
      "2931  AAOI        1       0.991519\n",
      "      name  index_y  ai_confidence\n",
      "2939  AAOI        1       0.991882\n",
      "      name  index_y  ai_confidence\n",
      "2947  AAOI        1       0.992118\n",
      "      name  index_y  ai_confidence\n",
      "2955  AAOI        1       0.992237\n",
      "      name  index_y  ai_confidence\n",
      "2963  AAOI        1       0.992344\n",
      "      name  index_y  ai_confidence\n",
      "2971  AAOI        1        0.99254\n",
      "      name  index_y  ai_confidence\n",
      "2979  AAOI        1       0.992586\n",
      "      name  index_y  ai_confidence\n",
      "2987  AAOI        1       0.992559\n",
      "      name  index_y  ai_confidence\n",
      "2995  AAOI        1       0.992567\n",
      "      name  index_y  ai_confidence\n",
      "3003  AAOI        1       0.992669\n",
      "      name  index_y  ai_confidence\n",
      "3011  AAOI        1       0.992706\n",
      "      name  index_y  ai_confidence\n",
      "3019  AAOI        1       0.992739\n",
      "      name  index_y  ai_confidence\n",
      "3027  AAOI        1       0.992741\n",
      "      name  index_y  ai_confidence\n",
      "3035  AAOI        1       0.992749\n",
      "      name  index_y  ai_confidence\n",
      "3043  AAOI        1       0.992848\n",
      "      name  index_y  ai_confidence\n",
      "3051  AAOI        1       0.992922\n",
      "      name  index_y  ai_confidence\n",
      "3059  AAOI        1        0.99309\n",
      "      name  index_y  ai_confidence\n",
      "3067  AAOI        1       0.993086\n",
      "      name  index_y  ai_confidence\n",
      "3075  AAOI        1       0.993153\n",
      "      name  index_y  ai_confidence\n",
      "3083  AAOI        1       0.993168\n",
      "      name  index_y  ai_confidence\n",
      "3091  AAOI        1       0.993183\n",
      "      name  index_y  ai_confidence\n",
      "3099  AAOI        1       0.993247\n",
      "      name  index_y  ai_confidence\n",
      "3107  AAOI        1       0.993166\n",
      "      name  index_y  ai_confidence\n",
      "3115  AAOI        1       0.993203\n",
      "      name  index_y  ai_confidence\n",
      "3123  AAOI        1       0.993185\n",
      "      name  index_y  ai_confidence\n",
      "3131  AAOI        1       0.993173\n",
      "      name  index_y  ai_confidence\n",
      "3139  AAOI        1       0.993154\n",
      "      name  index_y  ai_confidence\n",
      "3147  AAOI        1       0.993142\n",
      "      name  index_y  ai_confidence\n",
      "3155  AAOI        1       0.993127\n",
      "      name  index_y  ai_confidence\n",
      "3163  AAOI        1       0.993221\n",
      "      name  index_y  ai_confidence\n",
      "3171  AAOI        1       0.993243\n",
      "      name  index_y  ai_confidence\n",
      "3179  AAOI        1       0.993223\n",
      "      name  index_y  ai_confidence\n",
      "3187  AAOI        1       0.993174\n",
      "      name  index_y  ai_confidence\n",
      "3195  AAOI        1       0.993057\n",
      "      name  index_y  ai_confidence\n",
      "3203  AAOI        1       0.993194\n",
      "      name  index_y  ai_confidence\n",
      "3211  AAOI        1       0.993123\n",
      "      name  index_y  ai_confidence\n",
      "3219  AAOI        1       0.993077\n",
      "      name  index_y  ai_confidence\n",
      "3227  AAOI        1       0.993116\n",
      "      name  index_y  ai_confidence\n",
      "3235  AAOI        1       0.993128\n",
      "      name  index_y  ai_confidence\n",
      "3243  AAOI        1       0.993171\n",
      "      name  index_y  ai_confidence\n",
      "3251  AAOI        1       0.993189\n",
      "      name  index_y  ai_confidence\n",
      "3259  AAOI        1       0.993164\n",
      "      name  index_y  ai_confidence\n",
      "3267  AAOI        1       0.993273\n",
      "      name  index_y  ai_confidence\n",
      "3275  AAOI        1        0.99325\n",
      "      name  index_y  ai_confidence\n",
      "3283  AAOI        1        0.99329\n",
      "      name  index_y  ai_confidence\n",
      "3291  AAOI        1       0.993231\n",
      "      name  index_y  ai_confidence\n",
      "3299  AAOI        1       0.993128\n",
      "      name  index_y  ai_confidence\n",
      "3307  AAOI        1       0.993323\n",
      "      name  index_y  ai_confidence\n",
      "3315  AAOI        1       0.993221\n",
      "      name  index_y  ai_confidence\n",
      "3323  AAOI        1       0.993271\n",
      "      name  index_y  ai_confidence\n",
      "3331  AAOI        1       0.993302\n",
      "      name  index_y  ai_confidence\n",
      "3339  AAOI        1       0.993293\n",
      "      name  index_y  ai_confidence\n",
      "3347  AAOI        1       0.993304\n",
      "      name  index_y  ai_confidence\n",
      "3355  AAOI        1       0.993356\n",
      "      name  index_y  ai_confidence\n",
      "3363  AAOI        1       0.993143\n",
      "      name  index_y  ai_confidence\n",
      "3371  AAOI        1       0.992858\n",
      "      name  index_y  ai_confidence\n",
      "3379  AAOI        1       0.991696\n",
      "      name  index_y  ai_confidence\n",
      "3387  AAOI        1       0.988428\n",
      "      name  index_y  ai_confidence\n",
      "3395  AAOI        1       0.984537\n",
      "      name  index_y  ai_confidence\n",
      "3403  AAOI        1       0.965415\n",
      "      name  index_y  ai_confidence\n",
      "3411  AAOI        1       0.951101\n",
      "      name  index_y  ai_confidence\n",
      "3419  AAOI        1       0.915179\n",
      "      name  index_y  ai_confidence\n",
      "3427  AAOI        1       0.905383\n",
      "      name  index_y  ai_confidence\n",
      "3435  AAOI        1       0.868348\n",
      "      name  index_y  ai_confidence\n",
      "3443  AAOI        1       0.957401\n",
      "      name  index_y  ai_confidence\n",
      "3451  AAOI        1       0.858334\n",
      "      name  index_y  ai_confidence\n",
      "3459  AAOI        1       0.878882\n",
      "      name  index_y  ai_confidence\n",
      "3467  AAOI        1       0.953108\n",
      "      name  index_y  ai_confidence\n",
      "3475  AAOI        1       0.933763\n",
      "      name  index_y  ai_confidence\n",
      "3483  AAOI        1        0.96777\n",
      "      name  index_y  ai_confidence\n",
      "3491  AAOI        1       0.979576\n",
      "      name  index_y  ai_confidence\n",
      "3499  AAOI        1       0.974686\n",
      "      name  index_y  ai_confidence\n",
      "3507  AAOI        1       0.826085\n",
      "      name  index_y  ai_confidence\n",
      "3511  ABCL        0       0.770714\n",
      "      name  index_y  ai_confidence\n",
      "3515  AAOI        1       0.832331\n",
      "      name  index_y  ai_confidence\n",
      "3527  ABCL        0       0.793846\n",
      "      name  index_y  ai_confidence\n",
      "3531  AAOI        1       0.828456\n"
     ]
    }
   ],
   "source": [
    "# 7つずつに分割する\n",
    "result_df_split = np.array_split(result_df, len(result_df) // interval + 1)\n",
    "\n",
    "# 分割したデータフレームを表示する\n",
    "for df in result_df_split:\n",
    "    # val列で降順にソート\n",
    "    sorted_df = df.sort_values(by='ai_confidence', ascending=False)\n",
    "    print(sorted_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>index_y</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AADI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.590046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAME</td>\n",
       "      <td>0</td>\n",
       "      <td>0.720228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.621711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>1</td>\n",
       "      <td>0.651037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>AAOI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.828456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>AAON</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.703670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>ABCB</td>\n",
       "      <td>0</td>\n",
       "      <td>0.554590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>ABCL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.806267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3536 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  index_y       val\n",
       "0     AADI        0  0.618881\n",
       "1      AAL        0  0.590046\n",
       "2     AAME        0  0.720228\n",
       "3     AAOI        0  0.621711\n",
       "4     AAON        1  0.651037\n",
       "...    ...      ...       ...\n",
       "3531  AAOI        1  0.828456\n",
       "3532  AAON        0  0.710090\n",
       "3533  AAPL        0  0.703670\n",
       "3534  ABCB        0  0.554590\n",
       "3535  ABCL        0  0.806267\n",
       "\n",
       "[3536 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val列で降順にソート\n",
    "result_df = result_df.sort_values(by='val', ascending=False)\n",
    "\n",
    "# DataFrameの内容を表示\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_path = \"pread_AI.csv\"\n",
    "# headers = ','.join(t.columns.to_list())\n",
    "# with open(csv_path, \"w\", encoding=\"utf_8\") as csv_file:\n",
    "#     csv_file.write(headers + '\\n')\n",
    "pred_list = []\n",
    "for data_raw in dataset:\n",
    "    data = torch.Tensor(data_raw[0]).to(device)\n",
    "    pred = model(data)\n",
    "    pred = F.softmax(pred, dim=0)\n",
    "    pred_list.append(pred.tolist())\n",
    "\n",
    "    # with open(csv_path, 'a') as file:\n",
    "    #     writer = csv.writer(file)\n",
    "    #     writer.writerow(pred_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
